{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\boezi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import nltk\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk import tokenize\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeatureExtractor(ABC):\n",
    "    \"\"\"Abstract class to describe a generic extraction of a feature starting from some text\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self, text, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the extractor with the raw text and other parameters which depend on the extractor\n",
    "        Parameters:\n",
    "        - text: raw input to extract feature from\n",
    "        - **kwargs: dictionary containing specific extractor parameters\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def extract(self):\n",
    "        \"\"\"\n",
    "        Extract the feature from the input text\n",
    "        Return: an array containing the extracted features\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the feature engineering is trying to add other features in addition to the word embeddings to provide more useful information for each word of the document to summarize, these features will be concatenated to the word embeddings of each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the extraction of Tf and Idf the tokenization process is not the same of the keras Tokenizer, therefore the TfIDfVectorizer uses a tokenization process provided as input parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_tokenizer(corpus):\n",
    "    \"\"\"\n",
    "    Returns a function which tokenizes a sentence according to a vocabularu built on corpus\n",
    "    Parameters:\n",
    "     corpus: list of documents\n",
    "    Returns:\n",
    "     function to compute sentence tokenization\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    def tokenize_sentence(sentence):\n",
    "        \"\"\"\n",
    "        Returns a list of string token given a sentence\n",
    "        Parameters:\n",
    "         sentence: sentence to tokenize\n",
    "        Returns:\n",
    "         list of string tokens\n",
    "        \"\"\"\n",
    "        sequences = tokenizer.texts_to_sequences([sentence])\n",
    "        sequence = sequences[0]\n",
    "        tokenized_sentence = [tokenizer.index_word[i] for i in sequence]\n",
    "        return tokenized_sentence\n",
    "    \n",
    "    return tokenize_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.        , 1.91629073],\n",
       "       [2.        , 1.91629073],\n",
       "       [2.        , 1.91629073],\n",
       "       [0.        , 1.91629073],\n",
       "       [0.        , 1.91629073],\n",
       "       [0.        , 1.91629073],\n",
       "       [1.        , 1.22314355],\n",
       "       [0.        , 1.91629073],\n",
       "       [0.        , 1.91629073],\n",
       "       [0.        , 1.91629073],\n",
       "       [0.        , 1.91629073],\n",
       "       [0.        , 1.91629073],\n",
       "       [0.        , 1.91629073],\n",
       "       [0.        , 1.91629073],\n",
       "       [0.        , 1.91629073],\n",
       "       [0.        , 1.51082562],\n",
       "       [0.        , 1.51082562],\n",
       "       [1.        , 1.51082562],\n",
       "       [0.        , 1.91629073],\n",
       "       [0.        , 1.91629073],\n",
       "       [0.        , 1.91629073],\n",
       "       [1.        , 1.22314355],\n",
       "       [0.        , 1.91629073],\n",
       "       [0.        , 1.91629073],\n",
       "       [1.        , 1.51082562],\n",
       "       [3.        , 1.51082562],\n",
       "       [1.        , 1.51082562],\n",
       "       [1.        , 1.51082562],\n",
       "       [3.        , 1.51082562],\n",
       "       [1.        , 1.51082562],\n",
       "       [1.        , 1.22314355],\n",
       "       [1.        , 1.51082562],\n",
       "       [1.        , 1.51082562],\n",
       "       [0.        , 1.51082562],\n",
       "       [0.        , 1.51082562],\n",
       "       [0.        , 1.51082562],\n",
       "       [0.        , 1.91629073],\n",
       "       [2.        , 1.22314355],\n",
       "       [0.        , 1.91629073],\n",
       "       [0.        , 1.91629073],\n",
       "       [0.        , 1.91629073],\n",
       "       [2.        , 1.22314355],\n",
       "       [1.        , 1.91629073],\n",
       "       [0.        , 1.91629073],\n",
       "       [2.        , 1.91629073],\n",
       "       [1.        , 1.91629073],\n",
       "       [2.        , 1.91629073],\n",
       "       [0.        , 1.91629073],\n",
       "       [0.        , 1.91629073],\n",
       "       [2.        , 1.91629073],\n",
       "       [0.        , 1.91629073],\n",
       "       [0.        , 1.91629073],\n",
       "       [1.        , 1.91629073],\n",
       "       [0.        , 1.91629073],\n",
       "       [0.        , 1.91629073],\n",
       "       [2.        , 1.22314355],\n",
       "       [1.        , 1.91629073]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TfIDfExtractor(TextFeatureExtractor):\n",
    "\n",
    "    def __init__(self, text, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "         text: raw document to extract features from\n",
    "         **kwargs:\n",
    "          tokenize_fun: function to use for tokenization\n",
    "        \"\"\"\n",
    "        self.text = tokenize.sent_tokenize(text) # each sentence is considered as a document\n",
    "        self.tokenizer = kwargs['tokenize_fun']\n",
    "        self.vectorizer_tfidf = TfidfVectorizer(tokenizer=self.tokenizer)\n",
    "        self.vectorizer_tf = TfidfVectorizer(tokenizer=self.tokenizer, use_idf=False)\n",
    "        \n",
    "\n",
    "    def extract(self):\n",
    "        \"\"\"\n",
    "        Extraction of Tf and Idf features\n",
    "        Returns: \n",
    "         array containing for each word the category of tf and idf (one feature per column)\n",
    "        \"\"\"\n",
    "        x_tf = self.vectorizer_tf.fit_transform(self.text)\n",
    "        # get word -> index vocabulary\n",
    "        vocabulary = self.vectorizer_tf.vocabulary_\n",
    "\n",
    "        # idf computation\n",
    "        self.vectorizer_tfidf.fit_transform(self.text)\n",
    "        x_idf = self.vectorizer_tfidf.idf_\n",
    "        dict_idf = dict(zip(self.vectorizer_tfidf.get_feature_names(), x_idf))\n",
    "\n",
    "        # conversion of continuous values of tf and idf into categorical ones using five categories\n",
    "        x_tf_categorical = self.__convert_categorical(x_tf)\n",
    "\n",
    "        # generation of pair of vectors containing tf idf category for each word in the document\n",
    "        tf_words = []\n",
    "        idf_words = []\n",
    "        \n",
    "        for i,sentence in enumerate(self.text):\n",
    "            for word in self.tokenizer(sentence):\n",
    "                idf_words.append(dict_idf[word])\n",
    "                tf_words.append(x_tf_categorical[i, vocabulary[word]])\n",
    "\n",
    "        tf = np.reshape(tf_words, (len(tf_words),1))\n",
    "        idf = np.reshape(idf_words, (len(idf_words),1))\n",
    "        return np.concatenate((tf, idf), axis=1)\n",
    "                \n",
    "    def __convert_categorical(self, x):\n",
    "        \"\"\"\n",
    "        Convert continuous tf and idf value into categorical one\n",
    "        Parameters:\n",
    "         x: matrix of continuous values to convert\n",
    "        Return:\n",
    "         converted categorical matrix\n",
    "        \"\"\"\n",
    "        shape = x.shape\n",
    "        x_categorical = np.empty(shape)\n",
    "\n",
    "        # term frequency conversion\n",
    "        for i in range(shape[0]):\n",
    "            for j in range(shape[1]):\n",
    "                if x[i,j] >= 0 and x[i,j] < 0.2:\n",
    "                    x_categorical[i,j] = 0\n",
    "                elif x[i,j] >= 0.2 and x[i,j] < 0.4:\n",
    "                    x_categorical[i,j] = 1\n",
    "                elif x[i,j] >= 0.4 and x[i,j] < 0.6:\n",
    "                    x_categorical[i,j] = 2\n",
    "                elif x[i,j] >= 0.6 and x[i,j] < 0.8:\n",
    "                    x_categorical[i,j] = 3\n",
    "                else:\n",
    "                    x_categorical[i,j] = 4\n",
    "        return x_categorical\n",
    "\n",
    "# Use case\n",
    "\n",
    "document = \"\"\"What's up fellas? So I got a patron supported that wanted me to talk about passive aggressive women like this is a category of women. All right, guys all women a passive-aggressive. All right guys, when a woman won't set a date with you or you know tell you will see or she cancels a date.\"\"\"\n",
    "tokenize_sentence = generate_sentence_tokenizer(document)\n",
    "\n",
    "extractor = TfIDfExtractor(document, tokenize_fun=tokenize_sentence)\n",
    "extractor.extract()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class POSExtractor(TextFeatureExtractor):\n",
    "    \n",
    "    def __init__(self, text, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "         text: document which extracts POS tags from\n",
    "         **kwargs\n",
    "          tokenize_fun: function to use for text tokenization\n",
    "        \"\"\"\n",
    "\n",
    "        self.text = text\n",
    "        # tag set described calling nltk.help.upenn_tagset()\n",
    "        self.postag_set = ['CC', 'CD', 'DT', 'EX', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNS', 'PDT', 'POS',\n",
    "        'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WRB']\n",
    "        self.binarizer = LabelBinarizer().fit(self.postag_set)\n",
    "        self.tokenize = kwargs['tokenize_fun']\n",
    "\n",
    "    def extract(self):\n",
    "        \n",
    "        tokenized_text = self.tokenize(self.text)\n",
    "        pos_tags = nltk.pos_tag(tokenized_text)\n",
    "        tags = [pair[1] for pair in pos_tags]\n",
    "        return self.binarizer.transform(tags)\n",
    "\n",
    "\n",
    "document = \"\"\"What's up fellas? So I got a patron supported that wanted me to talk about passive aggressive women like this is a category of women. All right, guys all women a passive-aggressive. All right guys, when a woman won't set a date with you or you know tell you will see or she cancels a date.\"\"\"\n",
    "\n",
    "extractor = POSExtractor(document, tokenize_fun=tokenize_sentence)\n",
    "extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NERExtractor(TextFeatureExtractor):\n",
    "\n",
    "    def __init__(self, text, **kwargs):\n",
    "        self.text = text\n",
    "        self.tokenizer = kwargs['tokenize_fun']\n",
    "        # types are not directly accessible, found at https://github.com/explosion/spaCy/blob/master/spacy/symbols.pyx\n",
    "        self.types = ['PERSON', 'NORP', 'FACILITY', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART',\n",
    "        'LANGUAGE', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']\n",
    "        self.types.append('NOTHING') # custom type for tokens which do not correspond to any entuty\n",
    "        self.binarizer = LabelBinarizer().fit(self.types)\n",
    "\n",
    "    def extract(self):\n",
    "        \n",
    "        # loading pretrained model\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        doc = nlp(self.text)\n",
    "        ner_types = []\n",
    "        for ent in doc:\n",
    "            \n",
    "            tokenized_element = self.tokenizer(ent.text)\n",
    "            num = len(tokenized_element)\n",
    "\n",
    "            for _ in range(num):\n",
    "                if ent.ent_iob_ == 'O':\n",
    "                    ner_types.append('NOTHING')\n",
    "                else:\n",
    "                    ner_types.append(ent.ent_type_)\n",
    "\n",
    "        return self.binarizer.transform(ner_types)\n",
    "\n",
    "\n",
    "document = \"Apple is looking at buying U.K. startup for $1 billion Apple\"\n",
    "tokenize_sentence = generate_sentence_tokenizer(document)\n",
    "\n",
    "extractor = NERExtractor(document, tokenize_fun=tokenize_sentence)\n",
    "extractor.extract()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7845f4a5515a78a9f91cc9b9f963ac69a8eb4541851448ee62f001dbf9618064"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
