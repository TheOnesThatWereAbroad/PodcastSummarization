{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\boezi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import nltk\n",
    "nltk.download('tagsets')\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk import tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeatureExtractor(ABC):\n",
    "    \"\"\"Abstract class to describe a generic extraction of a feature starting from some text\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self, text, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the extractor with the raw text and other parameters which depend on the extractor\n",
    "        Parameters:\n",
    "        - text: raw input to extract feature from\n",
    "        - **kwargs: dictionary containing specific extractor parameters\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def extract(self):\n",
    "        \"\"\"\n",
    "        Extract the feature from the input text\n",
    "        Return: an array containing the extracted features\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the feature engineering is trying to add other features in addition to the word embeddings to provide more useful information for each word of the document to summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfIDfExtractor(TextFeatureExtractor):\n",
    "\n",
    "    def __init__(self, text, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "         text: raw document to extract td idf features\n",
    "         **kwargs: function to use for tokenization\n",
    "        \"\"\"\n",
    "        self.text = tokenize.sent_tokenize(text)\n",
    "        self.tokenize = kwargs['tokenize_fun']\n",
    "        self.vectorizer_tfidf = TfidfVectorizer(tokenizer=self.tokenize)\n",
    "        self.vectorizer_tf = TfidfVectorizer(tokenizer=self.tokenize, use_idf=False)\n",
    "        \n",
    "\n",
    "    def extract(self):\n",
    "        \"\"\"TODO\"\"\"\n",
    "        x_tf = self.vectorizer_tf.fit_transform(self.text)\n",
    "        # get word -> index vocabulary\n",
    "        vocabulary = self.vectorizer_tf.vocabulary_\n",
    "\n",
    "        self.vectorizer_tfidf.fit_transform(self.text)\n",
    "        x_idf = self.vectorizer_tfidf.idf_\n",
    "        dict_idf = dict(zip(self.vectorizer_tfidf.get_feature_names(), x_idf))\n",
    "        print(dict_idf)\n",
    "\n",
    "        # conversion of continuous values of tf and idf into categorical ones using five categories\n",
    "        x_tf_categorical = self.__convert_categorical(x_tf)\n",
    "\n",
    "        # generation of pair of vectors containing tf idf category for each word in the document\n",
    "        tf_words = []\n",
    "        idf_words = []\n",
    "        \n",
    "        for i,sentence in enumerate(self.text):\n",
    "            for word in self.tokenize(sentence):\n",
    "                idf_words.append(dict_idf[word])\n",
    "                tf_words.append(x_tf_categorical[i, vocabulary[word]])\n",
    "\n",
    "        return tf_words, idf_words\n",
    "                \n",
    "    def __convert_categorical(self, x):\n",
    "        \"\"\"\n",
    "        Convert continuous tf and idf value into categorical one\n",
    "        Parameters:\n",
    "         x: matrix of continuous values to convert\n",
    "        Return:\n",
    "         converted categorical matrix\n",
    "        \"\"\"\n",
    "        shape = x.shape\n",
    "        x_categorical = np.empty(shape)\n",
    "\n",
    "        # term frequency conversion\n",
    "        for i in range(shape[0]):\n",
    "            for j in range(shape[1]):\n",
    "                if x[i,j] >= 0 and x[i,j] < 0.2:\n",
    "                    x_categorical[i,j] = 0\n",
    "                elif x[i,j] >= 0.2 and x[i,j] < 0.4:\n",
    "                    x_categorical[i,j] = 1\n",
    "                elif x[i,j] >= 0.4 and x[i,j] < 0.6:\n",
    "                    x_categorical[i,j] = 2\n",
    "                elif x[i,j] >= 0.6 and x[i,j] < 0.8:\n",
    "                    x_categorical[i,j] = 3\n",
    "                else:\n",
    "                    x_categorical[i,j] = 4\n",
    "        return x_categorical\n",
    "\n",
    "\n",
    "\n",
    "document = \"\"\"What's up fellas? So I got a patron supported that wanted me to talk about passive aggressive women like this is a category of women. All right, guys all women a passive-aggressive. All right guys, when a woman won't set a date with you or you know tell you will see or she cancels a date.\"\"\"\n",
    "extractor = TfIDfExtractor(document)\n",
    "extractor.extract()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tokenize_fun'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-76236c28c064>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WRB']\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mextractor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPOSExtractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpenn_treebank_tagset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-51-76236c28c064>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpostag_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tag_set'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinarizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpostag_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokenize_fun'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tokenize_fun'"
     ]
    }
   ],
   "source": [
    "class POSExtractor(TextFeatureExtractor):\n",
    "    \n",
    "    def __init__(self, text, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "         text: document which extracts POS tags from\n",
    "         **kwargs\n",
    "          tokenize_fun: function to use for text tokenization\n",
    "        \"\"\"\n",
    "\n",
    "        self.text = text\n",
    "        self.postag_set = ['CC', 'CD', 'DT', 'EX', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNS', 'PDT', 'POS',\n",
    "        'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WRB']\n",
    "        self.binarizer = LabelBinarizer().fit(self.postag_set)\n",
    "        self.tokenize = kwargs['tokenize_fun']\n",
    "\n",
    "    def extract(self):\n",
    "        \n",
    "        tokenized_text = self.tokenize(self.text)\n",
    "        pos_tags = nltk.pos_tag(tokenized_text)\n",
    "        tags = [pair[1] for pair in pos_tags]\n",
    "        return self.binarizer.transform(tags)\n",
    "\n",
    "\n",
    "document = \"\"\"What's up fellas? So I got a patron supported that wanted me to talk about passive aggressive women like this is a category of women. All right, guys all women a passive-aggressive. All right guys, when a woman won't set a date with you or you know tell you will see or she cancels a date.\"\"\"\n",
    "\n",
    "penn_treebank_tagset = ['CC', 'CD', 'DT', 'EX', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNS', 'PDT', 'POS',\n",
    "'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WRB']\n",
    "\n",
    "extractor = POSExtractor(document, tag_set=penn_treebank_tagset)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7845f4a5515a78a9f91cc9b9f963ac69a8eb4541851448ee62f001dbf9618064"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
