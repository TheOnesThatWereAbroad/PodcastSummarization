{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\boezi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\boezi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\boezi\\AppData\\Local\\Programs\\Python\\Python37\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import nltk\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk import tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeatureExtractor(ABC):\n",
    "    \"\"\"Abstract class to describe a generic extraction of a feature starting from some text\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self, text, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the extractor with the raw text and other parameters which depend on the extractor\n",
    "        Parameters:\n",
    "        - text: raw input to extract feature from\n",
    "        - **kwargs: dictionary containing specific extractor parameters\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def extract(self):\n",
    "        \"\"\"\n",
    "        Extract the feature from the input text\n",
    "        Return: an array containing the extracted features\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the feature engineering is trying to add other features in addition to the word embeddings to provide more useful information for each word of the document to summarize, these features will be concatenated to the word embeddings of each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the extraction of Tf and Idf the tokenization process is the same of the keras Tokenizer and consists of lowercasing and punctuation stripping unless differently specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('about', 0), ('aggressive', 1), ('all', 2), ('cancels', 3), ('category', 4), ('date', 5), ('fellas', 6), ('got', 7), ('guys', 8), ('is', 9), ('know', 10), ('like', 11), ('me', 12), ('of', 13), ('or', 14), ('passive', 15), ('patron', 16), ('right', 17), ('see', 18), ('set', 19), ('she', 20), ('so', 21), ('supported', 22), ('talk', 23), ('tell', 24), ('that', 25), ('this', 26), ('to', 27), ('up', 28), ('wanted', 29), ('what', 30), ('when', 31), ('will', 32), ('with', 33), ('woman', 34), ('women', 35), ('won', 36), ('you', 37)]\n",
      "0 What's up fellas?\n",
      "[[28, 6]]\n",
      "up\n",
      "fellas\n",
      "1 So I got a patron supported that wanted me to talk about passive aggressive women like this is a category of women.\n",
      "[[21, 7, 16, 22, 25, 29, 12, 27, 23, 0, 15, 1, 35, 11, 26, 9, 4, 13, 35]]\n",
      "so\n",
      "got\n",
      "patron\n",
      "supported\n",
      "that\n",
      "wanted\n",
      "me\n",
      "to\n",
      "talk\n",
      "about\n",
      "passive\n",
      "aggressive\n",
      "women\n",
      "like\n",
      "this\n",
      "is\n",
      "category\n",
      "of\n",
      "women\n",
      "2 All right, guys all women a passive-aggressive.\n",
      "[[2, 17, 8, 2, 35, 15, 1]]\n",
      "all\n",
      "right\n",
      "guys\n",
      "all\n",
      "women\n",
      "passive\n",
      "aggressive\n",
      "3 All right guys, when a woman won't set a date with you or you know tell you will see or she cancels a date.\n",
      "[[2, 17, 8, 31, 34, 19, 5, 33, 37, 14, 37, 10, 24, 37, 32, 18, 14, 20, 3, 5]]\n",
      "all\n",
      "right\n",
      "guys\n",
      "when\n",
      "woman\n",
      "set\n",
      "date\n",
      "with\n",
      "you\n",
      "or\n",
      "you\n",
      "know\n",
      "tell\n",
      "you\n",
      "will\n",
      "see\n",
      "or\n",
      "she\n",
      "cancels\n",
      "date\n",
      "[2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 3.0, 1.0, 1.0, 3.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.        , 2.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 2.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       2.        , 3.        , 1.        , 1.        , 3.        ,\n",
       "       1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "       0.        , 2.        , 1.        , 2.        , 0.        ,\n",
       "       0.        , 2.        , 0.        , 0.        , 1.        ,\n",
       "       0.        , 0.        , 1.        , 1.91629073, 1.91629073,\n",
       "       1.91629073, 1.91629073, 1.91629073, 1.91629073, 1.91629073,\n",
       "       1.91629073, 1.91629073, 1.91629073, 1.91629073, 1.91629073,\n",
       "       1.51082562, 1.51082562, 1.51082562, 1.91629073, 1.91629073,\n",
       "       1.91629073, 1.91629073, 1.91629073, 1.51082562, 1.51082562,\n",
       "       1.51082562, 1.51082562, 1.51082562, 1.51082562, 1.51082562,\n",
       "       1.51082562, 1.51082562, 1.51082562, 1.51082562, 1.91629073,\n",
       "       1.91629073, 1.91629073, 1.91629073, 1.91629073, 1.91629073,\n",
       "       1.91629073, 1.91629073, 1.91629073, 1.91629073, 1.91629073,\n",
       "       1.91629073, 1.91629073, 1.91629073, 1.91629073, 1.91629073,\n",
       "       1.91629073])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TfIDfExtractor(TextFeatureExtractor):\n",
    "\n",
    "    def __init__(self, text, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "         text: raw document to extract features from\n",
    "         **kwargs:\n",
    "          tokenize_fun: function to use for tokenization\n",
    "        \"\"\"\n",
    "        self.text = tokenize.sent_tokenize(text) # each sentence is considered as a document\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.vectorizer_tfidf = TfidfVectorizer()\n",
    "        self.vectorizer_tf = TfidfVectorizer(use_idf=False)\n",
    "        \n",
    "\n",
    "    def extract(self):\n",
    "        \"\"\"\n",
    "        Extraction of Tf and Idf features\n",
    "        Returns: array containing for each word the category of tf and idf\n",
    "        \"\"\"\n",
    "        x_tf = self.vectorizer_tf.fit_transform(self.text)\n",
    "        # get word -> index vocabulary\n",
    "        vocabulary = self.vectorizer_tf.vocabulary_\n",
    "\n",
    "        print(sorted(vocabulary.items()))\n",
    "\n",
    "        # using the same vocabulary\n",
    "        self.tokenizer.word_index = vocabulary\n",
    "        self.tokenizer.index_word = {idx:w for w,idx in self.tokenizer.word_index.items()}\n",
    "\n",
    "        # idf computation\n",
    "        self.vectorizer_tfidf.fit_transform(self.text)\n",
    "        x_idf = self.vectorizer_tfidf.idf_\n",
    "        dict_idf = dict(zip(self.vectorizer_tfidf.get_feature_names(), x_idf))\n",
    "\n",
    "        # conversion of continuous values of tf and idf into categorical ones using five categories\n",
    "        x_tf_categorical = self.__convert_categorical(x_tf)\n",
    "\n",
    "        # generation of pair of vectors containing tf idf category for each word in the document\n",
    "        tf_words = []\n",
    "        idf_words = []\n",
    "        \n",
    "        for i,sentence in enumerate(self.text):\n",
    "            print(i, sentence)\n",
    "            for word in self.__tokenize_sentence(sentence):\n",
    "                idf_words.append(dict_idf[word])\n",
    "                tf_words.append(x_tf_categorical[i, vocabulary[word]])\n",
    "\n",
    "        print(tf_words)\n",
    "\n",
    "        tf = np.array(tf_words).T\n",
    "        idf = np.array(idf_words).T\n",
    "        return np.concatenate((tf, idf), axis=0)\n",
    "                \n",
    "    def __convert_categorical(self, x):\n",
    "        \"\"\"\n",
    "        Convert continuous tf and idf value into categorical one\n",
    "        Parameters:\n",
    "         x: matrix of continuous values to convert\n",
    "        Return:\n",
    "         converted categorical matrix\n",
    "        \"\"\"\n",
    "        shape = x.shape\n",
    "        x_categorical = np.empty(shape)\n",
    "\n",
    "        # term frequency conversion\n",
    "        for i in range(shape[0]):\n",
    "            for j in range(shape[1]):\n",
    "                if x[i,j] >= 0 and x[i,j] < 0.2:\n",
    "                    x_categorical[i,j] = 0\n",
    "                elif x[i,j] >= 0.2 and x[i,j] < 0.4:\n",
    "                    x_categorical[i,j] = 1\n",
    "                elif x[i,j] >= 0.4 and x[i,j] < 0.6:\n",
    "                    x_categorical[i,j] = 2\n",
    "                elif x[i,j] >= 0.6 and x[i,j] < 0.8:\n",
    "                    x_categorical[i,j] = 3\n",
    "                else:\n",
    "                    x_categorical[i,j] = 4\n",
    "        return x_categorical\n",
    "\n",
    "\n",
    "    def __tokenize_sentence(self, sentence):\n",
    "        \"\"\"\n",
    "        Return a list of string tokens starting from a sentence\n",
    "        Parameters:\n",
    "         sentence: sentence to tokenize\n",
    "        Return:\n",
    "         list of tokens\n",
    "        \"\"\"\n",
    "\n",
    "        sequences = self.tokenizer.texts_to_sequences([sentence])\n",
    "        print(sequences)\n",
    "        sequence = sequences[0]\n",
    "        tokenized_sentence = [self.tokenizer.index_word[i] for i in sequence]\n",
    "        return tokenized_sentence\n",
    "\n",
    "\n",
    "\n",
    "document = \"\"\"What's up fellas? So I got a patron supported that wanted me to talk about passive aggressive women like this is a category of women. All right, guys all women a passive-aggressive. All right guys, when a woman won't set a date with you or you know tell you will see or she cancels a date.\"\"\"\n",
    "extractor = TfIDfExtractor(document)\n",
    "extractor.extract()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tokenize_fun'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-76236c28c064>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WRB']\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mextractor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPOSExtractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpenn_treebank_tagset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-51-76236c28c064>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpostag_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tag_set'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinarizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpostag_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokenize_fun'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tokenize_fun'"
     ]
    }
   ],
   "source": [
    "class POSExtractor(TextFeatureExtractor):\n",
    "    \n",
    "    def __init__(self, text, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "         text: document which extracts POS tags from\n",
    "         **kwargs\n",
    "          tokenize_fun: function to use for text tokenization\n",
    "        \"\"\"\n",
    "\n",
    "        self.text = text\n",
    "        self.postag_set = ['CC', 'CD', 'DT', 'EX', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNS', 'PDT', 'POS',\n",
    "        'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WRB']\n",
    "        self.binarizer = LabelBinarizer().fit(self.postag_set)\n",
    "        self.tokenize = kwargs['tokenize_fun']\n",
    "\n",
    "    def extract(self):\n",
    "        \n",
    "        tokenized_text = self.tokenize(self.text)\n",
    "        pos_tags = nltk.pos_tag(tokenized_text)\n",
    "        tags = [pair[1] for pair in pos_tags]\n",
    "        return self.binarizer.transform(tags)\n",
    "\n",
    "\n",
    "document = \"\"\"What's up fellas? So I got a patron supported that wanted me to talk about passive aggressive women like this is a category of women. All right, guys all women a passive-aggressive. All right guys, when a woman won't set a date with you or you know tell you will see or she cancels a date.\"\"\"\n",
    "\n",
    "penn_treebank_tagset = ['CC', 'CD', 'DT', 'EX', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNS', 'PDT', 'POS',\n",
    "'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WRB']\n",
    "\n",
    "extractor = POSExtractor(document)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7845f4a5515a78a9f91cc9b9f963ac69a8eb4541851448ee62f001dbf9618064"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
