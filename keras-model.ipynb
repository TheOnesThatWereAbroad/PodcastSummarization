{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "SEQUENCE_LENGTH = 250\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(os.path.abspath(\"\"), 'podcasts-no-audio-13GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:  Index(['show_uri', 'show_name', 'show_description', 'publisher', 'language',\n",
      "       'rss_link', 'episode_uri', 'episode_name', 'episode_description',\n",
      "       'duration', 'show_filename_prefix', 'episode_filename_prefix'],\n",
      "      dtype='object')\n",
      "Shape:  (105360, 12)\n"
     ]
    }
   ],
   "source": [
    "metadata_path_train = os.path.join(dataset_path, 'metadata.tsv')\n",
    "metadata_train = pd.read_csv(metadata_path_train, sep='\\t')\n",
    "print(\"Columns: \", metadata_train.columns)\n",
    "print(\"Shape: \", metadata_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show_uri                                 spotify:show:2NYtxEZyYelR6RMKmjfPLB\n",
      "show_name                                               Kream in your Koffee\n",
      "show_description           A 20-something blunt female takes on the world...\n",
      "publisher                                                        Katie Houle\n",
      "language                                                              ['en']\n",
      "rss_link                            https://anchor.fm/s/11b84b68/podcast/rss\n",
      "episode_uri                           spotify:episode:000A9sRBYdVh66csG2qEdj\n",
      "episode_name                                         1: It’s Christmas Time!\n",
      "episode_description        On the first ever episode of Kream in your Kof...\n",
      "duration                                                           12.700133\n",
      "show_filename_prefix                             show_2NYtxEZyYelR6RMKmjfPLB\n",
      "episode_filename_prefix                               000A9sRBYdVh66csG2qEdj\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Copy this uri into the browser to listen to the episode:\n",
      " spotify:episode:000A9sRBYdVh66csG2qEdj\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "episode_example_train = metadata_train.iloc[i]\n",
    "print(episode_example_train)\n",
    "print(\"\\nCopy this uri into the browser to listen to the episode:\\n\",\n",
    "      episode_example_train['episode_uri'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(episode):\n",
    "    # extract the 2 reference number/letter to access the episode transcript\n",
    "    show_filename = episode['show_filename_prefix']\n",
    "    episode_filename = episode['episode_filename_prefix'] + \".json\"\n",
    "    dir_1, dir_2 = re.match(r'show_(\\d)(\\w).*', show_filename).groups()\n",
    "\n",
    "    interval_folders = [range(0, 3), range(3, 6), range(6, 8)]\n",
    "\n",
    "    # check which is the main folder containing the transcript\n",
    "    main_dir = \"\"\n",
    "    for interval in interval_folders:\n",
    "        if int(dir_1) in interval:\n",
    "            main_dir = \"podcasts-transcripts-{}to{}\".format(interval[0],\n",
    "                                                            interval[-1])\n",
    "    assert main_dir != \"\"\n",
    "\n",
    "    # check if the transcript file in all the derived subfolders exist\n",
    "    transcipt_path = os.path.join(dataset_path, \"spotify-podcasts-2020\",\n",
    "                                \"podcasts-transcripts\", dir_1, dir_2,\n",
    "                                show_filename, episode_filename)\n",
    "\n",
    "    return transcipt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while not os.path.isfile(get_path(metadata_train.iloc[i])):\n",
    "    i = random.randint(0,10000)\n",
    "    print(get_path(metadata_train.iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "show_uri                                 spotify:show:2NYtxEZyYelR6RMKmjfPLB\n",
       "show_name                                               Kream in your Koffee\n",
       "show_description           A 20-something blunt female takes on the world...\n",
       "publisher                                                        Katie Houle\n",
       "language                                                              ['en']\n",
       "rss_link                            https://anchor.fm/s/11b84b68/podcast/rss\n",
       "episode_uri                           spotify:episode:000A9sRBYdVh66csG2qEdj\n",
       "episode_name                                         1: It’s Christmas Time!\n",
       "episode_description        On the first ever episode of Kream in your Kof...\n",
       "duration                                                           12.700133\n",
       "show_filename_prefix                             show_2NYtxEZyYelR6RMKmjfPLB\n",
       "episode_filename_prefix                               000A9sRBYdVh66csG2qEdj\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode = metadata_train.iloc[i]\n",
    "episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcription(episode):\n",
    "    with open(get_path(episode), 'r') as f:\n",
    "        episode_json = json.load(f)\n",
    "        # seems that the last result in each trastcript is a repetition of the first one, so we ignore it\n",
    "        transcripts = [\n",
    "            result[\"alternatives\"][0]['transcript'] if 'transcript' in result[\"alternatives\"][0] else \"\"\n",
    "            for result in episode_json[\"results\"][:-1]\n",
    "        ]\n",
    "        return \" \".join(transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 None\n",
      "1 nan\n",
      "2 nan\n",
      "3 nan\n",
      "4 nan\n",
      "5 nan\n",
      "6 nan\n",
      "7 nan\n",
      "8 nan\n",
      "9 nan\n"
     ]
    }
   ],
   "source": [
    "link_removal_pattern = re.compile(r\"[\\w\\s:\\-\\p{So}]*((https:|www\\.).*)$\")\n",
    "\n",
    "metadata_train.show_description = metadata_train.iloc[:10].show_description.apply(lambda desc: link_removal_pattern.sub(\"\", str(desc)))\n",
    "for row, val in metadata_train.show_description.iloc[:10].iteritems():\n",
    "    print(row, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 12:25:45.613860: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "strip_chars = string.punctuation\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    no_punct = tf.strings.regex_replace(lowercase,\n",
    "                                        \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "    no_links = tf.strings.regex_replace(no_punct,\n",
    "                                        \"[\\w\\s:\\-\\p{So}]*((https:|www\\.).*)$\", \"\")\n",
    "    return no_links\n",
    "\n",
    "X_vectorizer = keras.layers.TextVectorization(max_tokens=VOCAB_SIZE, output_mode='int', output_sequence_length=SEQUENCE_LENGTH)\n",
    "y_vectorizer = keras.layers.TextVectorization(max_tokens=VOCAB_SIZE, output_mode='int', output_sequence_length=SEQUENCE_LENGTH + 1, standardize=custom_standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "for _, row in metadata_train.iloc[:2000].iterrows():\n",
    "    if os.path.isfile(get_path(row)) and type(row['episode_description'])==str:\n",
    "        X.append(get_transcription(row))\n",
    "        y.append(\"[start]\"+row['episode_description']+\"[end]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_strings(transcription, summary):\n",
    "    transcription = X_vectorizer(transcription)\n",
    "    summary = y_vectorizer(summary)\n",
    "    return ({\"encoder_inputs\": transcription, \"decoder_inputs\": summary[:, :-1],}, summary[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vectorizer.adapt(X)\n",
    "y_vectorizer.adapt(y)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((np.array(X),np.array(y)))\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.map(format_strings)\n",
    "dataset = dataset.shuffle(2048).prefetch(16).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'encoder_inputs': <tf.Tensor: shape=(32, 250), dtype=int64, numpy=\n",
      "array([[ 463,    5,    2, ...,    0,    0,    0],\n",
      "       [1530,    5,    2, ...,    2, 8389, 1332],\n",
      "       [  78,  135,  267, ...,    4,   63,    5],\n",
      "       ...,\n",
      "       [ 769,    3,  463, ...,  737,    7, 6372],\n",
      "       [ 242, 9909,  160, ..., 2398,   11,   45],\n",
      "       [  66,  769,  769, ...,    0,    0,    0]])>, 'decoder_inputs': <tf.Tensor: shape=(32, 250), dtype=int64, numpy=\n",
      "array([[  90,    8,  566, ...,    0,    0,    0],\n",
      "       [3311,   54,  150, ...,    0,    0,    0],\n",
      "       [ 473,  765,  238, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [   1, 5059,    1, ...,    0,    0,    0],\n",
      "       [2447,   98,    3, ...,    0,    0,    0],\n",
      "       [ 657, 1926, 1237, ...,    0,    0,    0]])>}, <tf.Tensor: shape=(32, 250), dtype=int64, numpy=\n",
      "array([[   8,  566,    1, ...,    0,    0,    0],\n",
      "       [  54,  150, 1889, ...,    0,    0,    0],\n",
      "       [ 765,  238,   50, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [5059,    1,    4, ...,    0,    0,    0],\n",
      "       [  98,    3,    2, ...,    0,    0,    0],\n",
      "       [1926, 1237,    6, ...,    0,    0,    0]])>)\n"
     ]
    }
   ],
   "source": [
    "for element in dataset:\n",
    "    print(element)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [keras.layers.Dense(dense_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = keras.layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.token_embeddings = keras.layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = keras.layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoder(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [keras.layers.Dense(latent_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = keras.layers.LayerNormalization()\n",
    "        self.layernorm_3 = keras.layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "latent_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,),\n",
    "                             dtype=\"int64\",\n",
    "                             name=\"encoder_inputs\")\n",
    "x = PositionalEmbedding(SEQUENCE_LENGTH, VOCAB_SIZE, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,),\n",
    "                             dtype=\"int64\",\n",
    "                             name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim),\n",
    "                                 name=\"decoder_state_inputs\")\n",
    "x = PositionalEmbedding(SEQUENCE_LENGTH, VOCAB_SIZE, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "decoder_outputs = keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs, name=\"decoder\")\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs],\n",
    "                          decoder_outputs,\n",
    "                          name=\"transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding (Position  (None, None, 256)   2624000     ['encoder_inputs[0][0]']         \n",
      " alEmbedding)                                                                                     \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder (Transform  (None, None, 256)   3155456     ['positional_embedding[0][0]']   \n",
      " erEncoder)                                                                                       \n",
      "                                                                                                  \n",
      " decoder (Functional)           (None, None, 10000)  10453520    ['decoder_inputs[0][0]',         \n",
      "                                                                  'transformer_encoder[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16,232,976\n",
      "Trainable params: 16,232,976\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 400s 6s/step - loss: 2.1581 - accuracy: 0.0656\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 458s 7s/step - loss: 1.9765 - accuracy: 0.1318\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 379s 6s/step - loss: 1.8163 - accuracy: 0.1857\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 299s 5s/step - loss: 1.6937 - accuracy: 0.2154\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 293s 5s/step - loss: 1.6318 - accuracy: 0.2310\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 287s 5s/step - loss: 1.5761 - accuracy: 0.2443\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 285s 5s/step - loss: 1.5264 - accuracy: 0.2561\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 283s 4s/step - loss: 1.4826 - accuracy: 0.2649\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 284s 5s/step - loss: 1.4435 - accuracy: 0.2751\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 286s 5s/step - loss: 1.4059 - accuracy: 0.2837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fda53dfe610>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "transformer.summary()\n",
    "transformer.compile(\"rmsprop\",\n",
    "                    loss=\"sparse_categorical_crossentropy\",\n",
    "                    metrics=[\"accuracy\"])\n",
    "transformer.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n",
      "[start] [end]\n"
     ]
    }
   ],
   "source": [
    "y_vocab = y_vectorizer.get_vocabulary()\n",
    "y_index_lookup = dict(zip(range(len(y_vocab)), y_vocab))\n",
    "max_decoded_sentence_length = 30\n",
    "\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = X_vectorizer([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = y_vectorizer([decoded_sentence\n",
    "                                                      ])[:, :-1]\n",
    "        predictions = transformer(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\n",
    "\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = y_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for _ in range(30):\n",
    "    input_sentence = random.choice(X)\n",
    "    summarized = decode_sequence(input_sentence)\n",
    "    print(summarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63e28586807c6502c782d898cf9a0cc5787bb3d77952b17b51ec8bdcd4044a3d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
