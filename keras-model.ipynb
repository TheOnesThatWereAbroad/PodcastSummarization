{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import regex as re\n",
    "from urllib import request\n",
    "import zipfile\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "SEQUENCE_LENGTH = 2500\n",
    "BATCH_SIZE = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(os.path.abspath(\"\"), 'podcasts-no-audio-13GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:  Index(['show_uri', 'show_name', 'show_description', 'publisher', 'language',\n",
      "       'rss_link', 'episode_uri', 'episode_name', 'episode_description',\n",
      "       'duration', 'show_filename_prefix', 'episode_filename_prefix'],\n",
      "      dtype='object')\n",
      "Shape:  (105360, 12)\n"
     ]
    }
   ],
   "source": [
    "metadata_path_train = os.path.join(dataset_path, 'metadata.tsv')\n",
    "metadata_train = pd.read_csv(metadata_path_train, sep='\\t')\n",
    "print(\"Columns: \", metadata_train.columns)\n",
    "print(\"Shape: \", metadata_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show_uri                                 spotify:show:2NYtxEZyYelR6RMKmjfPLB\n",
      "show_name                                               Kream in your Koffee\n",
      "show_description           A 20-something blunt female takes on the world...\n",
      "publisher                                                        Katie Houle\n",
      "language                                                              ['en']\n",
      "rss_link                            https://anchor.fm/s/11b84b68/podcast/rss\n",
      "episode_uri                           spotify:episode:000A9sRBYdVh66csG2qEdj\n",
      "episode_name                                         1: Itâ€™s Christmas Time!\n",
      "episode_description        On the first ever episode of Kream in your Kof...\n",
      "duration                                                           12.700133\n",
      "show_filename_prefix                             show_2NYtxEZyYelR6RMKmjfPLB\n",
      "episode_filename_prefix                               000A9sRBYdVh66csG2qEdj\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Copy this uri into the browser to listen to the episode:\n",
      " spotify:episode:000A9sRBYdVh66csG2qEdj\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "episode_example_train = metadata_train.iloc[i]\n",
    "print(episode_example_train)\n",
    "print(\"\\nCopy this uri into the browser to listen to the episode:\\n\",\n",
    "      episode_example_train['episode_uri'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(episode):\n",
    "    # extract the 2 reference number/letter to access the episode transcript\n",
    "    show_filename = episode['show_filename_prefix']\n",
    "    episode_filename = episode['episode_filename_prefix'] + \".json\"\n",
    "    dir_1, dir_2 = re.match(r'show_(\\d)(\\w).*', show_filename).groups()\n",
    "\n",
    "    interval_folders = [range(0, 3), range(3, 6), range(6, 8)]\n",
    "\n",
    "    # check which is the main folder containing the transcript\n",
    "    main_dir = \"\"\n",
    "    for interval in interval_folders:\n",
    "        if int(dir_1) in interval:\n",
    "            main_dir = \"podcasts-transcripts-{}to{}\".format(interval[0],\n",
    "                                                            interval[-1])\n",
    "    assert main_dir != \"\"\n",
    "\n",
    "    # check if the transcript file in all the derived subfolders exist\n",
    "    transcipt_path = os.path.join(dataset_path, \"spotify-podcasts-2020\",\n",
    "                                \"podcasts-transcripts\", dir_1, dir_2,\n",
    "                                show_filename, episode_filename)\n",
    "\n",
    "    return transcipt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while not os.path.isfile(get_path(metadata_train.iloc[i])):\n",
    "    i = random.randint(0,10000)\n",
    "    print(get_path(metadata_train.iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "show_uri                                 spotify:show:2NYtxEZyYelR6RMKmjfPLB\n",
       "show_name                                               Kream in your Koffee\n",
       "show_description           A 20-something blunt female takes on the world...\n",
       "publisher                                                        Katie Houle\n",
       "language                                                              ['en']\n",
       "rss_link                            https://anchor.fm/s/11b84b68/podcast/rss\n",
       "episode_uri                           spotify:episode:000A9sRBYdVh66csG2qEdj\n",
       "episode_name                                         1: Itâ€™s Christmas Time!\n",
       "episode_description        On the first ever episode of Kream in your Kof...\n",
       "duration                                                           12.700133\n",
       "show_filename_prefix                             show_2NYtxEZyYelR6RMKmjfPLB\n",
       "episode_filename_prefix                               000A9sRBYdVh66csG2qEdj\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode = metadata_train.iloc[i]\n",
    "episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcription(episode):\n",
    "    with open(get_path(episode), 'r') as f:\n",
    "        episode_json = json.load(f)\n",
    "        # seems that the last result in each trastcript is a repetition of the first one, so we ignore it\n",
    "        transcripts = [\n",
    "            result[\"alternatives\"][0]['transcript'] if 'transcript' in result[\"alternatives\"][0] else \"\"\n",
    "            for result in episode_json[\"results\"][:-1]\n",
    "        ]\n",
    "        return \" \".join(transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 A 20-something blunt female takes on the world and gives you her take on it. Enjoy visits from special guests and friends to give insight and input into interesting situations.\n",
      "1 Ever wonder what murder took place on today in true crime history? If so, sit back and grab a cup of coffee as you enjoy your daily dose of TC goodness. Your host, Korina Biemesderfer, guides you through history with tales of murder, abduction, serial killers, crimes of passion, cults and more in this short form daily true crime podcast.\n",
      "2 Inside the 18 is your source for all things Goalkeeping! Each week we are joined by guests from around the world. WeÂ recap the weeks events,Â discuss new training techniquesÂ and have candid conversations with professional goalkeepers and goalkeeper coaches. The show is a must listen for the goalkeeping enthusiast!    #insidethe18 #goalkeeperpodcast #thegoalkeepers #goalkicks #ederson #NWSL #ashlynharris #UWSNT #MLS #USMNT #goalkeepercoaches #degea #neuer #navas #areola #oblak #USL #goalkeepersaves #courtois #technefutbol #gkunion #gk #progk #goalkeepertraining #soccer #futbol #messi #ronaldo\n",
      "3 Your favorite podcast for everything @Chiefs! Providing #ChiefsKingdom with up to the minute news & insight all day, everyday with @KC_Winn & @grant_morse7ðŸŽ™\n",
      "4 The comedy podcast about toxic characters, writers, and tropes of literature, folklore, myths, and legend.   Join host Emily Edwards to discuss feminist literature, toxic masculinity, gender roles, and intersectional representation in books. These are the Fuckbois of Literature.\n",
      "5 Podcasts useful for UPSC aspirants! Mainly discussion aired on Rajya Sabha TV and All India Radio. Subscribe now!\n",
      "6 Enter the world of dominant women and submissive sissies in these exciting stories of sissification and humiliation written by Kylie Gable and performed by a host of sexy and talented readers.\n",
      "7 We are four, 30 somethings living in a city showing what it means to live Chastity, Friendship and the Catholic Faith. \n",
      "8 Get ready to whiten those knuckles and hold fast as we get underway to talk with Coast Guard veterans about the most daring, dangerous, and epic sea stories ever told. Whether facing ruthless men who prey on other mariners or storms that turn calm seas into graveyards, those who go down to the sea and cast off lines enter the most challenging and dangerous environment on earth. Only here will you hear their stories and the lessons gained through their experience.\n",
      "9 Letâ€™s be real and honest! Life can be challenging and stressful! I am a mom, a teacher, a life coach and a motivational speaker. Meeting so many people from so many places has made me realize that we are all on a similar quest for happiness and inspiration. This podcast will be uplifting, honest and damn funny! Join me each Monday night as me and my guests impart attainable goals and strategies that will uplift you and make you smile. \n"
     ]
    }
   ],
   "source": [
    "link_removal_pattern = re.compile(\n",
    "    r\"([\\w\\s:\\-\\p{So}]*((https:|www\\.).*)$|---.*$)\")\n",
    "\n",
    "metadata_train.show_description = metadata_train.iloc[:10].show_description.apply(lambda desc: link_removal_pattern.sub(\"\", str(desc)))\n",
    "for row, val in metadata_train.show_description.iloc[:10].iteritems():\n",
    "    print(row, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingMatrix():\n",
    "    \"\"\"Generates an embedding matrix using GloVE, given a vocabulary/\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        glove_url=\"http://nlp.stanford.edu/data/glove.6B.zip\",\n",
    "        embedding_dim=100,\n",
    "        embedding_folder=\"glove\"\n",
    "    ):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.download_glove_if_needed(\n",
    "            glove_url=glove_url, embedding_folder=embedding_folder\n",
    "        )\n",
    "\n",
    "        # create the embeddings vocabulary\n",
    "        self.glove_dict = self.parse_glove(embedding_folder)\n",
    "\n",
    "    def download_glove_if_needed(self, glove_url, embedding_folder):\n",
    "        \"\"\"\n",
    "        Downloads the glove embeddings from the internet\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        glove_url : The url of the GloVe embeddings.\n",
    "        embedding_folder: folder where the embedding will be downloaded\n",
    "        \"\"\"\n",
    "        # create embedding folder if it does not exist\n",
    "        if not os.path.exists(embedding_folder):\n",
    "            os.makedirs(embedding_folder)\n",
    "\n",
    "        # extract the embedding if it is not extracted\n",
    "        if not glob.glob(\n",
    "            os.path.join(embedding_folder, \"**/glove*.txt\"), recursive=True\n",
    "        ):\n",
    "\n",
    "            # download the embedding if it does not exist\n",
    "            embedding_zip = os.path.join(embedding_folder, glove_url.split(\"/\")[-1])\n",
    "            if not os.path.exists(embedding_zip):\n",
    "                print(\"Downloading the GloVe embeddings...\")\n",
    "                request.urlretrieve(glove_url, embedding_zip)\n",
    "                print(\"Successful download!\")\n",
    "\n",
    "            # extract the embedding\n",
    "            print(\"Extracting the embeddings...\")\n",
    "            with zipfile.ZipFile(embedding_zip, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(embedding_folder)\n",
    "                print(\"Successfully extracted the embeddings!\")\n",
    "            os.remove(embedding_zip)\n",
    "\n",
    "    def parse_glove(self, embedding_folder):\n",
    "        \"\"\"\n",
    "        Parses the GloVe embeddings from their files, filling the vocabulary.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        embedding_folder : folder where the embedding files are stored\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dictionary representing the vocabulary from the embeddings\n",
    "        \"\"\"\n",
    "        print(\"Creating glove vocabulary...\")\n",
    "        vocabulary = {\"<pad>\": np.zeros(self.embedding_dim)}\n",
    "        embedding_file = os.path.join(\n",
    "            embedding_folder, \"glove.6B.\" + str(self.embedding_dim) + \"d.txt\"\n",
    "        )\n",
    "        with open(embedding_file, encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                word, coefs = line.split(maxsplit=1)\n",
    "                coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "                vocabulary[word] = coefs\n",
    "        return vocabulary\n",
    "\n",
    "    def create_embedding_matrix(self, vocabulary):\n",
    "        \"\"\"\n",
    "        Creates the embedding matrix from the vocabulary.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocabulary : dictionary representing the vocabulary from the vectorizer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        embedding_matrix : numpy array representing the embedding matrix\n",
    "        \"\"\"\n",
    "        print(\"Creating embedding matrix...\")\n",
    "        embedding_matrix = np.zeros((len(vocabulary), self.embedding_dim))\n",
    "        for i, word in enumerate(vocabulary):\n",
    "            if word in self.glove_dict:\n",
    "                embedding_matrix[i] = self.glove_dict[word]\n",
    "            elif word not in [\"\", \"[UNK]\"]:\n",
    "                embedding_matrix[i] = np.random.uniform(size=self.embedding_dim)\n",
    "        return np.array(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 15:42:23.550139: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "strip_chars = string.punctuation\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    no_punct = tf.strings.regex_replace(lowercase,\n",
    "                                        \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "    no_links = tf.strings.regex_replace(no_punct,\n",
    "                                        \"[\\w\\s:\\-\\p{So}]*((https:|www\\.).*)$\", \"\")\n",
    "    return no_links\n",
    "\n",
    "X_vectorizer = keras.layers.TextVectorization(max_tokens=VOCAB_SIZE, output_mode='int', output_sequence_length=SEQUENCE_LENGTH)\n",
    "y_vectorizer = keras.layers.TextVectorization(max_tokens=VOCAB_SIZE, output_mode='int', output_sequence_length=SEQUENCE_LENGTH + 1, standardize=custom_standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "for _, row in metadata_train.iloc[:2000].iterrows():\n",
    "    if os.path.isfile(get_path(row)) and type(row['episode_description'])==str:\n",
    "        X.append(get_transcription(row))\n",
    "        y.append(\"[start]\"+row['episode_description']+\"[end]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_strings(transcription, summary):\n",
    "    transcription = X_vectorizer(transcription)\n",
    "    summary = y_vectorizer(summary)\n",
    "    return ({\"encoder_inputs\": transcription, \"decoder_inputs\": summary[:, :-1],}, summary[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vectorizer.adapt(X)\n",
    "y_vectorizer.adapt(y)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((np.array(X),np.array(y)))\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.map(format_strings)\n",
    "dataset = dataset.shuffle(2048).prefetch(16).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [keras.layers.Dense(dense_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = keras.layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, token_embedding_matrix, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.token_embeddings = keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                                       output_dim=embed_dim,\n",
    "                                                       weights=[token_embedding_matrix])\n",
    "        self.position_embeddings = keras.layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoder(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [keras.layers.Dense(latent_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = keras.layers.LayerNormalization()\n",
    "        self.layernorm_3 = keras.layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating glove vocabulary...\n",
      "Creating embedding matrix...\n",
      "Creating embedding matrix...\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 300\n",
    "latent_dim = 2048\n",
    "num_heads = 8\n",
    "e = EmbeddingMatrix(embedding_dim=embed_dim)\n",
    "\n",
    "X_embedding_matrix = e.create_embedding_matrix(\n",
    "    vocabulary=X_vectorizer.get_vocabulary())\n",
    "y_embedding_matrix = e.create_embedding_matrix(\n",
    "    vocabulary=y_vectorizer.get_vocabulary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = keras.Input(shape=(None,),\n",
    "                             dtype=\"int64\",\n",
    "                             name=\"encoder_inputs\")\n",
    "x = PositionalEmbedding(SEQUENCE_LENGTH, VOCAB_SIZE, embed_dim, X_embedding_matrix)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,),\n",
    "                             dtype=\"int64\",\n",
    "                             name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim),\n",
    "                                 name=\"decoder_state_inputs\")\n",
    "x = PositionalEmbedding(SEQUENCE_LENGTH, VOCAB_SIZE, embed_dim, X_embedding_matrix)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "decoder_outputs = keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs, name=\"decoder\")\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs],\n",
    "                          decoder_outputs,\n",
    "                          name=\"transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding (Position  (None, None, 300)   3750000     ['encoder_inputs[0][0]']         \n",
      " alEmbedding)                                                                                     \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder (Transform  (None, None, 300)   4119848     ['positional_embedding[0][0]']   \n",
      " erEncoder)                                                                                       \n",
      "                                                                                                  \n",
      " decoder (Functional)           (None, None, 10000)  13767948    ['decoder_inputs[0][0]',         \n",
      "                                                                  'transformer_encoder[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,637,796\n",
      "Trainable params: 21,637,796\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      " 41/167 [======>.......................] - ETA: 3:00:52 - loss: 0.2343 - accuracy: 0.0597"
     ]
    }
   ],
   "source": [
    "\n",
    "transformer.summary()\n",
    "transformer.compile(\"rmsprop\",\n",
    "                    loss=\"sparse_categorical_crossentropy\",\n",
    "                    metrics=[\"accuracy\"])\n",
    "transformer.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[start] and the and the and the and the and the and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "[start] and the and the and the and the and the and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "[start] and the and the and the and the and the and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "[start] and the and the and the and the and the and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "[start] and the and the and the and the and the and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n"
     ]
    }
   ],
   "source": [
    "y_vocab = y_vectorizer.get_vocabulary()\n",
    "y_index_lookup = dict(zip(range(len(y_vocab)), y_vocab))\n",
    "max_decoded_sentence_length = 30\n",
    "min_length = 10\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = X_vectorizer([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = y_vectorizer([decoded_sentence\n",
    "                                                      ])[:, :-1]\n",
    "        predictions = transformer(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(\n",
    "            predictions[0,\n",
    "                        i, :]) if i >min_length else np.argsort(predictions[0,\n",
    "                                                                     i, :])[-2] # Cannot take [end] right after [start]\n",
    "        sampled_token = y_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for _ in range(5):\n",
    "    input_sentence = random.choice(X)\n",
    "    summarized = decode_sequence(input_sentence)\n",
    "    print(summarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63e28586807c6502c782d898cf9a0cc5787bb3d77952b17b51ec8bdcd4044a3d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
