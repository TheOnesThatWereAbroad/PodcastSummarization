{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "SEQUENCE_LENGTH = 250\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(os.path.abspath(\"\"), 'podcasts-no-audio-13GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:  Index(['show_uri', 'show_name', 'show_description', 'publisher', 'language',\n",
      "       'rss_link', 'episode_uri', 'episode_name', 'episode_description',\n",
      "       'duration', 'show_filename_prefix', 'episode_filename_prefix'],\n",
      "      dtype='object')\n",
      "Shape:  (105360, 12)\n"
     ]
    }
   ],
   "source": [
    "metadata_path_train = os.path.join(dataset_path, 'metadata.tsv')\n",
    "metadata_train = pd.read_csv(metadata_path_train, sep='\\t')\n",
    "print(\"Columns: \", metadata_train.columns)\n",
    "print(\"Shape: \", metadata_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show_uri                                 spotify:show:2NYtxEZyYelR6RMKmjfPLB\n",
      "show_name                                               Kream in your Koffee\n",
      "show_description           A 20-something blunt female takes on the world...\n",
      "publisher                                                        Katie Houle\n",
      "language                                                              ['en']\n",
      "rss_link                            https://anchor.fm/s/11b84b68/podcast/rss\n",
      "episode_uri                           spotify:episode:000A9sRBYdVh66csG2qEdj\n",
      "episode_name                                         1: It’s Christmas Time!\n",
      "episode_description        On the first ever episode of Kream in your Kof...\n",
      "duration                                                           12.700133\n",
      "show_filename_prefix                             show_2NYtxEZyYelR6RMKmjfPLB\n",
      "episode_filename_prefix                               000A9sRBYdVh66csG2qEdj\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Copy this uri into the browser to listen to the episode:\n",
      " spotify:episode:000A9sRBYdVh66csG2qEdj\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "episode_example_train = metadata_train.iloc[i]\n",
    "print(episode_example_train)\n",
    "print(\"\\nCopy this uri into the browser to listen to the episode:\\n\",\n",
    "      episode_example_train['episode_uri'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(episode):\n",
    "    # extract the 2 reference number/letter to access the episode transcript\n",
    "    show_filename = episode['show_filename_prefix']\n",
    "    episode_filename = episode['episode_filename_prefix'] + \".json\"\n",
    "    dir_1, dir_2 = re.match(r'show_(\\d)(\\w).*', show_filename).groups()\n",
    "\n",
    "    interval_folders = [range(0, 3), range(3, 6), range(6, 8)]\n",
    "\n",
    "    # check which is the main folder containing the transcript\n",
    "    main_dir = \"\"\n",
    "    for interval in interval_folders:\n",
    "        if int(dir_1) in interval:\n",
    "            main_dir = \"podcasts-transcripts-{}to{}\".format(interval[0],\n",
    "                                                            interval[-1])\n",
    "    assert main_dir != \"\"\n",
    "\n",
    "    # check if the transcript file in all the derived subfolders exist\n",
    "    transcipt_path = os.path.join(dataset_path, main_dir, \"spotify-podcasts-2020\",\n",
    "                                \"podcasts-transcripts\", dir_1, dir_2,\n",
    "                                show_filename, episode_filename)\n",
    "\n",
    "    return transcipt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/simone/UniBO/Lab/NLP/PodcastSummarization/podcasts-no-audio-13GB/podcasts-transcripts-0to2/spotify-podcasts-2020/podcasts-transcripts/0/I/show_0I22C9iyvVT3M6DEILuD9F/0CNXKXYlbvZe0aup8UK4wJ.json\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while not os.path.isfile(get_path(metadata_train.iloc[i])):\n",
    "    i = random.randint(0,10000)\n",
    "    print(get_path(metadata_train.iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "show_uri                                 spotify:show:0I22C9iyvVT3M6DEILuD9F\n",
       "show_name                                           The Orthobullets Podcast\n",
       "show_description           A Daily High-Yield review podcast by Orthobull...\n",
       "publisher                                                       Orthobullets\n",
       "language                                                              ['en']\n",
       "rss_link                             https://anchor.fm/s/415caa8/podcast/rss\n",
       "episode_uri                           spotify:episode:0CNXKXYlbvZe0aup8UK4wJ\n",
       "episode_name               Question Session⎪Traumatic Anterior Shoulder I...\n",
       "episode_description        In this episode, we review the high-yield topi...\n",
       "duration                                                           19.181717\n",
       "show_filename_prefix                             show_0I22C9iyvVT3M6DEILuD9F\n",
       "episode_filename_prefix                               0CNXKXYlbvZe0aup8UK4wJ\n",
       "Name: 2725, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode = metadata_train.iloc[i]\n",
    "episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcription(episode):\n",
    "    with open(get_path(episode), 'r') as f:\n",
    "        episode_json = json.load(f)\n",
    "        # seems that the last result in each trastcript is a repetition of the first one, so we ignore it\n",
    "        transcripts = [\n",
    "            result[\"alternatives\"][0]['transcript'] if 'transcript' in result[\"alternatives\"][0] else \"\"\n",
    "            for result in episode_json[\"results\"][:-1]\n",
    "        ]\n",
    "        return \" \".join(transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-19 17:31:19.257275: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "strip_chars = string.punctuation\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "X_vectorizer = keras.layers.TextVectorization(max_tokens=VOCAB_SIZE, output_mode='int', output_sequence_length=SEQUENCE_LENGTH)\n",
    "y_vectorizer = keras.layers.TextVectorization(max_tokens=VOCAB_SIZE, output_mode='int', output_sequence_length=SEQUENCE_LENGTH + 1, standardize=custom_standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "for _, row in metadata_train.iloc[:1000].iterrows():\n",
    "    if os.path.isfile(get_path(row)) and type(row['episode_description'])==str:\n",
    "        X.append(get_transcription(row))\n",
    "        y.append(\"[start]\"+row['episode_description']+\"[end]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_strings(transcription, summary):\n",
    "    transcription = X_vectorizer(transcription)\n",
    "    summary = y_vectorizer(summary)\n",
    "    return ({\"encoder_inputs\": transcription, \"decoder_inputs\": summary[:, :-1],}, summary[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vectorizer.adapt(X)\n",
    "y_vectorizer.adapt(y)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((np.array(X),np.array(y)))\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.map(format_strings)\n",
    "dataset = dataset.shuffle(2048).prefetch(16).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [keras.layers.Dense(dense_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = keras.layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.token_embeddings = keras.layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = keras.layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoder(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [keras.layers.Dense(latent_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = keras.layers.LayerNormalization()\n",
    "        self.layernorm_3 = keras.layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "latent_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,),\n",
    "                             dtype=\"int64\",\n",
    "                             name=\"encoder_inputs\")\n",
    "x = PositionalEmbedding(SEQUENCE_LENGTH, VOCAB_SIZE, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,),\n",
    "                             dtype=\"int64\",\n",
    "                             name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim),\n",
    "                                 name=\"decoder_state_inputs\")\n",
    "x = PositionalEmbedding(SEQUENCE_LENGTH, VOCAB_SIZE, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "decoder_outputs = keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs, name=\"decoder\")\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs],\n",
    "                          decoder_outputs,\n",
    "                          name=\"transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding (Position  (None, None, 256)   2624000     ['encoder_inputs[0][0]']         \n",
      " alEmbedding)                                                                                     \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder (Transform  (None, None, 256)   3155456     ['positional_embedding[0][0]']   \n",
      " erEncoder)                                                                                       \n",
      "                                                                                                  \n",
      " decoder (Functional)           (None, None, 10000)  10453520    ['decoder_inputs[0][0]',         \n",
      "                                                                  'transformer_encoder[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16,232,976\n",
      "Trainable params: 16,232,976\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/simone/UniBO/Lab/NLP/PodcastSummarization/keras-model.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/simone/UniBO/Lab/NLP/PodcastSummarization/keras-model.ipynb#ch0000017?line=0'>1</a>\u001b[0m transformer\u001b[39m.\u001b[39msummary()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/simone/UniBO/Lab/NLP/PodcastSummarization/keras-model.ipynb#ch0000017?line=1'>2</a>\u001b[0m transformer\u001b[39m.\u001b[39mcompile(\u001b[39m\"\u001b[39m\u001b[39mrmsprop\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/simone/UniBO/Lab/NLP/PodcastSummarization/keras-model.ipynb#ch0000017?line=2'>3</a>\u001b[0m                     loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/simone/UniBO/Lab/NLP/PodcastSummarization/keras-model.ipynb#ch0000017?line=3'>4</a>\u001b[0m                     metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/simone/UniBO/Lab/NLP/PodcastSummarization/keras-model.ipynb#ch0000017?line=4'>5</a>\u001b[0m transformer\u001b[39m.\u001b[39mfit(dataset, epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "transformer.summary()\n",
    "transformer.compile(\"rmsprop\",\n",
    "                    loss=\"sparse_categorical_crossentropy\",\n",
    "                    metrics=[\"accuracy\"])\n",
    "transformer.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-19 17:30:38.052107: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at lookup_table_op.cc:929 : FAILED_PRECONDITION: Table not initialized.\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Exception encountered when calling layer \"string_lookup\" (type StringLookup).\n\nTable not initialized. [Op:LookupTableFindV2]\n\nCall arguments received:\n  • inputs=<tf.RaggedTensor [[b'youre', b'not', b'taught', ..., b'it', b'youre', b'sad']]>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/simone/UniBO/Lab/NLP/PodcastSummarization/keras-model.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simone/UniBO/Lab/NLP/PodcastSummarization/keras-model.ipynb#ch0000019?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m30\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simone/UniBO/Lab/NLP/PodcastSummarization/keras-model.ipynb#ch0000019?line=24'>25</a>\u001b[0m     input_sentence \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mchoice(X)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/simone/UniBO/Lab/NLP/PodcastSummarization/keras-model.ipynb#ch0000019?line=25'>26</a>\u001b[0m     summarized \u001b[39m=\u001b[39m decode_sequence(input_sentence)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simone/UniBO/Lab/NLP/PodcastSummarization/keras-model.ipynb#ch0000019?line=26'>27</a>\u001b[0m     \u001b[39mprint\u001b[39m(summarized)\n",
      "\u001b[1;32m/Users/simone/UniBO/Lab/NLP/PodcastSummarization/keras-model.ipynb Cell 17'\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_sentence)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/simone/UniBO/Lab/NLP/PodcastSummarization/keras-model.ipynb#ch0000019?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode_sequence\u001b[39m(input_sentence):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/simone/UniBO/Lab/NLP/PodcastSummarization/keras-model.ipynb#ch0000019?line=6'>7</a>\u001b[0m     tokenized_input_sentence \u001b[39m=\u001b[39m X_vectorizer([input_sentence])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/simone/UniBO/Lab/NLP/PodcastSummarization/keras-model.ipynb#ch0000019?line=7'>8</a>\u001b[0m     decoded_sentence \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m[start]\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/simone/UniBO/Lab/NLP/PodcastSummarization/keras-model.ipynb#ch0000019?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_decoded_sentence_length):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/layers/preprocessing/index_lookup.py:657\u001b[0m, in \u001b[0;36mIndexLookup._lookup_dense\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/layers/preprocessing/index_lookup.py?line=654'>655</a>\u001b[0m   lookups \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mzeros_like(inputs, dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value_dtype)\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/layers/preprocessing/index_lookup.py?line=655'>656</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/layers/preprocessing/index_lookup.py?line=656'>657</a>\u001b[0m   lookups \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlookup_table\u001b[39m.\u001b[39;49mlookup(inputs)\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/layers/preprocessing/index_lookup.py?line=658'>659</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_token \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/layers/preprocessing/index_lookup.py?line=659'>660</a>\u001b[0m   mask_locations \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mequal(inputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mask_key)\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Exception encountered when calling layer \"string_lookup\" (type StringLookup).\n\nTable not initialized. [Op:LookupTableFindV2]\n\nCall arguments received:\n  • inputs=<tf.RaggedTensor [[b'youre', b'not', b'taught', ..., b'it', b'youre', b'sad']]>"
     ]
    }
   ],
   "source": [
    "y_vocab = y_vectorizer.get_vocabulary()\n",
    "y_index_lookup = dict(zip(range(len(y_vocab)), y_vocab))\n",
    "max_decoded_sentence_length = 30\n",
    "\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = X_vectorizer([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = y_vectorizer([decoded_sentence\n",
    "                                                      ])[:, :-1]\n",
    "        predictions = transformer(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\n",
    "\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = y_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for _ in range(30):\n",
    "    input_sentence = random.choice(X)\n",
    "    summarized = decode_sequence(input_sentence)\n",
    "    print(summarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63e28586807c6502c782d898cf9a0cc5787bb3d77952b17b51ec8bdcd4044a3d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
