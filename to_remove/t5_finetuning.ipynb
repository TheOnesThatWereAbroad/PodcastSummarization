{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 fine-tuning in a Text Summarization task\n",
    "\n",
    "Nowadays, the AI community has two ways to approach automatic text summarization, Extractive Summarization and Abstractive Summarization:\n",
    "- _Extractive Summarization_: the extractive approach selects the most important phrases and lines from the documents. It then combines all the important lines to create the summary. So, in this case, every line and word of the summary actually belongs to the original document which is summarized.\n",
    "- _Abstractive Summarization_: The abstractive approach uses new phrases and terms that are different from the original document, keeping the meaning the same, just like how humans do in summarization. So, it is much harder than the extractive approach.\n",
    "\n",
    "The **abstractive text summarization** is one of the most challenging tasks in natural language processing, involving understanding of long passages, information compression, and language generation. The dominant paradigm for training machine learning models to do this is sequence-to-sequence (seq2seq) learning, where a neural network learns to map input sequences to output sequences. While these seq2seq models were initially developed using recurrent neural networks, Transformer encoder-decoder models have recently become favored as they are more effective at modeling the dependencies present in the long sequences encountered in summarization.\n",
    "\n",
    "Transformer models combined with self-supervised pre-training (e.g., BERT, GPT-2, RoBERTa, XLNet, ALBERT, T5, ELECTRA) have shown to be a powerful framework for producing general language learning.\n",
    "Text-To-Text Transfer Transformer (T5) model, pre-trained on Colossal Clean Crawled Corpus (C4), a cleaned version of Common Crawl that is two orders of magnitude larger than Wikipedia, achieves state-of-the-art results on many NLP benchmarks while being flexible enough to be fine-tuned to a variety of important downstream tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import os\n",
    "import regex as re\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(os.path.abspath(\"\"), 'podcasts-no-audio-13GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:  Index(['show_uri', 'show_name', 'show_description', 'publisher', 'language',\n",
      "       'rss_link', 'episode_uri', 'episode_name', 'episode_description',\n",
      "       'duration', 'show_filename_prefix', 'episode_filename_prefix'],\n",
      "      dtype='object')\n",
      "Shape:  (105360, 12)\n"
     ]
    }
   ],
   "source": [
    "metadata_path_train = os.path.join(dataset_path, 'metadata.tsv')\n",
    "metadata_train = pd.read_csv(metadata_path_train, sep='\\t')\n",
    "print(\"Columns: \", metadata_train.columns)\n",
    "print(\"Shape: \", metadata_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(episode):\n",
    "    # extract the 2 reference number/letter to access the episode transcript\n",
    "    show_filename = episode['show_filename_prefix']\n",
    "    episode_filename = episode['episode_filename_prefix'] + \".json\"\n",
    "    dir_1, dir_2 = re.match(r'show_(\\d)(\\w).*', show_filename).groups()\n",
    "\n",
    "    # check if the transcript file in all the derived subfolders exist\n",
    "    transcipt_path = os.path.join(dataset_path, \"spotify-podcasts-2020\",\n",
    "                                \"podcasts-transcripts\", dir_1, dir_2,\n",
    "                                show_filename, episode_filename)\n",
    "\n",
    "    return transcipt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the transcript files exist\n",
    "for i in range(len(metadata_train)):\n",
    "    assert os.path.exists(get_path(metadata_train.iloc[i]))\n",
    "\n",
    "print(\"All files exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcription(episode):\n",
    "    with open(get_path(episode), 'r') as f:\n",
    "        episode_json = json.load(f)\n",
    "        # seems that the last result in each trastcript is a repetition of the first one, so we ignore it\n",
    "        transcripts = [\n",
    "            result[\"alternatives\"][0]['transcript'] if 'transcript' in result[\"alternatives\"][0] else \"\"\n",
    "            for result in episode_json[\"results\"][:-1]\n",
    "        ]\n",
    "        return \" \".join(transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build gold dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:  Index(['show name', 'episode name', 'episode id', 'creator description',\n",
      "       'EGFB', 'lexrank summary', 'EGFB.1', 'textrank summary', 'EGFB.2',\n",
      "       'lsa summary', 'EGFB.3', 'quasi-supervised summary', 'EGFB.4',\n",
      "       'supervised summary', 'EGFB.5', 'show_uri', 'show_name',\n",
      "       'show_description', 'publisher', 'language', 'rss_link', 'episode_uri',\n",
      "       'episode_name', 'episode_description', 'duration',\n",
      "       'show_filename_prefix', 'episode_filename_prefix'],\n",
      "      dtype='object')\n",
      "Shape:  (150, 27)\n"
     ]
    }
   ],
   "source": [
    "metadata_path_gold = os.path.join(dataset_path, '150gold.tsv')\n",
    "metadata_gold = pd.read_csv(metadata_path_gold, sep='\\t')\n",
    "metadata_gold = pd.merge(metadata_gold, metadata_train, left_on='episode id', right_on='episode_uri')\n",
    "\n",
    "print(\"Columns: \", metadata_gold.columns)\n",
    "print(\"Shape: \", metadata_gold.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality = {\n",
    "    'B': 1,\n",
    "    'F': 2,\n",
    "    'G': 3,\n",
    "    'E': 4\n",
    "}\n",
    "\n",
    "# convert egfb columns to a quality score\n",
    "egfb_columns = ['EGFB', 'EGFB.1', 'EGFB.2', 'EGFB.3', 'EGFB.4', 'EGFB.5']\n",
    "egfb_to_quality = metadata_gold[egfb_columns].applymap(lambda x: quality[x])\n",
    "\n",
    "# remove rows with no quality > 1\n",
    "egfb_to_quality = egfb_to_quality[[any(row > 1) for row in egfb_to_quality.values]] \n",
    "\n",
    "# select the best transcript for each episode\n",
    "best_egfb = egfb_to_quality.apply(lambda x: x.idxmax(), axis=1)\n",
    "best_summary = [metadata_gold.iloc[i, np.argwhere(metadata_gold.columns == egfb)[0][0] - 1] for i, egfb in best_egfb.iteritems()]\n",
    "\n",
    "metadata_gold = metadata_gold.loc[best_egfb.index]\n",
    "metadata_gold['best_summary'] = best_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add transcripts\n",
    "metadata_gold['transcript'] = metadata_gold.apply(get_transcription, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode id</th>\n",
       "      <th>transcript</th>\n",
       "      <th>best_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spotify:episode:4KRC1TZ28FavN3J5zLHEtQ</td>\n",
       "      <td>What's up fellas? So I got a patron supported...</td>\n",
       "      <td>All right guys now as y'all guys might know so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spotify:episode:4tdDQcsBOUVWnA9XrpgTzS</td>\n",
       "      <td>If you are bored you are boring.  One of my ki...</td>\n",
       "      <td>It was the first and last time I ever said tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spotify:episode:626YAxomH0HZ6nCW9NLlGY</td>\n",
       "      <td>Visit Larisa English club.com English everyday...</td>\n",
       "      <td>Prepositions of movement review two is the sec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spotify:episode:6AUFl7KQWN6pzGFEIEKFQu</td>\n",
       "      <td>So so and salutations Summers and welcome to t...</td>\n",
       "      <td>My passion for The Sims 4 Grew From consuming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spotify:episode:6IDbemwG5t6XMlctbqcna7</td>\n",
       "      <td>Hi everyone. This is Justin from a liquidy pla...</td>\n",
       "      <td>This week on Nothing But A Bob Thang, Nathan a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>spotify:episode:2zr8iztbD8xSbuWO60tfHg</td>\n",
       "      <td>Well, everybody's clear here with a word from ...</td>\n",
       "      <td>It was a significant weekend in the NWSL, with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>spotify:episode:2SfUG4VtJFkyiuNgHALlsC</td>\n",
       "      <td>All right. Now - just one second now I'm confu...</td>\n",
       "      <td>During the first ever Frank and Eric Movie Par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>spotify:episode:2c2WPjRpoCSxtnAw0WsoqG</td>\n",
       "      <td>What is up? Everyone? Alright, so I've been pr...</td>\n",
       "      <td>LFT Radio - Lifelong Fitness and Training In t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>spotify:episode:6oZYPfBhCdpTSamM9Uj0v9</td>\n",
       "      <td>Oh, so you have to do a lot eight game. It was...</td>\n",
       "      <td>On today's show, we sit down with LSU freshman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>spotify:episode:1QOhuDRITc31SeemiqoibR</td>\n",
       "      <td>Hello and welcome to a special edition of th...</td>\n",
       "      <td>The Olympic qualifiers are here and we have pl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 episode id  \\\n",
       "0    spotify:episode:4KRC1TZ28FavN3J5zLHEtQ   \n",
       "1    spotify:episode:4tdDQcsBOUVWnA9XrpgTzS   \n",
       "2    spotify:episode:626YAxomH0HZ6nCW9NLlGY   \n",
       "3    spotify:episode:6AUFl7KQWN6pzGFEIEKFQu   \n",
       "5    spotify:episode:6IDbemwG5t6XMlctbqcna7   \n",
       "..                                      ...   \n",
       "145  spotify:episode:2zr8iztbD8xSbuWO60tfHg   \n",
       "146  spotify:episode:2SfUG4VtJFkyiuNgHALlsC   \n",
       "147  spotify:episode:2c2WPjRpoCSxtnAw0WsoqG   \n",
       "148  spotify:episode:6oZYPfBhCdpTSamM9Uj0v9   \n",
       "149  spotify:episode:1QOhuDRITc31SeemiqoibR   \n",
       "\n",
       "                                            transcript  \\\n",
       "0     What's up fellas? So I got a patron supported...   \n",
       "1    If you are bored you are boring.  One of my ki...   \n",
       "2    Visit Larisa English club.com English everyday...   \n",
       "3    So so and salutations Summers and welcome to t...   \n",
       "5    Hi everyone. This is Justin from a liquidy pla...   \n",
       "..                                                 ...   \n",
       "145  Well, everybody's clear here with a word from ...   \n",
       "146  All right. Now - just one second now I'm confu...   \n",
       "147  What is up? Everyone? Alright, so I've been pr...   \n",
       "148  Oh, so you have to do a lot eight game. It was...   \n",
       "149    Hello and welcome to a special edition of th...   \n",
       "\n",
       "                                          best_summary  \n",
       "0    All right guys now as y'all guys might know so...  \n",
       "1    It was the first and last time I ever said tha...  \n",
       "2    Prepositions of movement review two is the sec...  \n",
       "3    My passion for The Sims 4 Grew From consuming ...  \n",
       "5    This week on Nothing But A Bob Thang, Nathan a...  \n",
       "..                                                 ...  \n",
       "145  It was a significant weekend in the NWSL, with...  \n",
       "146  During the first ever Frank and Eric Movie Par...  \n",
       "147  LFT Radio - Lifelong Fitness and Training In t...  \n",
       "148  On today's show, we sit down with LSU freshman...  \n",
       "149  The Olympic qualifiers are here and we have pl...  \n",
       "\n",
       "[142 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = metadata_gold[['episode id', 'transcript', 'best_summary']]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Translating text to numbers is known as encoding. Encoding is done in a two-step process: the tokenization, followed by the conversion to input IDs using the same vocabulary used when the model was pretrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max input length:  638\n",
      "Max target length:  310\n"
     ]
    }
   ],
   "source": [
    "max_input_length = int(np.quantile(train_data['transcript'].apply(len), 0.01))\n",
    "max_target_length = int(np.quantile(train_data['best_summary'].apply(len), 0.4))\n",
    "print(\"Max input length: \", max_input_length)\n",
    "print(\"Max target length: \", max_target_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoTokenizer class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_pandas(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"facebook/bart-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(dataset, text_column, summary_column, max_input_length, max_target_length, padding, prefix=\"summarize: \"):\n",
    "    inputs = dataset[text_column]\n",
    "    targets = dataset[summary_column]\n",
    "    inputs = [prefix + inp for inp in inputs]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on train dataset: 100%|██████████| 1/1 [00:00<00:00,  1.08ba/s]\n"
     ]
    }
   ],
   "source": [
    "padding = \"max_length\"\n",
    "train_dataset = train_dataset.map(\n",
    "                lambda x: preprocess_function(x, \"transcript\", \"best_summary\", max_input_length, max_target_length, padding, prefix=\"summarize: \"),\n",
    "                batched=True,\n",
    "                remove_columns=train_dataset.column_names,\n",
    "                desc=\"Running tokenizer on train dataset\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def sample_generator(dataset, model, tokenizer, shuffle, pad_to_multiple_of=None):\n",
    "    if shuffle:\n",
    "        sample_ordering = np.random.permutation(len(dataset))\n",
    "    else:\n",
    "        sample_ordering = np.arange(len(dataset))\n",
    "    for sample_idx in sample_ordering:\n",
    "        example = dataset[int(sample_idx)]\n",
    "        # Handle dicts with proper padding and conversion to tensor.\n",
    "        example = tokenizer.pad(example, return_tensors=\"np\", pad_to_multiple_of=pad_to_multiple_of)\n",
    "        example = {key: tf.convert_to_tensor(arr, dtype_hint=tf.int32) for key, arr in example.items()}\n",
    "        if model is not None and hasattr(model, \"prepare_decoder_input_ids_from_labels\"):\n",
    "            decoder_input_ids = model.prepare_decoder_input_ids_from_labels(\n",
    "                labels=tf.expand_dims(example[\"labels\"], 0)\n",
    "            )\n",
    "            example[\"decoder_input_ids\"] = tf.squeeze(decoder_input_ids, 0)\n",
    "        yield example, example[\"labels\"]  # TF needs some kind of labels, even if we don't use them\n",
    "    return\n",
    "\n",
    "# region Helper functions\n",
    "def dataset_to_tf(dataset, model, tokenizer, total_batch_size, num_epochs, shuffle):\n",
    "    if dataset is None:\n",
    "        return None\n",
    "    train_generator = partial(sample_generator, dataset, model, tokenizer, shuffle=shuffle)\n",
    "    train_signature = {\n",
    "        feature: tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "        for feature in dataset.features\n",
    "        if feature != \"special_tokens_mask\"\n",
    "    }\n",
    "    if (\n",
    "        model is not None\n",
    "        and \"decoder_input_ids\" not in train_signature\n",
    "        and hasattr(model, \"prepare_decoder_input_ids_from_labels\")\n",
    "    ):\n",
    "        train_signature[\"decoder_input_ids\"] = train_signature[\"labels\"]\n",
    "    # This may need to be changed depending on your particular model or tokenizer!\n",
    "    padding_values = {\n",
    "        key: tf.convert_to_tensor(tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0, dtype=tf.int32)\n",
    "        for key in train_signature.keys()\n",
    "    }\n",
    "    padding_values[\"labels\"] = tf.convert_to_tensor(-100, dtype=tf.int32)\n",
    "    train_signature[\"labels\"] = train_signature[\"input_ids\"]\n",
    "    train_signature = (train_signature, train_signature[\"labels\"])\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "    tf_dataset = (\n",
    "        tf.data.Dataset.from_generator(train_generator, output_signature=train_signature)\n",
    "        .with_options(options)\n",
    "        .padded_batch(\n",
    "            batch_size=total_batch_size,\n",
    "            drop_remainder=True,\n",
    "            padding_values=(padding_values, np.array(-100, dtype=np.int32)),\n",
    "        )\n",
    "        .repeat(int(num_epochs))\n",
    "    )\n",
    "    return tf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBartForConditionalGeneration.\n",
      "\n",
      "All the layers of TFBartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'model.shared/model.shared/weight:0' shape=(50265, 768) dtype=float32, numpy=\n",
       "array([[ 0.0125351 ,  0.0014286 , -0.00962067, ...,  0.00217628,\n",
       "         0.10565186,  0.01028442],\n",
       "       [-0.01139832, -0.016922  , -0.01837158, ..., -0.01313782,\n",
       "        -0.00431824, -0.00532532],\n",
       "       [ 0.08416748, -0.03894043,  0.00963593, ...,  0.05834961,\n",
       "         0.00823975,  0.03567505],\n",
       "       ...,\n",
       "       [ 0.01409912, -0.0241394 , -0.02073669, ..., -0.00463867,\n",
       "         0.02406311, -0.007061  ],\n",
       "       [ 0.00705338, -0.03909302, -0.02728271, ...,  0.00548172,\n",
       "         0.01307678, -0.00415421],\n",
       "       [ 0.00116348,  0.00416565, -0.02301025, ...,  0.00653839,\n",
       "        -0.00830078,  0.01797485]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_batch_size = 2\n",
    "num_train_epochs = 3\n",
    "learning_rate = 5e-5\n",
    "tf_train_dataset = dataset_to_tf(\n",
    "            train_dataset,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            total_batch_size=total_train_batch_size,\n",
    "            num_epochs=num_train_epochs,\n",
    "            shuffle=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "# region Optimizer, loss and LR scheduling\n",
    "# Scheduler and math around the number of training steps.\n",
    "num_update_steps_per_epoch = len(train_dataset) // total_train_batch_size\n",
    "num_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=learning_rate, num_train_steps=num_train_steps, num_warmup_steps=0\n",
    ")\n",
    "\n",
    "def masked_sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    # We clip the negative labels to 0 to avoid NaNs appearing in the output and\n",
    "    # fouling up everything that comes afterwards. The loss values corresponding to clipped values\n",
    "    # will be masked later anyway, but even masked NaNs seem to cause overflows for some reason.\n",
    "    # 1e6 is chosen as a reasonable upper bound for the number of token indices - in the unlikely\n",
    "    # event that you have more than 1 million tokens in your vocabulary, consider increasing this value.\n",
    "    # More pragmatically, consider redesigning your tokenizer.\n",
    "    losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        tf.clip_by_value(y_true, 0, int(1e6)), y_pred, from_logits=True\n",
    "    )\n",
    "    # Compute the per-sample loss only over the unmasked tokens\n",
    "    losses = tf.ragged.boolean_mask(losses, y_true != -100)\n",
    "    losses = tf.reduce_mean(losses, axis=-1)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "# region Metric\n",
    "metric = load_metric(\"rouge\")\n",
    "# endregion\n",
    "\n",
    "# region Training\n",
    "model.compile(loss={\"logits\": masked_sparse_categorical_crossentropy}, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'tf_bart_for_conditional_generation/model/decoder/layers.4/Gelu/truediv' defined at (most recent call last):\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2854, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2900, in _run_cell\n      return runner(coro)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3098, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3301, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3361, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\peppe\\AppData\\Local\\Temp\\ipykernel_1100\\3393712521.py\", line 1, in <cell line: 1>\n      model.fit(\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 996, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1265, in run_call_with_unpacked_inputs\n      # If the variable holds the weights themselves, return them\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\bart\\modeling_tf_bart.py\", line 1297, in call\n      outputs = self.model(\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1265, in run_call_with_unpacked_inputs\n      # If the variable holds the weights themselves, return them\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\bart\\modeling_tf_bart.py\", line 1092, in call\n      decoder_outputs = self.decoder(\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1265, in run_call_with_unpacked_inputs\n      # If the variable holds the weights themselves, return them\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\bart\\modeling_tf_bart.py\", line 952, in call\n      for idx, decoder_layer in enumerate(self.layers):\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\bart\\modeling_tf_bart.py\", line 952, in call\n      for idx, decoder_layer in enumerate(self.layers):\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\bart\\modeling_tf_bart.py\", line 964, in call\n      hidden_states, layer_self_attn, layer_cross_attn, present_key_value = decoder_layer(\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\bart\\modeling_tf_bart.py\", line 440, in call\n      hidden_states = self.activation_fn(self.fc1(hidden_states))\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\activations.py\", line 351, in gelu\n      return tf.nn.gelu(x, approximate)\nNode: 'tf_bart_for_conditional_generation/model/decoder/layers.4/Gelu/truediv'\nfailed to allocate memory\n\t [[{{node tf_bart_for_conditional_generation/model/decoder/layers.4/Gelu/truediv}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_34117]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\peppe\\UNIBO\\Natural Language Processing\\lab\\PodcastSummarization\\t5_finetuning.ipynb Cell 27'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/peppe/UNIBO/Natural%20Language%20Processing/lab/PodcastSummarization/t5_finetuning.ipynb#ch0000026?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/peppe/UNIBO/Natural%20Language%20Processing/lab/PodcastSummarization/t5_finetuning.ipynb#ch0000026?line=1'>2</a>\u001b[0m                 tf_train_dataset,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/peppe/UNIBO/Natural%20Language%20Processing/lab/PodcastSummarization/t5_finetuning.ipynb#ch0000026?line=2'>3</a>\u001b[0m                 epochs\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(num_train_epochs),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/peppe/UNIBO/Natural%20Language%20Processing/lab/PodcastSummarization/t5_finetuning.ipynb#ch0000026?line=3'>4</a>\u001b[0m                 steps_per_epoch\u001b[39m=\u001b[39;49mnum_update_steps_per_epoch,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/peppe/UNIBO/Natural%20Language%20Processing/lab/PodcastSummarization/t5_finetuning.ipynb#ch0000026?line=4'>5</a>\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/peppe/anaconda3/envs/nlp/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/peppe/anaconda3/envs/nlp/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///c%3A/Users/peppe/anaconda3/envs/nlp/lib/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/peppe/anaconda3/envs/nlp/lib/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/peppe/anaconda3/envs/nlp/lib/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/peppe/anaconda3/envs/nlp/lib/site-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/peppe/anaconda3/envs/nlp/lib/site-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> <a href='file:///c%3A/Users/peppe/anaconda3/envs/nlp/lib/site-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     <a href='file:///c%3A/Users/peppe/anaconda3/envs/nlp/lib/site-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     <a href='file:///c%3A/Users/peppe/anaconda3/envs/nlp/lib/site-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     <a href='file:///c%3A/Users/peppe/anaconda3/envs/nlp/lib/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'tf_bart_for_conditional_generation/model/decoder/layers.4/Gelu/truediv' defined at (most recent call last):\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2854, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2900, in _run_cell\n      return runner(coro)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3098, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3301, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3361, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\peppe\\AppData\\Local\\Temp\\ipykernel_1100\\3393712521.py\", line 1, in <cell line: 1>\n      model.fit(\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 996, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1265, in run_call_with_unpacked_inputs\n      # If the variable holds the weights themselves, return them\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\bart\\modeling_tf_bart.py\", line 1297, in call\n      outputs = self.model(\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1265, in run_call_with_unpacked_inputs\n      # If the variable holds the weights themselves, return them\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\bart\\modeling_tf_bart.py\", line 1092, in call\n      decoder_outputs = self.decoder(\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1265, in run_call_with_unpacked_inputs\n      # If the variable holds the weights themselves, return them\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\bart\\modeling_tf_bart.py\", line 952, in call\n      for idx, decoder_layer in enumerate(self.layers):\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\bart\\modeling_tf_bart.py\", line 952, in call\n      for idx, decoder_layer in enumerate(self.layers):\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\bart\\modeling_tf_bart.py\", line 964, in call\n      hidden_states, layer_self_attn, layer_cross_attn, present_key_value = decoder_layer(\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\bart\\modeling_tf_bart.py\", line 440, in call\n      hidden_states = self.activation_fn(self.fc1(hidden_states))\n    File \"c:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\activations.py\", line 351, in gelu\n      return tf.nn.gelu(x, approximate)\nNode: 'tf_bart_for_conditional_generation/model/decoder/layers.4/Gelu/truediv'\nfailed to allocate memory\n\t [[{{node tf_bart_for_conditional_generation/model/decoder/layers.4/Gelu/truediv}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_34117]"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "                tf_train_dataset,\n",
    "                epochs=int(num_train_epochs),\n",
    "                steps_per_epoch=num_update_steps_per_epoch,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"And while we pause I'd like to take a moment and talk about our sponsor. Anchor.  Anchor is the easiest way to set up a podcast and it's totally free.  They give you tools that help you to record and edit your podcast right from your phone or computer.  anchor  Does all the work for you by Distributing your podcast to places like Spotify and apple podcast?  and many more  you can make money from your podcast with no minimum listenership.  It's everything you need to make a podcast in one place.  So if you're interested in starting your own podcast download the free anchor app today or go to Anchor dot f m-- to get started.  Now it's time to go back.  To our bedtime reading. Hi everyone. My name is Amber Lawton and you are listening to Bible at bedtime a podcast where I read the Bible to help you fall asleep. The reason I decided to start this podcast is because over the years.  I've had trouble falling asleep, and I discovered that I enjoy listening to the Bible.  But I find some narration to be difficult to follow when trying to fall asleep.  So here tonight. I'm going to read our first selection.  I decided to start with a dreamer Joseph and for tonight. This will be part one reading Genesis chapter 37 through chapter 40. So before we get started go ahead and lay down.  Finding a comfortable position.  Take a deep breath in.  And then out.  And as you breathe exhaling the stress of your day and letting your focus be on peaceful sleep as you hear the word now Genesis 37 Joseph's dreams.    Jacob lived in the land where his father had stayed the land of Canaan. These are the family records of Jacob. I 17 years of age Joseph tended sheep with his brothers. The young man was working with the sons of bilhah and zilpah his father's wives and he brought a bad report about them to their father.  Now Israel loved Joseph more than his other Sons because Joseph was a son born to him in his old age and he made a robe of Many Colors for him.  When his brothers saw that their father loved him more than all his brothers. They hated him and could not bring themselves to speak peaceably to him then Joseph had a dream.  When he told it to his brothers, they hated him even more.  He said to them listen to this dream. I had there we were binding sheaves of grain in the field. Suddenly my Sheaf stood up and your sheaves gathered around it and bow down to my Sheaf. Are you really going to Reign Over Us. His brothers asked him.  Are you really going to rule us?  So they hated him even more because of his dream and what he had said.  Then he had another dream and told it to his brothers. Look he said I had another dream and this time the sun moon and Eleven stars were bowing down to me. He told his father and brothers and his father rebuked him.  What kind of dream is this that you have had he said am I and your mother and your brothers really going to come and bow down to the ground before you?  His brothers were jealous of him, but his father kept the matter in mind Joseph sold into slavery.  His brothers had gone to pasture their fathers flocks at shechem.  Israel said to Joseph your brother's, you know are pastoring the flocks at shechem. Get ready. I'm sending you to them. I'm ready Joseph replied. Then Israel said to him go and see how your brothers and the flocks are doing and bring word back to me.  So he sent him from the Hebron Valley and he went to shechem.  A man found him there wandering in the field and asked him. What are you looking for?  I'm looking for my brothers. Joseph said can you tell me where they are pastoring their flocks? They've moved on from here. The man said I heard them say let's go to Jonathan. So Joseph set out after his brothers and found them at tothen.  They saw him in the distance and before he had reached them. They plotted to kill him.  They said to one another. Oh, look here comes that dream expert so now come on. Let's kill him and throw him into one of the pits.  We can say that a vicious animal ate him then we'll see what becomes of his dreams.  When Ruben heard this he tried to save him from them. He said let's not take his life.  Rubin also said to them don't shed blood throw him into this pit in the wilderness, but don't lay a hand on him.  Intending to rescue him from them and return him to his father.  When Joseph came to his brothers, they stripped off Joseph's robe the robe of Many Colors that he had on then they took him and threw him into the pit. The pit was empty without water.  They sat down to eat a meal and when they looked up there was a caravan of Ishmael lights coming from Gilead.  Their camels were carrying aromatic gum Balsam and resin going down to Egypt.  Judah said to his brothers. What do we gain if we kill our brother and cover up his blood?  Come on, let's sell him to the ishmaelites and not lay a hand on him for he is our brother our own flesh and his brothers agreed.  When many a night Traders passed by his brothers pulled Joseph out of the pit and sold him for twenty pieces of silver to the ishmaelites who took Joseph to Egypt.  When Ruben return to the pit and saw that Joseph was not there. He tore his clothes. He went back to his brothers and said the boy is gone. What am I going to do? So they took Joseph's robe slaughtered and they'll go and dipped the robe in its blood.  They sent the robe of Many Colors to their father and said we found this examine it is it your son's robe or not. His father recognized it. It is my son's robe. He said a vicious animal has devoured him.  Joseph has been Torn to Pieces then Jacob tore his clothes put sackcloth around his waist and mourn for his son many days.  All his sons and daughters tried to comfort him, but he refused to be comforted. No, he said I will go down to Shield to my son morning and his father wept for him. Meanwhile, the midianites sold Joseph in Egypt to potiphar and officer of pharaoh and the captain of the guards.  chapter 39  Joseph in Potiphar's house   Now Joseph had been taken to Egypt and Egyptian named potiphar and officer of pharaoh and the captain of the guards bought him from the ishmaelites who had brought him there.  The Lord was with Joseph and he became a successful man serving in the household of his Egyptian master.  When his master saw that the Lord was with him and that the Lord made everything. He did successful Joseph found favor with his master and became his personal attendant.  Potiphar also put him in charge of his household and placed all that. He owned under his authority.  From the time that he put him in charge of this household and all that. He owned the Lord blessed the Egyptians house because of Joseph. The Lord's blessing was on all that. He owned in his house and in his Fields, he left all that. He owned under Joseph's Authority. He did not concern himself with anything except the food he ate.  Now Joseph was well-built and handsome after some time. His master's wife looked longingly at Joseph and said sleep with me, but he refused look he said to his master's wife with me here. My master does not concern himself with anything in his house.  And he has put all that he owns under my authority.  No one in this house is greater than I am. He has withheld nothing from me except you because you are his wife.  So, how could I do this immense evil? And how could I sin against God although she spoke to Joseph day after day he refused to go to bed with her.  Now one day he went into the house to do his work.  And none of the household servants were there.  She grabbed him by his garment and said sleep with me.  But leaving His Garment in her hand, he escaped and ran outside.  When she saw that he had left His Garment with her and had run outside. She called her household servants. Look she said to them my husband brought a Hebrew man to make fools of us. He came to me so he could sleep with me and I screamed as loud as I could.  When he heard me screaming for help, he left his garment beside me and ran outside.  She put Joseph's garment beside her until his master came home. Then she told him the same story.  The Hebrew slave you brought to us came to make a fool of me. But when I screamed for help, he left his garment beside me and ran outside.  When his master heard the story his wife told him. These are the things your slave did to me. He was Furious and had him thrown into prison where the Kings prisoners were confined. So Joseph was there in prison  Joseph in prison  But the Lord was with Joseph and extended kindness to him. He granted him favor with the prison Warden the warden put all the prisoners.  Who are in prison under Joseph's Authority and he was responsible for everything that was done there.  The warden did not bother with anything under Joseph's Authority because the Lord was with him and the Lord made everything that he did successful.  chapter 40  Joseph interprets to prisoners dreams   After this the king of Egypt's cupbearer and Baker offended their Master the king of Egypt Pharaoh was angry with his two officers the chief cupbearer and the chief Baker and put them in custody in the house of the captain of the guards in the prison where Joseph was confined.  The captain of the guards assigned Joseph to them as their personal attendant and they were in custody for some time the king of Egypt's cupbearer and Baker who were confined in the prison.  each had a dream  both had a dream on the same night and each dream had its own meaning.  When Joseph came to them in the morning, he saw that they looked distraught. So he asked pharaohs officers who were in custody with him and his master's house. Why do you look so sad today?  We had dreams they said to him but there is no one to interpret them. Then Joseph said to them don't interpretations belong to God. Tell me your dreams.  So the chief cupbearer told his dream to Joseph in my dream. There was a Vine in front of me on the vine were three branches as soon as it budded it blossoms come out and it clusters ripened into grapes.  Pharaoh's cup was in my hand and I took the grapes squeeze them into Pharaoh's cup and place the cup in Pharaoh's hand.  This is its interpretation. Joseph said to him the three branches are three days in just three days Pharaoh will lift up your head and restore you to your position. You will put Pharaoh's Cup in his hand the way you used to when you are his cupbearer.  But when all goes well for you remember that I was with you. Please show kindness to me by mentioning me to pharaoh and get me out of this prison for I was kidnapped from the land of the Hebrews and even here I have done nothing that they should put me in the dungeon.  When the chief Baker saw that the interpretation was positive. He said to Joseph. I also had a dream three baskets of white bread were on my head in the top basket were all sorts of baked goods for Pharaoh, but the birds were eating them out of the baskets on my head.  This is its interpretation. Joseph replied the three baskets are three days in just three days Pharaoh will lift up your head from off you and hang you on a tree.  Then the birds will eat the flesh from your body.  On the third day, which was Pharaoh's birthday. He gave A Feast for all his servants. He elevated the chief cupbearer and the chief Baker among his servants Pharaoh restore the chief cupbearer to his position as cupbearer and he placed the cup in Pharaoh's hand. But Pharaoh hang to the chief Baker just as Joseph had explained to them.  Yet the chief cupbearer did not remember Joseph he forgot him.  That's it for tonight's reading. Hopefully you have fallen asleep tomorrow. We will continue.  with chapter 41   sleep well tonight.  And rest knowing that the Lord is with you. Good night.  \""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_exaple = train_data.iloc[45].transcript\n",
    "transcript_exaple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bible at bedtime is a podcast where I read the Bible to help you fall asleep. This week Amber Lawton reads Genesis chapter 37 through chapter 40. Take a deep breath in. And then out as you breathe exhaling the stress of your day and letting your focus be on peaceful sleep as you hear the word now Genesis 37.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best summarization\n",
    "train_data.iloc[45].best_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = np.reshape(tokenizer(transcript_exaple, max_length=max_input_length, padding=padding, truncation=True).input_ids, (1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "output =model.generate(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Are you really going to rule us? He said to them\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e800dd11dddfb1e3886769e91ed8bbe987a221798b85010fca298ae8afbc389e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
