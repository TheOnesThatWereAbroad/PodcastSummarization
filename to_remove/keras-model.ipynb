{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import regex as re\n",
    "from urllib import request\n",
    "import zipfile\n",
    "import glob\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "from nltk.corpus import words\n",
    "from idf_cleaner import clean_with_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/simone/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "SEQUENCE_LENGTH = 1200\n",
    "BATCH_SIZE = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(os.path.abspath(\"\"), 'podcasts-no-audio-13GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:  Index(['show_uri', 'show_name', 'show_description', 'publisher', 'language',\n",
      "       'rss_link', 'episode_uri', 'episode_name', 'episode_description',\n",
      "       'duration', 'show_filename_prefix', 'episode_filename_prefix'],\n",
      "      dtype='object')\n",
      "Shape:  (105360, 12)\n"
     ]
    }
   ],
   "source": [
    "metadata_path_train = os.path.join(dataset_path, 'metadata.tsv')\n",
    "metadata_train = pd.read_csv(metadata_path_train, sep='\\t')\n",
    "print(\"Columns: \", metadata_train.columns)\n",
    "print(\"Shape: \", metadata_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show_uri                                 spotify:show:2NYtxEZyYelR6RMKmjfPLB\n",
      "show_name                                               Kream in your Koffee\n",
      "show_description           A 20-something blunt female takes on the world...\n",
      "publisher                                                        Katie Houle\n",
      "language                                                              ['en']\n",
      "rss_link                            https://anchor.fm/s/11b84b68/podcast/rss\n",
      "episode_uri                           spotify:episode:000A9sRBYdVh66csG2qEdj\n",
      "episode_name                                         1: It’s Christmas Time!\n",
      "episode_description        On the first ever episode of Kream in your Kof...\n",
      "duration                                                           12.700133\n",
      "show_filename_prefix                             show_2NYtxEZyYelR6RMKmjfPLB\n",
      "episode_filename_prefix                               000A9sRBYdVh66csG2qEdj\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Copy this uri into the browser to listen to the episode:\n",
      " spotify:episode:000A9sRBYdVh66csG2qEdj\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "episode_example_train = metadata_train.iloc[i]\n",
    "print(episode_example_train)\n",
    "print(\"\\nCopy this uri into the browser to listen to the episode:\\n\",\n",
    "      episode_example_train['episode_uri'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(episode):\n",
    "    # extract the 2 reference number/letter to access the episode transcript\n",
    "    show_filename = episode['show_filename_prefix']\n",
    "    episode_filename = episode['episode_filename_prefix'] + \".json\"\n",
    "    dir_1, dir_2 = re.match(r'show_(\\d)(\\w).*', show_filename).groups()\n",
    "\n",
    "    interval_folders = [range(0, 3), range(3, 6), range(6, 8)]\n",
    "\n",
    "    # check which is the main folder containing the transcript\n",
    "    main_dir = \"\"\n",
    "    for interval in interval_folders:\n",
    "        if int(dir_1) in interval:\n",
    "            main_dir = \"podcasts-transcripts-{}to{}\".format(interval[0],\n",
    "                                                            interval[-1])\n",
    "    assert main_dir != \"\"\n",
    "\n",
    "    # check if the transcript file in all the derived subfolders exist\n",
    "    transcipt_path = os.path.join(dataset_path, \"spotify-podcasts-2020\",\n",
    "                                \"podcasts-transcripts\", dir_1, dir_2,\n",
    "                                show_filename, episode_filename)\n",
    "\n",
    "    return transcipt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while not os.path.isfile(get_path(metadata_train.iloc[i])):\n",
    "    i = random.randint(0,10000)\n",
    "    print(get_path(metadata_train.iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "show_uri                                 spotify:show:2NYtxEZyYelR6RMKmjfPLB\n",
       "show_name                                               Kream in your Koffee\n",
       "show_description           A 20-something blunt female takes on the world...\n",
       "publisher                                                        Katie Houle\n",
       "language                                                              ['en']\n",
       "rss_link                            https://anchor.fm/s/11b84b68/podcast/rss\n",
       "episode_uri                           spotify:episode:000A9sRBYdVh66csG2qEdj\n",
       "episode_name                                         1: It’s Christmas Time!\n",
       "episode_description        On the first ever episode of Kream in your Kof...\n",
       "duration                                                           12.700133\n",
       "show_filename_prefix                             show_2NYtxEZyYelR6RMKmjfPLB\n",
       "episode_filename_prefix                               000A9sRBYdVh66csG2qEdj\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode = metadata_train.iloc[i]\n",
    "episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcription(episode):\n",
    "    with open(get_path(episode), 'r') as f:\n",
    "        episode_json = json.load(f)\n",
    "        # seems that the last result in each trastcript is a repetition of the first one, so we ignore it\n",
    "        transcripts = [\n",
    "            result[\"alternatives\"][0]['transcript'] if 'transcript' in result[\"alternatives\"][0] else \"\"\n",
    "            for result in episode_json[\"results\"][:-1]\n",
    "        ]\n",
    "        return \" \".join(transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 On the first ever episode of Kream in your Koffee, Katie talks about tips for Christmas shopping. We also get a little insight into who and what we’ll be hearing about in next weeks episode! \n",
      "1 See something, say something. It’s a mantra many live by. If you see something strange, call it in or make someone aware, even if it seems innocuous. Jennifer San Marco had strange behaviors. It was clear to many that the woman suffered from mental illness that was being untreated. But, many wrote it off. And, on January 30th 2006 her strange behaviors bubbled over and the Goleta Postal Facility shootings began. Jennifer San Marco Kills (2006)\n",
      "2 Today’s episode is a sit down Michael and Omar had with former USMNT and former USWNT goalkeeper coach, Phil Wheddon. Phil discusses how he started the International Goalkeeper Coaches Conference (IGCC), what the US needs to do in order to create a successful goalkeeping curriculum across the United States and lastly, he gives his best advice to young goalkeepers and goalkeeper coaches.  Make sure to Subscribe to Inside the 18 on both Apple Podcasts and Spotify and don’t forget to leave us a rating and a comment to let us know how we’re doing!  Contact Information: @la_goalkeeping_academy @progkacademy_ @aviatasports @philwheddon \n",
      "3 Join us as we take a look at all current Chiefs news, including the draft, free agency, in-house contract talks, and much more!  \n",
      "4 The modern morality tail of how to stay good for Christmas. Ashley Beall of Make It Modern Podcast (@MIM_Podcast) joins me to discuss the redemption arcs of one of KidLit's biggest fuckbois, Edmund Pevensie.  iTunes Spotify YouTube Stitcher Google Play Music Anchor Do you want to work with us? Email us!  \n",
      "5 . \n",
      "6 Miss Jenn Davis reads the final part of The Sissy's Mentor.  It's the story of a young inexperienced college freshman and the dominant woman who takes him under her wing or is that wig?   You can find a lot more great erotic content from Jenn at MissJennDavis.com. Remember that this podcast plays clips (Usually 40%-50%) from audios.  At least half the show is missing and in that half are the very sexiest parts. Please support Candy Apple Press at our clipsites: IWantClips Clips4Sale and buy our books: Amazon.com You can also donate to the Podcast directly at Anchor.fm.  Wherever you listen, please give us good feedback. Thanks, Kylie  \n",
      "7 In today's episode of Chasity and the City we talk about our Journey with Chasity. How we live the virtue of Chasity now and misconceptions we had about Chastity before we educated ourselves. We share some real talk and relatable stories about our journey to living chastely.    \n",
      "8 Former Boatswain’s Mate Dan Shirey talks pitchpoling a 44 ft Motor Lifeboat on the Chetco River bar, responding to a sinking fishing vessel despite knowing he lacked enough fuel to make it back to shore, escorting a listing cargo ship hundreds of miles to reach a safe port, qualifying as a diver, using the skills gained through his service to pursue underwater treasure hunting, and diving on the wrecks of the same whaling ships that were the cause of the 1897 Overland Relief Expedition where some of the earliest Coast Guard heroes earned their fame.  \n",
      "9 Join us as we learn the tricks and tips of staying positive and upbeat despite life’s challenges!   \n"
     ]
    }
   ],
   "source": [
    "link_removal_pattern = re.compile(\n",
    "    r\"([\\w\\s:\\-\\p{So}]*((https:|www\\.).*)$|---.*$)\")\n",
    "\n",
    "metadata_train.episode_description = metadata_train.episode_description.apply(lambda desc: link_removal_pattern.sub(\"\", str(desc)))\n",
    "for row, val in metadata_train.episode_description.iloc[:10].iteritems():\n",
    "    print(row, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingMatrix():\n",
    "    \"\"\"Generates an embedding matrix using GloVE, given a vocabulary/\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        glove_url=\"http://nlp.stanford.edu/data/glove.6B.zip\",\n",
    "        embedding_dim=100,\n",
    "        embedding_folder=\"glove\"\n",
    "    ):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.download_glove_if_needed(\n",
    "            glove_url=glove_url, embedding_folder=embedding_folder\n",
    "        )\n",
    "\n",
    "        # create the embeddings vocabulary\n",
    "        self.glove_dict = self.parse_glove(embedding_folder)\n",
    "\n",
    "    def download_glove_if_needed(self, glove_url, embedding_folder):\n",
    "        \"\"\"\n",
    "        Downloads the glove embeddings from the internet\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        glove_url : The url of the GloVe embeddings.\n",
    "        embedding_folder: folder where the embedding will be downloaded\n",
    "        \"\"\"\n",
    "        # create embedding folder if it does not exist\n",
    "        if not os.path.exists(embedding_folder):\n",
    "            os.makedirs(embedding_folder)\n",
    "\n",
    "        # extract the embedding if it is not extracted\n",
    "        if not glob.glob(\n",
    "            os.path.join(embedding_folder, \"**/glove*.txt\"), recursive=True\n",
    "        ):\n",
    "\n",
    "            # download the embedding if it does not exist\n",
    "            embedding_zip = os.path.join(embedding_folder, glove_url.split(\"/\")[-1])\n",
    "            if not os.path.exists(embedding_zip):\n",
    "                print(\"Downloading the GloVe embeddings...\")\n",
    "                request.urlretrieve(glove_url, embedding_zip)\n",
    "                print(\"Successful download!\")\n",
    "\n",
    "            # extract the embedding\n",
    "            print(\"Extracting the embeddings...\")\n",
    "            with zipfile.ZipFile(embedding_zip, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(embedding_folder)\n",
    "                print(\"Successfully extracted the embeddings!\")\n",
    "            os.remove(embedding_zip)\n",
    "\n",
    "    def parse_glove(self, embedding_folder):\n",
    "        \"\"\"\n",
    "        Parses the GloVe embeddings from their files, filling the vocabulary.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        embedding_folder : folder where the embedding files are stored\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dictionary representing the vocabulary from the embeddings\n",
    "        \"\"\"\n",
    "        print(\"Creating glove vocabulary...\")\n",
    "        vocabulary = {\"<pad>\": np.zeros(self.embedding_dim)}\n",
    "        embedding_file = os.path.join(\n",
    "            embedding_folder, \"glove.6B.\" + str(self.embedding_dim) + \"d.txt\"\n",
    "        )\n",
    "        with open(embedding_file, encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                word, coefs = line.split(maxsplit=1)\n",
    "                coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "                vocabulary[word] = coefs\n",
    "        return vocabulary\n",
    "\n",
    "    def create_embedding_matrix(self, vocabulary):\n",
    "        \"\"\"\n",
    "        Creates the embedding matrix from the vocabulary.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocabulary : dictionary representing the vocabulary from the vectorizer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        embedding_matrix : numpy array representing the embedding matrix\n",
    "        \"\"\"\n",
    "        print(\"Creating embedding matrix...\")\n",
    "        embedding_matrix = np.zeros((len(vocabulary), self.embedding_dim))\n",
    "        for i, word in enumerate(vocabulary):\n",
    "            if word in self.glove_dict:\n",
    "                embedding_matrix[i] = self.glove_dict[word]\n",
    "            elif word not in [\"\", \"[UNK]\"]:\n",
    "                embedding_matrix[i] = np.random.uniform(size=self.embedding_dim)\n",
    "        return np.array(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-15 15:24:42.811006: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "strip_chars = string.punctuation\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    no_punct = tf.strings.regex_replace(lowercase,\n",
    "                                        \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "    no_links = tf.strings.regex_replace(no_punct,\n",
    "                                        \"[\\w\\s:\\-\\p{So}]*((https:|www\\.).*)$\", \"\")\n",
    "    return no_links\n",
    "\n",
    "X_vectorizer = keras.layers.TextVectorization(max_tokens=VOCAB_SIZE, output_mode='int', output_sequence_length=SEQUENCE_LENGTH)\n",
    "y_vectorizer = keras.layers.TextVectorization(max_tokens=VOCAB_SIZE, output_mode='int', output_sequence_length=SEQUENCE_LENGTH + 1, standardize=custom_standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There now are 140 records in the dataset\n"
     ]
    }
   ],
   "source": [
    "X, y = [], []\n",
    "for _, row in metadata_train.iterrows():\n",
    "    if os.path.isfile(get_path(row)) and type(row['episode_description'])==str:\n",
    "        transcription = get_transcription(row)\n",
    "        if len(transcription.split()) < 1200 and len(transcription.split()) > 1:\n",
    "            X.append(get_transcription(row))\n",
    "            y.append(row['episode_description'])\n",
    "print(f\"There now are {len(X)} records in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X, y = clean_with_idf(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[start]It’s a mantra many live by. Jennifer San Marco had strange behaviors. It was clear to many that the woman suffered from mental illness that was being untreated. And, on January 30th 2006 her strange behaviors bubbled over and the Goleta Postal Facility shootings began. Jennifer San Marco Kills (2006)[end]',\n",
       " '[start]Liz looks at the evidence for evolution for your GCSE Biology exam. Ideal for preparing your for GCSE Biology exam.[end]',\n",
       " '[start]Trans women and YouTubers Gage Adkins and Jae Noel discuss LGBTQ+ topics/stories from their own experiences.[end]',\n",
       " \"[start]I have from Happiful Magazine and Counselling Directory launches on 7 January 2019. We'll hear from great people who have spoken about mental health, and learn more about the passions that shape their lives, as well as reflections on their own mental health.[end]\",\n",
       " \"[start]Shout it out on tonight's Chompers![end]\",\n",
       " '[start]This is a very exciting bonus episode. Isa Mazzei joins me to discuss her film, \"Cam.\"[end]',\n",
       " '[start]How can you completely change your interaction with men?[end]',\n",
       " '[start]I decided to experiment by quitting social media to focus more on my own goals, away from comparing myself to others.[end]',\n",
       " '[start]If you like ASMR you will love this White Noise Machine on Amazon![end]',\n",
       " '[start]We often associate a life of luxury with money, but in a time-poor world is the most luxurious thing we can offer ourselves a little space? Perhaps your idea of luxury isn’t a holiday, or a fancy car - but instead it’s an afternoon nap, or a walk in the park? We create shared experiences which bring people together and celebrate the power of doing less in a more meaningful way.[end]',\n",
       " '[start]My Loves! We are going to dive into our truth and stand in the light of our bravery that has brought us so far! I will provide more information as we move along, but truly, I am so excited to share my vulnerability and realness with you.[end]',\n",
       " '[start]This study guide is for Block 6 (PEDS and OB). EMT students and students preparing for NREMT test can find this lecture very helpful.[end]',\n",
       " '[start]What we focus on expands, so let\\'s fill our life with supporting \"I AM\\'s\"; I Am Powerful, I AM Brilliant, I AM Desirable, I AM Wealthy, I AM Successful... these thoughts, filled with emotion, attract even stronger thoughts and emotions, eventually manifesting in our life.[end]',\n",
       " '[start]Hey everybody, thanks for tuning into Centre Sage - the podcast where I scour the corners of our planet in search of modern day sages and talk to them about their coolest projects and achievements. I do all of this in the hopes that we get to pick up a smart thing or two that we can implement in our everyday lives. Bonus points if you can count the number of car honks \\xa0in the background 📯[end]',\n",
       " '[start]With Brad championing Blockbusters, Callum a self-proclaimed indie fiend, bringing the low budget and avant-garde, with Sam sitting pretty as devils advocate in the middle (usually). There will be conflict, there will be anecdotes, there will be meaningless conversation and there will be blood! This is the film podcast not entirely about films. Dive in![end]',\n",
       " \"[start]My first time was wild! I can't tell you how much I learned about working with assignors by being one myself.[end]\",\n",
       " \"[start]Froggy has never celebrated Christmas; usually, he's taking his long winter's nap. But not this year![end]\",\n",
       " '[start]In my opinion, when done correctly this type of diet is best for controlling and preventing chronic disease – most notably diabetes and obesity – and can make you feel awesome![end]',\n",
       " '[start]In today’s session we use visualization to become relaxed and peaceful.[end]',\n",
       " '[start]Do you overly complicate the simple?[end]',\n",
       " '[start]You are too funny![end]',\n",
       " '[start]In just about everything in life we need to work with others.[end]',\n",
       " '[start]I had this question sent into me by Jesse. Paint color I was talking about Soft Focus by Behr is the paint color I mention.[end]',\n",
       " '[start]Everyone has a nagging part of them they want to get rid of or change. To level up, we realize somethings gotta give.[end]',\n",
       " '[start]Why Vixx’s Leo is in a very life threatening situation EP.[end]',\n",
       " '[start]This is how much real day traders make!! I hope you enjoy this![end]',\n",
       " \"[start]This podcast isn't only for space enthusiasts, but for any novice interested in astronomy and space travel! Subscribe to our YouTube Channel for our upcoming video podcast! Powered by News Landed LLC and Appleosophy Media.[end]\",\n",
       " '[start]Just like the Moog Modular changed the history of electronic music and Giorgio’s career, Cursor X could have a similar impact empowering a new era for the design of industrial vehicles[end]',\n",
       " '[start]Latest episode of The Oxford University Press- Complete Geography for Cambridge IGCSE- Student Revision Podcast.[end]',\n",
       " '[start]Burning up?[end]',\n",
       " '[start]To fill in: Hvis barna ikke er sultne, _______ (spiser - senere - vi) Fordi han ikke var syk, _____ (de - hjem - dro) .. 😊[end]',\n",
       " '[start]See if you can figure them out![end]',\n",
       " '[start]Make sure to stay gametastic![end]',\n",
       " '[start]Enjoy :)[end]',\n",
       " '[start]Can you HEAR where we are on Earth?[end]',\n",
       " '[start]Richard takes a second look at alternative methods of extracting metals for your GCSE Chemistry exam.[end]',\n",
       " \"[start]In this episode, she will look at an analysis of how Eric's character changes across the play. Ideal for preparing for your GCSE English Literature exam.[end]\",\n",
       " '[start]Ellen looks at types of economies for your A Level Economics exam.[end]',\n",
       " '[start]Do these things befall me deservedly or undeservedly?[end]',\n",
       " '[start]Our films also include interviews with many living Masters who have been guided by Sai Baba to help awaken the spiritual consciousness of devotees. For More information on all our films ->>[end]',\n",
       " '[start]Jono get bubbly, lit, and GLOWING as he looks at some tests for common gasses for your GCSE chemistry exam. Ideal for preparing your for GCSE Chemistry exam.[end]',\n",
       " '[start]Immerse BIble Reading Porgram (Kingdoms)[end]',\n",
       " \"[start]We're finishing our series on women in broadcasting.[end]\",\n",
       " '[start]Have allergies? A busy life? And it really ROCKS![end]',\n",
       " '[start]And listen to find out why![end]',\n",
       " '[start]Time to get into shape! Jono goes over the molecular geometries you need to know for your A Level Chemistry exam. Ideal for preparing you for your A Level Chemistry exam.[end]',\n",
       " '[start]Friday - Life Is Hard[end]',\n",
       " '[start]You’re gonna go from procrastinating on your goals to taking massive action towards them.[end]',\n",
       " '[start]Let us start this first by saying that I believe diet and lifestyle are always best, but in cases where this is not enough, taking a supplement can certainly be beneficial. In this episode Sarah-Kate gives you her list of supplements that she gets the most questions about from patients – and which she recommends you take on a daily basis.[end]',\n",
       " \"[start]We're running errands, checking off to-do lists, and missing the valuable pockets of time where we can check back in with our body. This body scan meditation will offer you a pause in your on the go day, where you can settle into stillness and check back in with your body. Please join me for 5 minutes of your day as we take a time out, to time back in.[end]\",\n",
       " \"[start]Today's topic: How to please your boss![end]\",\n",
       " \"[start]To help GFOA members build trust, we've collected stories from GFOA members about high points (and low points) in their careers.[end]\",\n",
       " '[start]Bill explains these two special verbs in simple to understand English![end]',\n",
       " '[start]A Good Omens vignette, by drawlight.[end]',\n",
       " '[start]Its time for another blast from the past with more HISTORICAL FIGURES![end]',\n",
       " '[start]A soul stirring poem written by Amit Nainawat.[end]',\n",
       " '[start]It’s time to cut that tangled mess. Cut it off.[end]',\n",
       " '[start]A quick introduction to the cycle and what you can look forward to in the coming months.[end]',\n",
       " '[start]Hosted by Mr. Robots of the Fallout Lorecast, Dave Chafinz of Vault Boys WV and Kenneth Vigue of CHAD: A Fallout 76 Podcast. Join us for our first episode arriving Saturday,[end]',\n",
       " '[start]Too warm?[end]',\n",
       " '[start]\\xa0This Hindi Bhajan \"Mera Aapki Krupa Se\" is sung by my favorite \\xa0singer. We thank our listeners for continuing \\xa0the journey with us during this entire year.[end]']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_strings(transcription, summary):\n",
    "    transcription = X_vectorizer(transcription)\n",
    "    summary = y_vectorizer(summary)\n",
    "    return ({\"encoder_inputs\": transcription, \"decoder_inputs\": summary[:, :-1],}, summary[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vectorizer.adapt(X)\n",
    "y_vectorizer.adapt(y)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((np.array(X),np.array(y)))\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.map(format_strings)\n",
    "dataset = dataset.shuffle(2048).prefetch(16).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [keras.layers.Dense(dense_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = keras.layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, token_embedding_matrix, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.token_embeddings = keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                                       output_dim=embed_dim,\n",
    "                                                       weights=[token_embedding_matrix])\n",
    "        self.position_embeddings = keras.layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoder(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [keras.layers.Dense(latent_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = keras.layers.LayerNormalization()\n",
    "        self.layernorm_3 = keras.layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating glove vocabulary...\n",
      "Creating embedding matrix...\n",
      "Creating embedding matrix...\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 300\n",
    "latent_dim = 2048\n",
    "num_heads = 8\n",
    "e = EmbeddingMatrix(embedding_dim=embed_dim)\n",
    "\n",
    "X_embedding_matrix = e.create_embedding_matrix(\n",
    "    vocabulary=X_vectorizer.get_vocabulary())\n",
    "y_embedding_matrix = e.create_embedding_matrix(\n",
    "    vocabulary=y_vectorizer.get_vocabulary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = keras.Input(shape=(None,),\n",
    "                             dtype=\"int64\",\n",
    "                             name=\"encoder_inputs\")\n",
    "x = PositionalEmbedding(SEQUENCE_LENGTH, VOCAB_SIZE, embed_dim, X_embedding_matrix)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,),\n",
    "                             dtype=\"int64\",\n",
    "                             name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim),\n",
    "                                 name=\"decoder_state_inputs\")\n",
    "x = PositionalEmbedding(SEQUENCE_LENGTH, VOCAB_SIZE, embed_dim, X_embedding_matrix)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "decoder_outputs = keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs, name=\"decoder\")\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs],\n",
    "                          decoder_outputs,\n",
    "                          name=\"transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding (Position  (None, None, 300)   3360000     ['encoder_inputs[0][0]']         \n",
      " alEmbedding)                                                                                     \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder (Transform  (None, None, 300)   4119848     ['positional_embedding[0][0]']   \n",
      " erEncoder)                                                                                       \n",
      "                                                                                                  \n",
      " decoder (Functional)           (None, None, 10000)  13377948    ['decoder_inputs[0][0]',         \n",
      "                                                                  'transformer_encoder[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 20,857,796\n",
      "Trainable params: 20,857,796\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - 954s 41s/step - loss: 0.3274 - accuracy: 0.0280\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - 506s 21s/step - loss: 0.3018 - accuracy: 0.0249\n",
      "Epoch 3/10\n",
      " 7/24 [=======>......................] - ETA: 7:07 - loss: 0.3555 - accuracy: 0.0227"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Volumes/MontaliSimone/Datasets/keras-model.ipynb Cell 22'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/MontaliSimone/Datasets/keras-model.ipynb#ch0000021?line=0'>1</a>\u001b[0m transformer\u001b[39m.\u001b[39msummary()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/MontaliSimone/Datasets/keras-model.ipynb#ch0000021?line=1'>2</a>\u001b[0m transformer\u001b[39m.\u001b[39mcompile(\u001b[39m\"\u001b[39m\u001b[39mrmsprop\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/MontaliSimone/Datasets/keras-model.ipynb#ch0000021?line=2'>3</a>\u001b[0m                     loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/MontaliSimone/Datasets/keras-model.ipynb#ch0000021?line=3'>4</a>\u001b[0m                     metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Volumes/MontaliSimone/Datasets/keras-model.ipynb#ch0000021?line=4'>5</a>\u001b[0m transformer\u001b[39m.\u001b[39;49mfit(dataset, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/engine/training.py?line=1376'>1377</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/engine/training.py?line=1377'>1378</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/engine/training.py?line=1378'>1379</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/engine/training.py?line=1379'>1380</a>\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/engine/training.py?line=1380'>1381</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/engine/training.py?line=1381'>1382</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/engine/training.py?line=1382'>1383</a>\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/engine/training.py?line=1383'>1384</a>\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/engine/training.py?line=1384'>1385</a>\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/keras/engine/training.py?line=1385'>1386</a>\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=911'>912</a>\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=913'>914</a>\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=914'>915</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=916'>917</a>\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=917'>918</a>\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=943'>944</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=944'>945</a>\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=945'>946</a>\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=946'>947</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=947'>948</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=948'>949</a>\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=949'>950</a>\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=950'>951</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=2952'>2953</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=2953'>2954</a>\u001b[0m   (graph_function,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=2954'>2955</a>\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=2955'>2956</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=2956'>2957</a>\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1848'>1849</a>\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1849'>1850</a>\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1850'>1851</a>\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1851'>1852</a>\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1852'>1853</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1853'>1854</a>\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1854'>1855</a>\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1855'>1856</a>\u001b[0m     args,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1856'>1857</a>\u001b[0m     possible_gradient_type,\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1857'>1858</a>\u001b[0m     executing_eagerly)\n\u001b[1;32m   <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1858'>1859</a>\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=496'>497</a>\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=497'>498</a>\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=498'>499</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=499'>500</a>\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=500'>501</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=501'>502</a>\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=502'>503</a>\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=503'>504</a>\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=504'>505</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=505'>506</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=506'>507</a>\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=507'>508</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=510'>511</a>\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=511'>512</a>\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     <a href='file:///opt/anaconda3/envs/nlp1/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "transformer.summary()\n",
    "transformer.compile(\"rmsprop\",\n",
    "                    loss=\"sparse_categorical_crossentropy\",\n",
    "                    metrics=[\"accuracy\"])\n",
    "transformer.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[start] and the and the and the and the and the and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "[start] and the and the and the and the and the and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "[start] and the and the and the and the and the and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "[start] and the and the and the and the and the and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "[start] and the and the and the and the and the and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n"
     ]
    }
   ],
   "source": [
    "y_vocab = y_vectorizer.get_vocabulary()\n",
    "y_index_lookup = dict(zip(range(len(y_vocab)), y_vocab))\n",
    "max_decoded_sentence_length = 30\n",
    "min_length = 10\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = X_vectorizer([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = y_vectorizer([decoded_sentence\n",
    "                                                      ])[:, :-1]\n",
    "        predictions = transformer(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(\n",
    "            predictions[0,\n",
    "                        i, :]) if i >min_length else np.argsort(predictions[0,\n",
    "                                                                     i, :])[-2] # Cannot take [end] right after [start]\n",
    "        sampled_token = y_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for _ in range(5):\n",
    "    input_sentence = random.choice(X)\n",
    "    summarized = decode_sequence(input_sentence)\n",
    "    print(summarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63e28586807c6502c782d898cf9a0cc5787bb3d77952b17b51ec8bdcd4044a3d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
