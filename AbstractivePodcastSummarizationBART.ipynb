{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Abstractive Summarization of Long Podcast Transcripts with BART using Semantic Self-segmentation\n",
    "Podcasts are a rapidly growing medium for news, commentary, entertainment, and learning.  Some podcast shows release new episodes on a regular schedule (daily, weekly, etc); others irregularly.  Some podcast shows feature short episodes of 5 minutes or less touching on one or two topics; others may release 3+ hour long episodes touching on a wide range of topics.  Some are structured as news delivery, some as conversations, some as storytelling.\n",
    "\n",
    "Given a podcast episode, its audio, and transcription, return a short text snippet capturing the most important information in the content. Returned summaries should be grammatical, standalone statement of significantly shorter length than the input episode description.\n",
    "\n",
    "The user task is to provide a short text summary that the user might read when deciding whether to listen to a podcast. Thus the summary should accurately convey the content of the podcast, and be short enough to quickly read on a smartphone screen. It should also be human-readable.\n",
    "\n",
    "For further information about the challenge, take a look to Podcasts Track Guidelines:\n",
    "- [TREC 2020 Podcasts Track](https://trecpodcasts.github.io/participant-instructions-2020.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import pysbd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from datasets import Dataset\n",
    "from datasets import load_metric\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rouge import Rouge\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "\n",
    "from transcript_utils import get_path, get_transcription, semantic_segmentation, extract_features\n",
    "\n",
    "# Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dataset reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:  Index(['show_uri', 'show_name', 'show_description', 'publisher', 'language',\n",
      "       'rss_link', 'episode_uri', 'episode_name', 'episode_description',\n",
      "       'duration', 'show_filename_prefix', 'episode_filename_prefix'],\n",
      "      dtype='object')\n",
      "Shape:  (105360, 12)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.join(os.path.abspath(\"D:/\"), 'podcasts-no-audio-13GB')\n",
    "# dataset_path = os.path.join(os.path.abspath(\"\"), 'podcasts-no-audio-13GB')\n",
    "\n",
    "metadata_path_train = os.path.join(dataset_path, 'metadata.tsv')\n",
    "metadata_train = pd.read_csv(metadata_path_train, sep='\\t')\n",
    "print(\"Columns: \", metadata_train.columns)\n",
    "print(\"Shape: \", metadata_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics about episode duration:\n",
      "count    105360.000000\n",
      "mean         33.845715\n",
      "std          22.735674\n",
      "min           0.175317\n",
      "25%          13.552638\n",
      "50%          31.643375\n",
      "75%          50.446825\n",
      "max         304.953900\n",
      "Name: duration, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Statistics about episode duration:\\n\"\n",
    "      f\"{metadata_train['duration'].describe()}\")\n",
    "metadata_train['duration'].hist(bins=1000, figsize=(10,5), log=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics about number of episodes per show:\n",
      "count    18376.000000\n",
      "mean         5.733566\n",
      "std         19.310585\n",
      "min          1.000000\n",
      "25%          1.000000\n",
      "50%          2.000000\n",
      "75%          4.000000\n",
      "max       1072.000000\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_episodes = metadata_train.groupby(['show_filename_prefix']).apply(lambda x: list(zip(x['episode_filename_prefix'], x['episode_description']))).to_dict()\n",
    "show_n_episodes = {k: len(v) for k, v in show_episodes.items()}\n",
    "print(\"Statistics about number of episodes per show:\\n\"\n",
    "      f\"{pd.Series(show_n_episodes.values()).describe()}\")\n",
    "pd.Series(show_n_episodes.values()).hist(bins=1000, figsize=(10,5), log=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Dataset cleaning\n",
    "We filtered the descriptions to establish a subset that is more appropriate as a ground truth set compared to full set of descriptions.\n",
    "\n",
    "\n",
    "First of all, some of the episodes contain a `NaN` value in the `episode_description` and `show_description` columns. Let's remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping NaN values: \n",
      " show_uri                   False\n",
      "show_name                  False\n",
      "show_description            True\n",
      "publisher                  False\n",
      "language                   False\n",
      "rss_link                   False\n",
      "episode_uri                False\n",
      "episode_name               False\n",
      "episode_description         True\n",
      "duration                   False\n",
      "show_filename_prefix       False\n",
      "episode_filename_prefix    False\n",
      "dtype: bool\n",
      "\n",
      "After dropping NaN values:\n",
      " show_uri                   False\n",
      "show_name                  False\n",
      "show_description           False\n",
      "publisher                  False\n",
      "language                   False\n",
      "rss_link                   False\n",
      "episode_uri                False\n",
      "episode_name               False\n",
      "episode_description        False\n",
      "duration                   False\n",
      "show_filename_prefix       False\n",
      "episode_filename_prefix    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(\"Before dropping NaN values: \\n\", metadata_train.isna().any())\n",
    "metadata_train.dropna(subset=['episode_description', 'show_description'], inplace=True)\n",
    "print(\"\\nAfter dropping NaN values:\\n\", metadata_train.isna().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also available a *gold dataset* of 150 episodes composed by 6 set of summaries for each episode (900 document-summary-grade triplets) that were graded on the Bad/Fair/Good/Excellent scale (0-3).\n",
    "Before starting the cleaning process, we merged this gold dataset with the dataset we are going to clean, and the best summary of each episode will be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path_gold = os.path.join(dataset_path, '150gold.tsv')\n",
    "metadata_gold = pd.read_csv(metadata_path_gold, sep='\\t')\n",
    "\n",
    "quality = {\n",
    "    'B': 1,\n",
    "    'F': 2,\n",
    "    'G': 3,\n",
    "    'E': 4\n",
    "}\n",
    "\n",
    "# convert egfb columns to a quality score\n",
    "egfb_columns = ['EGFB', 'EGFB.1', 'EGFB.2', 'EGFB.3', 'EGFB.4', 'EGFB.5']\n",
    "egfb_to_quality = metadata_gold[egfb_columns].applymap(lambda x: quality[x])\n",
    "\n",
    "# remove rows with no quality > 1\n",
    "egfb_to_quality = egfb_to_quality[[any(row > 1) for row in egfb_to_quality.values]] \n",
    "\n",
    "# select the best transcript for each episode\n",
    "best_egfb = egfb_to_quality.apply(lambda x: x.idxmax(), axis=1)\n",
    "best_summary = [metadata_gold.iloc[i, np.argwhere(metadata_gold.columns == egfb)[0][0] - 1] for i, egfb in best_egfb.iteritems()]\n",
    "\n",
    "metadata_gold = metadata_gold.loc[best_egfb.index]\n",
    "metadata_gold['best_summary'] = best_summary\n",
    "\n",
    "# create a dictionary of the best summary for each episode\n",
    "gold_summaries = {row['episode id']: row['best_summary'] for i, row in metadata_gold.iterrows()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# substitute the episode descriptions correspondent to the episodes in the gold set with the best summary\n",
    "for i, row in metadata_train.iterrows():\n",
    "    if row['episode_uri'] in gold_summaries.keys():\n",
    "        metadata_train.at[i, 'episode_description'] = gold_summaries[row['episode_uri']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We strive to enhance the quality of creator descriptions using heuristics. In order to do that, the following cleaning steps are preformed:\n",
    "- remove sentences that contain URLs, email addresses in the episode descriptions\n",
    "- remove tokens corresponding to @mentions, #hashtags and emojii\n",
    "- removing the content after `\"---\"` that usually is a sponsorship or a boilerplate (e.g., “--- This episode is sponsored by ...” “--- Send in a voice message”)\n",
    "- identify sentences that contain not useful content and remove them from the descriptions. In order to do that, we compute a *salience score* for each sentence of the description by summing over word IDF scores. Then we remove sentences if their salience scores are lower than a threshold. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing word frequencies: 100%|██████████| 105153/105153 [07:58<00:00, 219.77it/s]\n"
     ]
    }
   ],
   "source": [
    "def compute_document_frequencies(descriptions):\n",
    "    \"\"\"\n",
    "    Compute the document frequencies in the whole dataset descriptions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    descriptions : list of str\n",
    "        The descriptions of the episodes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary of word frequencies\n",
    "    \"\"\"\n",
    "    seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "\n",
    "    # get a set of words contained in each description (words are all lowercase)\n",
    "    flattened_descriptions = []\n",
    "    for description in tqdm(descriptions, desc=\"Computing word frequencies\"):\n",
    "        description_set = set()\n",
    "        for sentence in seg.segment(description):\n",
    "            description_set.update([word.lower() for word in word_tokenize(sentence)])\n",
    "        flattened_descriptions.extend(list(description_set))\n",
    "            \n",
    "    counts = pd.Series(Counter(flattened_descriptions))  # Get counts and transform to Series\n",
    "    return counts\n",
    "\n",
    "# compute the document frequencies that will be used to compute the sentence salience score\n",
    "document_frequencies = compute_document_frequencies(metadata_train['episode_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the old dataframe to make comparisons\n",
    "metadata_train_old = metadata_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Removing boilerplate from the episode descriptions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105153/105153 [00:00<00:00, 120642.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing links and sponsors from the episode descriptions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105153/105153 [06:19<00:00, 276.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing emojii from the episode descriptions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105153/105153 [00:00<00:00, 124739.31it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_boilerplate(description):\n",
    "    \"\"\"\n",
    "    Remove boilerplate from the episode description\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    description : str\n",
    "        The episode description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A description without boilerplate (str)\n",
    "    \"\"\"\n",
    "    boilerplate_re = re.compile(r\"---.*\")\n",
    "    return boilerplate_re.sub(\"\", description)\n",
    "\n",
    "def remove_link_or_sponsors(description):\n",
    "    \"\"\"\n",
    "    Remove sentences containing links and sponsors or username and hashtag from the episode description\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    description : str\n",
    "        The episode description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A description without links and sponsors (str)\n",
    "    \"\"\"\n",
    "    username_and_hashtag_re = re.compile(r\"(\\B@\\w+|\\B#\\w+)\")\n",
    "    links_or_sponsors_re = re.compile(\n",
    "        r\"(http|https|[pP]atreon|[eE]mail|[dD]onate|IG|[iI]nstagram|[fF]acebook|[yY]outube|[tT]witter|[dD]iscord|[fF]ollow|[sS]potify)\"\n",
    "    )\n",
    "\n",
    "    # remove username and hashtag\n",
    "    description = username_and_hashtag_re.sub(\" \", description)\n",
    "\n",
    "    # remove sentences containing links and sponsors\n",
    "    seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "    sentences = seg.segment(description)\n",
    "    sentences = [sentence for sentence in sentences if not links_or_sponsors_re.search(sentence)] \n",
    "    return \" \".join(sentences)\n",
    "\n",
    "def remove_emojii(description):\n",
    "    \"\"\"\n",
    "    Remove emojii from the episode description\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    description : str\n",
    "        The episode description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A description without emojii (str)\n",
    "    \"\"\"\n",
    "    emoji_re = re.compile(r\"[^\\x00-\\x7F]+\")\n",
    "    return emoji_re.sub(\" \", description)\n",
    "\n",
    "print(\"\\nRemoving boilerplate from the episode descriptions:\")\n",
    "metadata_train['episode_description'] = metadata_train['episode_description'].progress_map(remove_boilerplate)\n",
    "\n",
    "print(\"Removing links and sponsors from the episode descriptions:\")\n",
    "metadata_train['episode_description'] = metadata_train['episode_description'].progress_map(remove_link_or_sponsors)\n",
    "\n",
    "print(\"Removing emojii from the episode descriptions:\")\n",
    "metadata_train['episode_description'] = metadata_train['episode_description'].progress_map(remove_emojii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Examples of comparisons before and after removing sponsors and links:\n",
      "BEFORE:\n",
      "\t- If you like ASMR you will love this White Noise Machine on Amazon! Tap here to check it out! If you enjoyed this make sure to give us a 5 star rating!  ---   This episode is sponsored by  · Anchor: The easiest way to make a podcast.  https://anchor.fm/app  Support this podcast: https://anchor.fm/AdamDino/support\n",
      "AFTER:\n",
      "\t- If you like ASMR you will love this White Noise Machine on Amazon!  Tap here to check it out!  If you enjoyed this make sure to give us a 5 star rating!  \n",
      "\n",
      "\n",
      "BEFORE:\n",
      "\t- Danielle and Jessi could talk your ears off when it comes to this topic. Episode 004 is all about their skincare routines, products they love, and tips and tricks for feeling radiant and confident in your own skin. Follow them @basicallyorganicpodcast (and @jessimechler @itsdaniellebridges) for tags of all the brands they’re currently loving! Rate and subscribe!!   ---   Support this podcast: https://anchor.fm/basicallyorganicpodcast/support\n",
      "AFTER:\n",
      "\t- Danielle and Jessi could talk your ears off when it comes to this topic.  Episode 004 is all about their skincare routines, products they love, and tips and tricks for feeling radiant and confident in your own skin.  Rate and subscribe!!   \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see a few examples of comparisons between the old and new descriptions\n",
    "samples = [137, 172]\n",
    "print(\"\\nExamples of comparisons before and after removing sponsors and links:\")\n",
    "for i in samples:\n",
    "        print(\"BEFORE:\" \n",
    "                f\"\\n\\t- {metadata_train_old['episode_description'].iloc[i]}\")\n",
    "        print(\"AFTER:\"\n",
    "                f\"\\n\\t- {metadata_train['episode_description'].iloc[i]}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105153/105153 [24:23<00:00, 71.84it/s] \n"
     ]
    }
   ],
   "source": [
    "def sentence_salience_score(sentence, num_descriptions, document_frequencies):\n",
    "    \"\"\"\n",
    "    Compute the salience score of a sentence by summing over word IDF scores.\n",
    "    Only alphabetic words that are longer that one character and are neither stop words nor words like 'episode' or 'podcast'\n",
    "    are considered when computing sentence salience scores.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence : str\n",
    "        The sentence to compute the salience score for\n",
    "    num_descriptions : int\n",
    "        The number of descriptions in the dataset\n",
    "    document_frequencies : pandas.Series\n",
    "        The document frequencies in the whole dataset descriptions\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    The salience score of the sentence (float)\n",
    "    \"\"\"\n",
    "    idf_scores = []\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "\n",
    "    # compute IDF scores for each word in the sentence and sum them up \n",
    "    \n",
    "    for word in tokenized_sentence:\n",
    "        lower_world = word.lower()\n",
    "        # consider only alphabetic words, and remove stop words, single character\n",
    "        if lower_world in document_frequencies.keys() and lower_world.isalpha() and lower_world not in stopwords.words('english') and len(lower_world) > 1 and lower_world not in ['episode', 'podcast']:\n",
    "            # get document frequency\n",
    "            df = document_frequencies[lower_world]\n",
    "\n",
    "            # compute idf score\n",
    "            idf_score = np.log(num_descriptions/df)\n",
    "            idf_scores.append(idf_score)\n",
    "\n",
    "    idf_scores = np.array(idf_scores) \n",
    "    salience_score = idf_scores.mean() if len(idf_scores)>0 else 0.0\n",
    "    return salience_score\n",
    "\n",
    "def remove_unuseful_sentences(description, num_descriptions, word_frequencies, threshold=3.6):\n",
    "    \"\"\"\n",
    "    Remove sentences that are not useful for the transcriptions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    description : str\n",
    "        The episode description\n",
    "    num_descriptions : int\n",
    "        The number of descriptions in the dataset\n",
    "    word_frequencies : pandas.Series\n",
    "        The word frequencies in the whole dataset descriptions\n",
    "    threshold : double\n",
    "        The threshold for the salience score of a sentence to be considered useful\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A description without unuseful sentences (str)\n",
    "    \"\"\"\n",
    "    # segment the text into sentences\n",
    "    seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "    sentences = seg.segment(description)\n",
    "    # remove sentences that are not useful for the transcriptions\n",
    "    sentences = [sentence for sentence in sentences if sentence_salience_score(sentence, num_descriptions, word_frequencies) > threshold]\n",
    "    return \" \".join(sentences)\n",
    "\n",
    "metadata_train['episode_description'] = metadata_train['episode_description'].progress_map(lambda x: remove_unuseful_sentences(x, metadata_train.shape[0], document_frequencies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Examples of comparisons before and after removing sponsors and links:\n",
      "BEFORE:\n",
      "\t- If you like ASMR you will love this White Noise Machine on Amazon! Tap here to check it out! If you enjoyed this make sure to give us a 5 star rating!  ---   This episode is sponsored by  · Anchor: The easiest way to make a podcast.  https://anchor.fm/app  Support this podcast: https://anchor.fm/AdamDino/support\n",
      "AFTER:\n",
      "\t- If you like ASMR you will love this White Noise Machine on Amazon!   Tap here to check it out!  \n",
      "\n",
      "\n",
      "BEFORE:\n",
      "\t- Danielle and Jessi could talk your ears off when it comes to this topic. Episode 004 is all about their skincare routines, products they love, and tips and tricks for feeling radiant and confident in your own skin. Follow them @basicallyorganicpodcast (and @jessimechler @itsdaniellebridges) for tags of all the brands they’re currently loving! Rate and subscribe!!   ---   Support this podcast: https://anchor.fm/basicallyorganicpodcast/support\n",
      "AFTER:\n",
      "\t- Danielle and Jessi could talk your ears off when it comes to this topic.   Episode 004 is all about their skincare routines, products they love, and tips and tricks for feeling radiant and confident in your own skin.  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see a few examples of comparisons between the old and new descriptions\n",
    "samples = [137, 172]\n",
    "print(\"\\nExamples of comparisons before and after removing unuseful sentences:\")\n",
    "for i in samples:\n",
    "        print(\"BEFORE:\" \n",
    "                f\"\\n\\t- {metadata_train_old['episode_description'].iloc[i]}\")\n",
    "        print(\"AFTER:\"\n",
    "                f\"\\n\\t- {metadata_train['episode_description'].iloc[i]}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to select a subset of the corpus that is suitable for training supervised models, we filtered the descriptions using three heuristics shown in the table below. These filters overlap to some extent, and remove about a third of the entire set. The remaining episodes we call the **Brass Set**.\n",
    "\n",
    "| Criterion                        | Threshold                                                    |\n",
    "| -------------------------------- | ------------------------------------------------------------ |\n",
    "| Length                           | descriptions that are very long (> 750 characters) or short (< 20 characters). |\n",
    "| Similarity to show description   | descriptions with high lexical overlap (over 50%) with their show description. |\n",
    "| Similarity to other descriptions | descriptions with high lexical overlap (over 60%) with other episode descriptions in the same show. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105153/105153 [00:01<00:00, 77508.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 17108 episodes (16.27%) because of too long or too short descriptions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88045/88045 [01:16<00:00, 1157.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1597 episodes (1.81%) because of too high overlap with the show description\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86448/86448 [41:22<00:00, 34.82it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 9033 episodes (10.45%) because of too high overlap with other descriptions in the same show\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def check_lenght_brass(episode, upper_bound=750, lower_bound=20):\n",
    "    \"\"\"\n",
    "    Check if the episode descriptions is not too long (> 750 characters) or not too short (< 20 characters)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    episode : pandas.Series\n",
    "        A row from the metadata file\n",
    "    upper_bound : int\n",
    "        The upper bound of the episode description length\n",
    "    lower_bound : int\n",
    "        The lower bound of the episode description length\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Boolean indicating if the episode description is long enough\n",
    "    \"\"\"\n",
    "    return len(episode['episode_description']) <= upper_bound and len(episode['episode_description']) >= lower_bound\n",
    "    \n",
    "def description_similarity(a, b):\n",
    "    \"\"\"\n",
    "    Measure the overlapping between two descriptions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : str\n",
    "        The first description\n",
    "    b : str\n",
    "        The second description\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Value indicating the overlapping between the two descriptions\n",
    "    \"\"\"\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def check_show_description_overlap_brass(episode, thresh=0.5):\n",
    "    \"\"\"\n",
    "    Check if the episode descriptions overlapping with the show description is not too high (< 0.5)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    episode : pandas.Series\n",
    "        A row from the metadata file\n",
    "    thresh : float\n",
    "        The threshold of the overlap between the episode description and the show description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Boolean indicating if the episode description is different enough from the show description\n",
    "    \"\"\"\n",
    "    return description_similarity(episode['show_description'], episode['episode_description']) < thresh\n",
    "    \n",
    "def check_other_description_overlap_brass(episode, show_episodes, thresh=0.6):\n",
    "    \"\"\"\n",
    "    Check if the episode descriptions overlapping with the other description in the same show is not too high (< 0.6)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    episode : pandas.Series\n",
    "        A row from the metadata file\n",
    "    show_episodes : dict\n",
    "        A dictionary of the episodes of the same show\n",
    "    thresh : float\n",
    "        The threshold of the overlap between the episode description and the other description\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Boolean indicating if the episode description is different enough from the other description\n",
    "    \"\"\"\n",
    "    for other_prefix, other_description in show_episodes[episode['show_filename_prefix']]:\n",
    "        if other_prefix != episode['episode_filename_prefix'] and description_similarity(episode['episode_description'], other_description) > thresh and len(episode['episode_description']) < len(other_description):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "brass_set_lenght = metadata_train[metadata_train.progress_apply(check_lenght_brass, axis=1)]\n",
    "print(f\"Removed {len(metadata_train) - len(brass_set_lenght)} episodes ({(100-(len(brass_set_lenght)/len(metadata_train)*100)):.2f}%) because of too long or too short descriptions\")\n",
    "\n",
    "brass_set_show_overlap = brass_set_lenght[brass_set_lenght.progress_apply(check_show_description_overlap_brass, axis=1)]\n",
    "print(f\"Removed {len(brass_set_lenght) - len(brass_set_show_overlap)} episodes ({(100-(len(brass_set_show_overlap)/len(brass_set_lenght)*100)):.2f}%) because of too high overlap with the show description\")\n",
    "\n",
    "show_episodes = brass_set_show_overlap.groupby(['show_filename_prefix']).apply(lambda x: list(zip(x['episode_filename_prefix'], x['episode_description']))).to_dict()\n",
    "brass_set = brass_set_show_overlap[brass_set_show_overlap.progress_apply(lambda x: check_other_description_overlap_brass(x, show_episodes), axis=1)]\n",
    "print(f\"Removed {len(brass_set_show_overlap) - len(brass_set)} episodes ({(100-(len(brass_set)/len(brass_set_show_overlap)*100)):.2f}%) because of too high overlap with other descriptions in the same show\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode description: \n",
      "\tLife and fashion all packed into a panini \n",
      "Show description: \n",
      "\tLife and fashion all packed into a panini\n",
      "Overlapping score: \n",
      "\t0.9879518072289156\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tToday, three of the worlds straightest males have gathered to talk about the straightest things. \n",
      "Show description: \n",
      "\tOn the Wearings-Socks Podcast, three of the worlds straightest males have gathered to talk about the straightest things.\n",
      "Overlapping score: \n",
      "\t0.8663594470046083\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tWe look back at the case of the Maryland Court vs Adnan Syed and tell you what we think really happened on that fateful day in 1999 \n",
      "Show description: \n",
      "\tWe're out here doing a podcast about the Serial Podcast that is based off of the State of Maryland v. Adnan Syed case that happened back in 1999.\n",
      "Overlapping score: \n",
      "\t0.51985559566787\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look to the removed episode descriptions due to the overlap with the show description\n",
    "removed_episodes_show_overlap = pd.concat([brass_set_lenght, brass_set_show_overlap]).drop_duplicates(keep=False)[['show_description', 'episode_description']]\n",
    "removed_episodes_show_overlap['overlapping'] = removed_episodes_show_overlap.apply(lambda row: description_similarity(row['show_description'], row['episode_description']), axis=1)\n",
    "\n",
    "num_to_visualize = 3\n",
    "\n",
    "for _ in range(num_to_visualize):\n",
    "    row = removed_episodes_show_overlap.sample()\n",
    "    print(f\"Episode description: \\n\\t{row['episode_description'].values[0]}\")\n",
    "    print(f\"Show description: \\n\\t{row['show_description'].values[0]}\")\n",
    "    print(f\"Overlapping score: \\n\\t{row['overlapping'].values[0]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode description: \n",
      "\tA real banger, one for the ages  \n",
      "Other episode description: \n",
      "\tThis is a banger, one for the ages  \n",
      "Overlapping score: \n",
      "\t0.8405797101449275\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tIn this exercise, drift off to sleep while listening to nature sounds. \n",
      "Other episode description: \n",
      "\tIn the full version of this exercise, focus on muscle relaxation while listening to nature sounds. \n",
      "Overlapping score: \n",
      "\t0.7176470588235294\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tIn this episode, Shawna and Larry talk with Jason Lobmeyer about helpful tips on how to be a great CCV kids coach. \n",
      "Other episode description: \n",
      "\tIn this episode, Shawna and Larry talk with George Mang about helpful tips on how deal with behavioral issues in a kids experience. \n",
      "Overlapping score: \n",
      "\t0.7125506072874493\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look to the removed episode descriptions due to the overlap with the other episode descriptions in the same show\n",
    "removed_episodes_other_overlap = pd.concat([brass_set, brass_set_show_overlap]).drop_duplicates(keep=False)[['show_filename_prefix', 'episode_filename_prefix', 'episode_description']]\n",
    "two_episodes_show  = {str(show_filename_prefix): show_episodes[show_filename_prefix] for show_filename_prefix in removed_episodes_other_overlap['show_filename_prefix'] if len(show_episodes[show_filename_prefix]) == 2 }\n",
    "removed_episodes_other_overlap = removed_episodes_other_overlap[removed_episodes_other_overlap['show_filename_prefix'].isin(two_episodes_show.keys())]\n",
    "other_episode_show = {}\n",
    "for i, row in removed_episodes_other_overlap.iterrows():\n",
    "    if row['show_filename_prefix'] in two_episodes_show:\n",
    "        if row['episode_filename_prefix'] in two_episodes_show[row['show_filename_prefix']][0]:\n",
    "            other_episode_show[row['show_filename_prefix']] = two_episodes_show[row['show_filename_prefix']][1][1]\n",
    "        else:\n",
    "            other_episode_show[row['show_filename_prefix']] = two_episodes_show[row['show_filename_prefix']][0][1]\n",
    "removed_episodes_other_overlap['other_episode_description'] = removed_episodes_other_overlap.apply(lambda row: other_episode_show[row['show_filename_prefix']], axis=1)\n",
    "removed_episodes_other_overlap['overlapping'] = removed_episodes_other_overlap.apply(lambda row: description_similarity(row['episode_description'], row['other_episode_description']), axis=1)\n",
    "\n",
    "num_to_visualize = 3\n",
    "\n",
    "for _ in range(num_to_visualize):\n",
    "    row = removed_episodes_other_overlap.sample()\n",
    "    print(f\"Episode description: \\n\\t{row['episode_description'].values[0]}\")\n",
    "    print(f\"Other episode description: \\n\\t{row['other_episode_description'].values[0]}\")\n",
    "    print(f\"Overlapping score: \\n\\t{row['overlapping'].values[0]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Further cleaning of the data\n",
    "The podcast episodes should be restricted to the English language, but they cover a range of geographical regions and we found a number of non-English podcasts in the dataset. So we remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77415/77415 [00:26<00:00, 2925.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 374 episodes (0.48%) because of non english description\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nltk.data.find('corpora/words')\n",
    "except LookupError:\n",
    "    nltk.download('words')\n",
    "wordset = set(words.words())\n",
    "\n",
    "def is_english(text, threshold = 0.3):\n",
    "    \"\"\"\n",
    "    Check if the text is written in english\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text to check\n",
    "    threshold : float\n",
    "        The threshold of the ratio of english words in the text\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Boolean indicating if the text is written in english\n",
    "    \"\"\"\n",
    "    tokenized = word_tokenize(text)\n",
    "    alpha_tokenized = [word.lower() for word in tokenized if word.isalpha()]\n",
    "    dictionary_score = sum([word.lower() in wordset for word in alpha_tokenized\n",
    "                           ]) / len(alpha_tokenized)\n",
    "    return dictionary_score > threshold\n",
    "\n",
    "# remove episodes with non english description\n",
    "len_old_brass_set = len(brass_set)\n",
    "brass_set = brass_set[brass_set.progress_apply(lambda x: is_english(x['episode_description']), axis=1)]\n",
    "print(f\"Removed {len_old_brass_set - len(brass_set)} episodes ({(100-(len(brass_set)/len_old_brass_set*100)):.2f}%) because of non english description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store brass set\n",
    "brass_set.to_csv(os.path.join(os.path.dirname(metadata_path_train), \"brass_set.tsv\"), index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# load brass set\n",
    "brass_set = pd.read_csv(os.path.join(dataset_path, \"brass_set.tsv\"), sep='\\t')\n",
    "print(len(brass_set.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Chunck classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 88ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:13<00:00,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:26<00:00, 43.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    I love this one.   Honestly, this is great.  I...\n",
      "1    How about that?  Right?  You haven't earned th...\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# extract the chunks from the transcript, classify them and take the best ones (the ones with the highest score up to 1024 BART tokens, that can be less real worlds!!!)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "chunk1 - 0.87 - 500 BART token\n",
    "chunk2 - 0.82 - 5 BART token\n",
    "chunk3 - 0.90 - 500 BART token\n",
    "chunk4 - 0.02\n",
    "chunk5 - 0.7  \n",
    "\"\"\"\n",
    "\n",
    "def get_prob(e):\n",
    "    return e['prob']\n",
    "\n",
    "\n",
    "def transcript_extraction(episode, chunk_classifier, sentence_encoder, tokenizer):\n",
    "    \"\"\"\n",
    "    Return\n",
    "    ------\n",
    "    Transcript after the selection of the most important chunks\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    # extraction of chunks from the episode\n",
    "    chunks = semantic_segmentation(get_transcription(episode, dataset_path), sentence_encoder)\n",
    "    # extraction of features for each chunk\n",
    "    for i in range(len(chunks)):\n",
    "        features.append(extract_features(chunks[i], sentence_encoder))\n",
    "    X = np.array(features)\n",
    "    # prediction of the classifier\n",
    "    y = chunk_classifier.predict(X)\n",
    "    # score for each chunk\n",
    "    scores = [{'chunk':chunks[i], 'prob':y[i]} for i in range(len(chunks))]\n",
    "    # sorting chunks according to the probability to be usseful\n",
    "    scores.sort(key=get_prob, reverse=True)\n",
    "    # filter chunks according to a maximum amount of 1024 tokens\n",
    "    count = 0\n",
    "    i = -1\n",
    "    max_tokens = 1024\n",
    "    while count <= max_tokens and i < len(scores) - 1:\n",
    "        i += 1\n",
    "        count += len(tokenizer(' '.join(scores[i]['chunk'])))\n",
    "    if i == len(scores) - 1:\n",
    "        useful_chunks = [scores[j]['chunk'] for j in range(len(scores))]\n",
    "    else:\n",
    "        useful_chunks = [scores[j]['chunk'] for j in range(i)]\n",
    "    # create new transcript\n",
    "    merged_chunks = []\n",
    "    for chunk in useful_chunks:\n",
    "        merged_chunks.append(' '.join(chunk))\n",
    "    return ' '.join(merged_chunks)\n",
    "    \n",
    "chunk_classifier = keras.models.load_model(\"modelChunkNN\")\n",
    "model_checkpoint = \"facebook/bart-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "sentence_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "transcripts = brass_set.iloc[:2].progress_apply(lambda x: transcript_extraction(x, chunk_classifier, sentence_encoder, tokenizer), axis=1)\n",
    "print(transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(\"hhaha\")['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 26.0/26.0 [00:00<00:00, 24.8kB/s]\n",
      "Downloading: 100%|██████████| 1.59k/1.59k [00:00<00:00, 1.63MB/s]\n",
      "Downloading: 100%|██████████| 878k/878k [00:01<00:00, 636kB/s]  \n",
      "Downloading: 100%|██████████| 446k/446k [00:00<00:00, 501kB/s]  \n",
      "Downloading: 100%|██████████| 1.29M/1.29M [00:01<00:00, 966kB/s] \n"
     ]
    }
   ],
   "source": [
    "#max_input_length = 1024\n",
    "#max_target_length = 256\n",
    "\n",
    "#train_dataset = Dataset.from_pandas(train_data)\n",
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"facebook/bart-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(dataset, text_column, summary_column, max_input_length, max_target_length, padding, prefix=\"summarize: \"):\n",
    "    inputs = dataset[text_column]\n",
    "    targets = dataset[summary_column]\n",
    "    inputs = [prefix + inp for inp in inputs]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = \"max_length\"\n",
    "train_dataset = train_dataset.map(\n",
    "                lambda x: preprocess_function(x, \"transcript\", \"best_summary\", max_input_length, max_target_length, padding, prefix=\"summarize: \"),\n",
    "                batched=True,\n",
    "                remove_columns=train_dataset.column_names,\n",
    "                desc=\"Running tokenizer on train dataset\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def sample_generator(dataset, model, tokenizer, shuffle, pad_to_multiple_of=None):\n",
    "    if shuffle:\n",
    "        sample_ordering = np.random.permutation(len(dataset))\n",
    "    else:\n",
    "        sample_ordering = np.arange(len(dataset))\n",
    "    for sample_idx in sample_ordering:\n",
    "        example = dataset[int(sample_idx)]\n",
    "        # Handle dicts with proper padding and conversion to tensor.\n",
    "        example = tokenizer.pad(example, return_tensors=\"np\", pad_to_multiple_of=pad_to_multiple_of)\n",
    "        example = {key: tf.convert_to_tensor(arr, dtype_hint=tf.int32) for key, arr in example.items()}\n",
    "        if model is not None and hasattr(model, \"prepare_decoder_input_ids_from_labels\"):\n",
    "            decoder_input_ids = model.prepare_decoder_input_ids_from_labels(\n",
    "                labels=tf.expand_dims(example[\"labels\"], 0)\n",
    "            )\n",
    "            example[\"decoder_input_ids\"] = tf.squeeze(decoder_input_ids, 0)\n",
    "        yield example, example[\"labels\"]  # TF needs some kind of labels, even if we don't use them\n",
    "    return\n",
    "\n",
    "# region Helper functions\n",
    "def dataset_to_tf(dataset, model, tokenizer, total_batch_size, num_epochs, shuffle):\n",
    "    if dataset is None:\n",
    "        return None\n",
    "    train_generator = partial(sample_generator, dataset, model, tokenizer, shuffle=shuffle)\n",
    "    train_signature = {\n",
    "        feature: tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "        for feature in dataset.features\n",
    "        if feature != \"special_tokens_mask\"\n",
    "    }\n",
    "    if (\n",
    "        model is not None\n",
    "        and \"decoder_input_ids\" not in train_signature\n",
    "        and hasattr(model, \"prepare_decoder_input_ids_from_labels\")\n",
    "    ):\n",
    "        train_signature[\"decoder_input_ids\"] = train_signature[\"labels\"]\n",
    "    # This may need to be changed depending on your particular model or tokenizer!\n",
    "    padding_values = {\n",
    "        key: tf.convert_to_tensor(tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0, dtype=tf.int32)\n",
    "        for key in train_signature.keys()\n",
    "    }\n",
    "    padding_values[\"labels\"] = tf.convert_to_tensor(-100, dtype=tf.int32)\n",
    "    train_signature[\"labels\"] = train_signature[\"input_ids\"]\n",
    "    train_signature = (train_signature, train_signature[\"labels\"])\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "    tf_dataset = (\n",
    "        tf.data.Dataset.from_generator(train_generator, output_signature=train_signature)\n",
    "        .with_options(options)\n",
    "        .padded_batch(\n",
    "            batch_size=total_batch_size,\n",
    "            drop_remainder=True,\n",
    "            padding_values=(padding_values, np.array(-100, dtype=np.int32)),\n",
    "        )\n",
    "        .repeat(int(num_epochs))\n",
    "    )\n",
    "    return tf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_batch_size = 2\n",
    "num_train_epochs = 3\n",
    "learning_rate = 5e-5\n",
    "tf_train_dataset = dataset_to_tf(\n",
    "            train_dataset,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            total_batch_size=total_train_batch_size,\n",
    "            num_epochs=num_train_epochs,\n",
    "            shuffle=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "# region Optimizer, loss and LR scheduling\n",
    "# Scheduler and math around the number of training steps.\n",
    "num_update_steps_per_epoch = len(train_dataset) // total_train_batch_size\n",
    "num_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=learning_rate, num_train_steps=num_train_steps, num_warmup_steps=0\n",
    ")\n",
    "\n",
    "def masked_sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    # We clip the negative labels to 0 to avoid NaNs appearing in the output and\n",
    "    # fouling up everything that comes afterwards. The loss values corresponding to clipped values\n",
    "    # will be masked later anyway, but even masked NaNs seem to cause overflows for some reason.\n",
    "    # 1e6 is chosen as a reasonable upper bound for the number of token indices - in the unlikely\n",
    "    # event that you have more than 1 million tokens in your vocabulary, consider increasing this value.\n",
    "    # More pragmatically, consider redesigning your tokenizer.\n",
    "    losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        tf.clip_by_value(y_true, 0, int(1e6)), y_pred, from_logits=True\n",
    "    )\n",
    "    # Compute the per-sample loss only over the unmasked tokens\n",
    "    losses = tf.ragged.boolean_mask(losses, y_true != -100)\n",
    "    losses = tf.reduce_mean(losses, axis=-1)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "# region Metric\n",
    "metric = load_metric(\"rouge\")\n",
    "# endregion\n",
    "\n",
    "# region Training\n",
    "model.compile(loss={\"logits\": masked_sparse_categorical_crossentropy}, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "                tf_train_dataset,\n",
    "                epochs=int(num_train_epochs),\n",
    "                steps_per_epoch=num_update_steps_per_epoch,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transcript_exaple = train_data.iloc[45].transcript\n",
    "transcript_exaple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best summarization\n",
    "train_data.iloc[45].best_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = np.reshape(tokenizer(transcript_exaple, max_length=max_input_length, padding=padding, truncation=True).input_ids, (1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output =model.generate(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd86b5c47339ba7c128568c1fdf273a8327309a329b63c3cc5de2095ec077d11"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
