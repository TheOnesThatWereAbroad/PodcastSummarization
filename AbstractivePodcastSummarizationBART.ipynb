{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Abstractive Summarization of Long Podcast Transcripts with BART using Semantic Self-segmentation\n",
    "Podcasts are a rapidly growing medium for news, commentary, entertainment, and learning.  Some podcast shows release new episodes on a regular schedule (daily, weekly, etc); others irregularly.  Some podcast shows feature short episodes of 5 minutes or less touching on one or two topics; others may release 3+ hour long episodes touching on a wide range of topics.  Some are structured as news delivery, some as conversations, some as storytelling.\n",
    "\n",
    "Given a podcast episode, its audio, and transcription, return a short text snippet capturing the most important information in the content. Returned summaries should be grammatical, standalone statement of significantly shorter length than the input episode description.\n",
    "\n",
    "The user task is to provide a short text summary that the user might read when deciding whether to listen to a podcast. Thus the summary should accurately convey the content of the podcast, and be short enough to quickly read on a smartphone screen. It should also be human-readable.\n",
    "\n",
    "For further information about the challenge, take a look to Podcasts Track Guidelines:\n",
    "- [TREC 2020 Podcasts Track](https://trecpodcasts.github.io/participant-instructions-2020.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import pysbd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from datasets import Dataset\n",
    "from datasets import load_metric\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dataset reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:  Index(['show_uri', 'show_name', 'show_description', 'publisher', 'language',\n",
      "       'rss_link', 'episode_uri', 'episode_name', 'episode_description',\n",
      "       'duration', 'show_filename_prefix', 'episode_filename_prefix'],\n",
      "      dtype='object')\n",
      "Shape:  (105360, 12)\n"
     ]
    }
   ],
   "source": [
    "# dataset_path = os.path.join(os.path.abspath(\"D:/\"), 'podcasts-no-audio-13GB')\n",
    "dataset_path = os.path.join(os.path.abspath(\"\"), 'podcasts-no-audio-13GB')\n",
    "\n",
    "metadata_path_train = os.path.join(dataset_path, 'metadata.tsv')\n",
    "metadata_train = pd.read_csv(metadata_path_train, sep='\\t')\n",
    "print(\"Columns: \", metadata_train.columns)\n",
    "print(\"Shape: \", metadata_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(episode):\n",
    "    \"\"\"\n",
    "    Get the path of the episode json file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    episode : pandas.Series\n",
    "        A row from the metadata file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    path : str\n",
    "        The absolute path of the episode json file\n",
    "    \"\"\"\n",
    "    # extract the 2 reference number/letter to access the episode transcript\n",
    "    show_filename = episode['show_filename_prefix']\n",
    "    episode_filename = episode['episode_filename_prefix'] + \".json\"\n",
    "    dir_1, dir_2 = re.match(r'show_(\\d)(\\w).*', show_filename).groups()\n",
    "\n",
    "    # check if the transcript file in all the derived subfolders exist\n",
    "    transcipt_path = os.path.join(dataset_path, \"spotify-podcasts-2020\",\n",
    "                                \"podcasts-transcripts\", dir_1, dir_2,\n",
    "                                show_filename, episode_filename)\n",
    "    return transcipt_path\n",
    "\n",
    "def get_transcription(episode):\n",
    "    \"\"\"\n",
    "    Extract the transcript from the episode json file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    episode : pandas.Series\n",
    "        A row from the metadata file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    transcript : str\n",
    "        The transcript of the episode\n",
    "    \"\"\"\n",
    "    with open(get_path(episode), 'r') as f:\n",
    "        episode_json = json.load(f)\n",
    "        # seems that the last result in each trastcript is a repetition of the first one, so we ignore it\n",
    "        transcripts = [\n",
    "            result[\"alternatives\"][0]['transcript'] if 'transcript' in result[\"alternatives\"][0] else \"\"\n",
    "            for result in episode_json[\"results\"][:-1]\n",
    "        ]\n",
    "        return \" \".join(transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics about episode duration:\n",
      "count    105360.000000\n",
      "mean         33.845715\n",
      "std          22.735674\n",
      "min           0.175317\n",
      "25%          13.552638\n",
      "50%          31.643375\n",
      "75%          50.446825\n",
      "max         304.953900\n",
      "Name: duration, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Statistics about episode duration:\\n\"\n",
    "      f\"{metadata_train['duration'].describe()}\")\n",
    "metadata_train['duration'].hist(bins=1000, figsize=(10,5), log=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics about number of episodes per show:\n",
      "count    18376.000000\n",
      "mean         5.733566\n",
      "std         19.310585\n",
      "min          1.000000\n",
      "25%          1.000000\n",
      "50%          2.000000\n",
      "75%          4.000000\n",
      "max       1072.000000\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_episodes = metadata_train.groupby(['show_filename_prefix']).apply(lambda x: list(zip(x['episode_filename_prefix'], x['episode_description']))).to_dict()\n",
    "show_n_episodes = {k: len(v) for k, v in show_episodes.items()}\n",
    "print(\"Statistics about number of episodes per show:\\n\"\n",
    "      f\"{pd.Series(show_n_episodes.values()).describe()}\")\n",
    "pd.Series(show_n_episodes.values()).hist(bins=1000, figsize=(10,5), log=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Dataset cleaning\n",
    "We filtered the descriptions to establish a subset that is more appropriate as a ground truth set compared to full set of descriptions.\n",
    "\n",
    "\n",
    "First of all, some of the episodes contain a `NaN` value in the `episode_description` and `show_description` columns. Let's remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping NaN values: \n",
      " show_uri                   False\n",
      "show_name                  False\n",
      "show_description            True\n",
      "publisher                  False\n",
      "language                   False\n",
      "rss_link                   False\n",
      "episode_uri                False\n",
      "episode_name               False\n",
      "episode_description         True\n",
      "duration                   False\n",
      "show_filename_prefix       False\n",
      "episode_filename_prefix    False\n",
      "dtype: bool\n",
      "\n",
      "After dropping NaN values:\n",
      " show_uri                   False\n",
      "show_name                  False\n",
      "show_description           False\n",
      "publisher                  False\n",
      "language                   False\n",
      "rss_link                   False\n",
      "episode_uri                False\n",
      "episode_name               False\n",
      "episode_description        False\n",
      "duration                   False\n",
      "show_filename_prefix       False\n",
      "episode_filename_prefix    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(\"Before dropping NaN values: \\n\", metadata_train.isna().any())\n",
    "metadata_train.dropna(subset=['episode_description', 'show_description'], inplace=True)\n",
    "print(\"\\nAfter dropping NaN values:\\n\", metadata_train.isna().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also available a *gold dataset* of 150 episodes composed by 6 set of summaries for each episode (900 document-summary-grade triplets) that were graded on the Bad/Fair/Good/Excellent scale (0-3).\n",
    "Before starting the cleaning process, we merged this gold dataset with the dataset we are going to clean, and the best summary of each episode will be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path_gold = os.path.join(dataset_path, '150gold.tsv')\n",
    "metadata_gold = pd.read_csv(metadata_path_gold, sep='\\t')\n",
    "\n",
    "quality = {\n",
    "    'B': 1,\n",
    "    'F': 2,\n",
    "    'G': 3,\n",
    "    'E': 4\n",
    "}\n",
    "\n",
    "# convert egfb columns to a quality score\n",
    "egfb_columns = ['EGFB', 'EGFB.1', 'EGFB.2', 'EGFB.3', 'EGFB.4', 'EGFB.5']\n",
    "egfb_to_quality = metadata_gold[egfb_columns].applymap(lambda x: quality[x])\n",
    "\n",
    "# remove rows with no quality > 1\n",
    "egfb_to_quality = egfb_to_quality[[any(row > 1) for row in egfb_to_quality.values]] \n",
    "\n",
    "# select the best transcript for each episode\n",
    "best_egfb = egfb_to_quality.apply(lambda x: x.idxmax(), axis=1)\n",
    "best_summary = [metadata_gold.iloc[i, np.argwhere(metadata_gold.columns == egfb)[0][0] - 1] for i, egfb in best_egfb.iteritems()]\n",
    "\n",
    "metadata_gold = metadata_gold.loc[best_egfb.index]\n",
    "metadata_gold['best_summary'] = best_summary\n",
    "\n",
    "# create a dictionary of the best summary for each episode\n",
    "gold_summaries = {row['episode id']: row['best_summary'] for i, row in metadata_gold.iterrows()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# substitute the episode descriptions correspondent to the episodes in the gold set with the best summary\n",
    "for i, row in metadata_train.iterrows():\n",
    "    if row['episode_uri'] in gold_summaries.keys():\n",
    "        metadata_train.at[i, 'episode_description'] = gold_summaries[row['episode_uri']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We strive to enhance the quality of creator descriptions using heuristics. In order to do that, the following cleaning steps are preformed:\n",
    "- remove sentences that contain URLs, email addresses in the episode descriptions\n",
    "- remove tokens corresponding to @mentions, #hashtags and emojii\n",
    "- removing the content after `\"---\"` that usually is a sponsorship or a boilerplate (e.g., “--- This episode is sponsored by ...” “--- Send in a voice message”)\n",
    "- identify sentences that contain not useful content and remove them from the descriptions. In order to do that, we compute a *salience score* for each sentence of the description by summing over word IDF scores. Then we remove sentences if their salience scores are lower than a threshold. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing word frequencies: 100%|██████████| 105153/105153 [07:58<00:00, 219.77it/s]\n"
     ]
    }
   ],
   "source": [
    "def compute_document_frequencies(descriptions):\n",
    "    \"\"\"\n",
    "    Compute the document frequencies in the whole dataset descriptions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    descriptions : list of str\n",
    "        The descriptions of the episodes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary of word frequencies\n",
    "    \"\"\"\n",
    "    seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "\n",
    "    # get a set of words contained in each description (words are all lowercase)\n",
    "    flattened_descriptions = []\n",
    "    for description in tqdm(descriptions, desc=\"Computing word frequencies\"):\n",
    "        description_set = set()\n",
    "        for sentence in seg.segment(description):\n",
    "            description_set.update([word.lower() for word in word_tokenize(sentence)])\n",
    "        flattened_descriptions.extend(list(description_set))\n",
    "            \n",
    "    counts = pd.Series(Counter(flattened_descriptions))  # Get counts and transform to Series\n",
    "    return counts\n",
    "\n",
    "# compute the document frequencies that will be used to compute the sentence salience score\n",
    "document_frequencies = compute_document_frequencies(metadata_train['episode_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the old dataframe to make comparisons\n",
    "metadata_train_old = metadata_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Removing boilerplate from the episode descriptions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105153/105153 [00:00<00:00, 120642.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing links and sponsors from the episode descriptions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105153/105153 [06:19<00:00, 276.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing emojii from the episode descriptions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105153/105153 [00:00<00:00, 124739.31it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_boilerplate(description):\n",
    "    \"\"\"\n",
    "    Remove boilerplate from the episode description\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    description : str\n",
    "        The episode description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A description without boilerplate (str)\n",
    "    \"\"\"\n",
    "    boilerplate_re = re.compile(r\"---.*\")\n",
    "    return boilerplate_re.sub(\"\", description)\n",
    "\n",
    "def remove_link_or_sponsors(description):\n",
    "    \"\"\"\n",
    "    Remove sentences containing links and sponsors or username and hashtag from the episode description\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    description : str\n",
    "        The episode description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A description without links and sponsors (str)\n",
    "    \"\"\"\n",
    "    username_and_hashtag_re = re.compile(r\"(\\B@\\w+|\\B#\\w+)\")\n",
    "    links_or_sponsors_re = re.compile(\n",
    "        r\"(http|https|[pP]atreon|[eE]mail|[dD]onate|IG|[iI]nstagram|[fF]acebook|[yY]outube|[tT]witter|[dD]iscord|[fF]ollow|www|\\.com|\\*|[sS]potify)\"\n",
    "    )\n",
    "\n",
    "    # remove username and hashtag\n",
    "    description = username_and_hashtag_re.sub(\" \", description)\n",
    "\n",
    "    # remove sentences containing links and sponsors\n",
    "    seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "    sentences = seg.segment(description)\n",
    "    sentences = [sentence for sentence in sentences if not links_or_sponsors_re.search(sentence)] \n",
    "    return \" \".join(sentences)\n",
    "\n",
    "def remove_emojii(description):\n",
    "    \"\"\"\n",
    "    Remove emojii from the episode description\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    description : str\n",
    "        The episode description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A description without emojii (str)\n",
    "    \"\"\"\n",
    "    emoji_re = re.compile(r\"[^\\x00-\\x7F]+\")\n",
    "    return emoji_re.sub(\" \", description)\n",
    "\n",
    "print(\"\\nRemoving boilerplate from the episode descriptions:\")\n",
    "metadata_train['episode_description'] = metadata_train['episode_description'].progress_map(remove_boilerplate)\n",
    "\n",
    "print(\"Removing links and sponsors from the episode descriptions:\")\n",
    "metadata_train['episode_description'] = metadata_train['episode_description'].progress_map(remove_link_or_sponsors)\n",
    "\n",
    "print(\"Removing emojii from the episode descriptions:\")\n",
    "metadata_train['episode_description'] = metadata_train['episode_description'].progress_map(remove_emojii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Examples of comparisons before and after removing sponsors and links:\n",
      "BEFORE:\n",
      "\t- If you like ASMR you will love this White Noise Machine on Amazon! Tap here to check it out! If you enjoyed this make sure to give us a 5 star rating!  ---   This episode is sponsored by  · Anchor: The easiest way to make a podcast.  https://anchor.fm/app  Support this podcast: https://anchor.fm/AdamDino/support\n",
      "AFTER:\n",
      "\t- If you like ASMR you will love this White Noise Machine on Amazon!  Tap here to check it out!  If you enjoyed this make sure to give us a 5 star rating!  \n",
      "\n",
      "\n",
      "BEFORE:\n",
      "\t- Danielle and Jessi could talk your ears off when it comes to this topic. Episode 004 is all about their skincare routines, products they love, and tips and tricks for feeling radiant and confident in your own skin. Follow them @basicallyorganicpodcast (and @jessimechler @itsdaniellebridges) for tags of all the brands they’re currently loving! Rate and subscribe!!   ---   Support this podcast: https://anchor.fm/basicallyorganicpodcast/support\n",
      "AFTER:\n",
      "\t- Danielle and Jessi could talk your ears off when it comes to this topic.  Episode 004 is all about their skincare routines, products they love, and tips and tricks for feeling radiant and confident in your own skin.  Rate and subscribe!!   \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see a few examples of comparisons between the old and new descriptions\n",
    "samples = [137, 172]\n",
    "print(\"\\nExamples of comparisons before and after removing sponsors and links:\")\n",
    "for i in samples:\n",
    "        print(\"BEFORE:\" \n",
    "                f\"\\n\\t- {metadata_train_old['episode_description'].iloc[i]}\")\n",
    "        print(\"AFTER:\"\n",
    "                f\"\\n\\t- {metadata_train['episode_description'].iloc[i]}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105153/105153 [24:23<00:00, 71.84it/s] \n"
     ]
    }
   ],
   "source": [
    "def sentence_salience_score(sentence, num_descriptions, document_frequencies):\n",
    "    \"\"\"\n",
    "    Compute the salience score of a sentence by summing over word IDF scores.\n",
    "    Only alphabetic words that are longer that one character and are neither stop words nor words like 'episode' or 'podcast'\n",
    "    are considered when computing sentence salience scores.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence : str\n",
    "        The sentence to compute the salience score for\n",
    "    num_descriptions : int\n",
    "        The number of descriptions in the dataset\n",
    "    document_frequencies : pandas.Series\n",
    "        The document frequencies in the whole dataset descriptions\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    The salience score of the sentence (float)\n",
    "    \"\"\"\n",
    "    idf_scores = []\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "\n",
    "    # compute IDF scores for each word in the sentence and sum them up \n",
    "    \n",
    "    for word in tokenized_sentence:\n",
    "        lower_world = word.lower()\n",
    "        # consider only alphabetic words, and remove stop words, single character\n",
    "        if lower_world in document_frequencies.keys() and lower_world.isalpha() and lower_world not in stopwords.words('english') and len(lower_world) > 1 and lower_world not in ['episode', 'podcast']:\n",
    "            # get document frequency\n",
    "            df = document_frequencies[lower_world]\n",
    "\n",
    "            # compute idf score\n",
    "            idf_score = np.log(num_descriptions/df)\n",
    "            idf_scores.append(idf_score)\n",
    "\n",
    "    idf_scores = np.array(idf_scores) \n",
    "    salience_score = idf_scores.mean() if len(idf_scores)>0 else 0.0\n",
    "    return salience_score\n",
    "\n",
    "def remove_unuseful_sentences(description, num_descriptions, word_frequencies, threshold=3.6):\n",
    "    \"\"\"\n",
    "    Remove sentences that are not useful for the transcriptions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    description : str\n",
    "        The episode description\n",
    "    num_descriptions : int\n",
    "        The number of descriptions in the dataset\n",
    "    word_frequencies : pandas.Series\n",
    "        The word frequencies in the whole dataset descriptions\n",
    "    threshold : double\n",
    "        The threshold for the salience score of a sentence to be considered useful\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A description without unuseful sentences (str)\n",
    "    \"\"\"\n",
    "    # segment the text into sentences\n",
    "    seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "    sentences = seg.segment(description)\n",
    "    # remove sentences that are not useful for the transcriptions\n",
    "    sentences = [sentence for sentence in sentences if sentence_salience_score(sentence, num_descriptions, word_frequencies) > threshold]\n",
    "    return \" \".join(sentences)\n",
    "\n",
    "metadata_train['episode_description'] = metadata_train['episode_description'].progress_map(lambda x: remove_unuseful_sentences(x, metadata_train.shape[0], document_frequencies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Examples of comparisons before and after removing sponsors and links:\n",
      "BEFORE:\n",
      "\t- If you like ASMR you will love this White Noise Machine on Amazon! Tap here to check it out! If you enjoyed this make sure to give us a 5 star rating!  ---   This episode is sponsored by  · Anchor: The easiest way to make a podcast.  https://anchor.fm/app  Support this podcast: https://anchor.fm/AdamDino/support\n",
      "AFTER:\n",
      "\t- If you like ASMR you will love this White Noise Machine on Amazon!   Tap here to check it out!  \n",
      "\n",
      "\n",
      "BEFORE:\n",
      "\t- Danielle and Jessi could talk your ears off when it comes to this topic. Episode 004 is all about their skincare routines, products they love, and tips and tricks for feeling radiant and confident in your own skin. Follow them @basicallyorganicpodcast (and @jessimechler @itsdaniellebridges) for tags of all the brands they’re currently loving! Rate and subscribe!!   ---   Support this podcast: https://anchor.fm/basicallyorganicpodcast/support\n",
      "AFTER:\n",
      "\t- Danielle and Jessi could talk your ears off when it comes to this topic.   Episode 004 is all about their skincare routines, products they love, and tips and tricks for feeling radiant and confident in your own skin.  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see a few examples of comparisons between the old and new descriptions\n",
    "samples = [137, 172]\n",
    "print(\"\\nExamples of comparisons before and after removing unuseful sentences:\")\n",
    "for i in samples:\n",
    "        print(\"BEFORE:\" \n",
    "                f\"\\n\\t- {metadata_train_old['episode_description'].iloc[i]}\")\n",
    "        print(\"AFTER:\"\n",
    "                f\"\\n\\t- {metadata_train['episode_description'].iloc[i]}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to select a subset of the corpus that is suitable for training supervised models, we filtered the descriptions using three heuristics shown in the table below. These filters overlap to some extent, and remove about a third of the entire set. The remaining episodes we call the **Brass Set**.\n",
    "\n",
    "| Criterion                        | Threshold                                                    |\n",
    "| -------------------------------- | ------------------------------------------------------------ |\n",
    "| Length                           | descriptions that are very long (> 750 characters) or short (< 20 characters). |\n",
    "| Similarity to show description   | descriptions with high lexical overlap (over 50%) with their show description. |\n",
    "| Similarity to other descriptions | descriptions with high lexical overlap (over 60%) with other episode descriptions in the same show. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105153/105153 [00:01<00:00, 77508.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 17108 episodes (16.27%) because of too long or too short descriptions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88045/88045 [01:16<00:00, 1157.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1597 episodes (1.81%) because of too high overlap with the show description\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86448/86448 [41:22<00:00, 34.82it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 9033 episodes (10.45%) because of too high overlap with other descriptions in the same show\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def check_lenght_brass(episode, upper_bound=750, lower_bound=20):\n",
    "    \"\"\"\n",
    "    Check if the episode descriptions is not too long (> 750 characters) or not too short (< 20 characters)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    episode : pandas.Series\n",
    "        A row from the metadata file\n",
    "    upper_bound : int\n",
    "        The upper bound of the episode description length\n",
    "    lower_bound : int\n",
    "        The lower bound of the episode description length\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Boolean indicating if the episode description is long enough\n",
    "    \"\"\"\n",
    "    return len(episode['episode_description']) <= upper_bound and len(episode['episode_description']) >= lower_bound\n",
    "    \n",
    "def description_similarity(a, b):\n",
    "    \"\"\"\n",
    "    Measure the overlapping between two descriptions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : str\n",
    "        The first description\n",
    "    b : str\n",
    "        The second description\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Value indicating the overlapping between the two descriptions\n",
    "    \"\"\"\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def check_show_description_overlap_brass(episode, thresh=0.5):\n",
    "    \"\"\"\n",
    "    Check if the episode descriptions overlapping with the show description is not too high (< 0.5)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    episode : pandas.Series\n",
    "        A row from the metadata file\n",
    "    thresh : float\n",
    "        The threshold of the overlap between the episode description and the show description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Boolean indicating if the episode description is different enough from the show description\n",
    "    \"\"\"\n",
    "    return description_similarity(episode['show_description'], episode['episode_description']) < thresh\n",
    "    \n",
    "def check_other_description_overlap_brass(episode, show_episodes, thresh=0.6):\n",
    "    \"\"\"\n",
    "    Check if the episode descriptions overlapping with the other description in the same show is not too high (< 0.6)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    episode : pandas.Series\n",
    "        A row from the metadata file\n",
    "    show_episodes : dict\n",
    "        A dictionary of the episodes of the same show\n",
    "    thresh : float\n",
    "        The threshold of the overlap between the episode description and the other description\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Boolean indicating if the episode description is different enough from the other description\n",
    "    \"\"\"\n",
    "    for other_prefix, other_description in show_episodes[episode['show_filename_prefix']]:\n",
    "        if other_prefix != episode['episode_filename_prefix'] and description_similarity(episode['episode_description'], other_description) > thresh and len(episode['episode_description']) < len(other_description):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "brass_set_lenght = metadata_train[metadata_train.progress_apply(check_lenght_brass, axis=1)]\n",
    "print(f\"Removed {len(metadata_train) - len(brass_set_lenght)} episodes ({(100-(len(brass_set_lenght)/len(metadata_train)*100)):.2f}%) because of too long or too short descriptions\")\n",
    "\n",
    "brass_set_show_overlap = brass_set_lenght[brass_set_lenght.progress_apply(check_show_description_overlap_brass, axis=1)]\n",
    "print(f\"Removed {len(brass_set_lenght) - len(brass_set_show_overlap)} episodes ({(100-(len(brass_set_show_overlap)/len(brass_set_lenght)*100)):.2f}%) because of too high overlap with the show description\")\n",
    "\n",
    "show_episodes = brass_set_show_overlap.groupby(['show_filename_prefix']).apply(lambda x: list(zip(x['episode_filename_prefix'], x['episode_description']))).to_dict()\n",
    "brass_set = brass_set_show_overlap[brass_set_show_overlap.progress_apply(lambda x: check_other_description_overlap_brass(x, show_episodes), axis=1)]\n",
    "print(f\"Removed {len(brass_set_show_overlap) - len(brass_set)} episodes ({(100-(len(brass_set)/len(brass_set_show_overlap)*100)):.2f}%) because of too high overlap with other descriptions in the same show\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode description: \n",
      "\tLife and fashion all packed into a panini \n",
      "Show description: \n",
      "\tLife and fashion all packed into a panini\n",
      "Overlapping score: \n",
      "\t0.9879518072289156\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tToday, three of the worlds straightest males have gathered to talk about the straightest things. \n",
      "Show description: \n",
      "\tOn the Wearings-Socks Podcast, three of the worlds straightest males have gathered to talk about the straightest things.\n",
      "Overlapping score: \n",
      "\t0.8663594470046083\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tWe look back at the case of the Maryland Court vs Adnan Syed and tell you what we think really happened on that fateful day in 1999 \n",
      "Show description: \n",
      "\tWe're out here doing a podcast about the Serial Podcast that is based off of the State of Maryland v. Adnan Syed case that happened back in 1999.\n",
      "Overlapping score: \n",
      "\t0.51985559566787\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look to the removed episode descriptions due to the overlap with the show description\n",
    "removed_episodes_show_overlap = pd.concat([brass_set_lenght, brass_set_show_overlap]).drop_duplicates(keep=False)[['show_description', 'episode_description']]\n",
    "removed_episodes_show_overlap['overlapping'] = removed_episodes_show_overlap.apply(lambda row: description_similarity(row['show_description'], row['episode_description']), axis=1)\n",
    "\n",
    "num_to_visualize = 3\n",
    "\n",
    "for _ in range(num_to_visualize):\n",
    "    row = removed_episodes_show_overlap.sample()\n",
    "    print(f\"Episode description: \\n\\t{row['episode_description'].values[0]}\")\n",
    "    print(f\"Show description: \\n\\t{row['show_description'].values[0]}\")\n",
    "    print(f\"Overlapping score: \\n\\t{row['overlapping'].values[0]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode description: \n",
      "\tA real banger, one for the ages  \n",
      "Other episode description: \n",
      "\tThis is a banger, one for the ages  \n",
      "Overlapping score: \n",
      "\t0.8405797101449275\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tIn this exercise, drift off to sleep while listening to nature sounds. \n",
      "Other episode description: \n",
      "\tIn the full version of this exercise, focus on muscle relaxation while listening to nature sounds. \n",
      "Overlapping score: \n",
      "\t0.7176470588235294\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tIn this episode, Shawna and Larry talk with Jason Lobmeyer about helpful tips on how to be a great CCV kids coach. \n",
      "Other episode description: \n",
      "\tIn this episode, Shawna and Larry talk with George Mang about helpful tips on how deal with behavioral issues in a kids experience. \n",
      "Overlapping score: \n",
      "\t0.7125506072874493\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look to the removed episode descriptions due to the overlap with the other episode descriptions in the same show\n",
    "removed_episodes_other_overlap = pd.concat([brass_set, brass_set_show_overlap]).drop_duplicates(keep=False)[['show_filename_prefix', 'episode_filename_prefix', 'episode_description']]\n",
    "two_episodes_show  = {str(show_filename_prefix): show_episodes[show_filename_prefix] for show_filename_prefix in removed_episodes_other_overlap['show_filename_prefix'] if len(show_episodes[show_filename_prefix]) == 2 }\n",
    "removed_episodes_other_overlap = removed_episodes_other_overlap[removed_episodes_other_overlap['show_filename_prefix'].isin(two_episodes_show.keys())]\n",
    "other_episode_show = {}\n",
    "for i, row in removed_episodes_other_overlap.iterrows():\n",
    "    if row['show_filename_prefix'] in two_episodes_show:\n",
    "        if row['episode_filename_prefix'] in two_episodes_show[row['show_filename_prefix']][0]:\n",
    "            other_episode_show[row['show_filename_prefix']] = two_episodes_show[row['show_filename_prefix']][1][1]\n",
    "        else:\n",
    "            other_episode_show[row['show_filename_prefix']] = two_episodes_show[row['show_filename_prefix']][0][1]\n",
    "removed_episodes_other_overlap['other_episode_description'] = removed_episodes_other_overlap.apply(lambda row: other_episode_show[row['show_filename_prefix']], axis=1)\n",
    "removed_episodes_other_overlap['overlapping'] = removed_episodes_other_overlap.apply(lambda row: description_similarity(row['episode_description'], row['other_episode_description']), axis=1)\n",
    "\n",
    "num_to_visualize = 3\n",
    "\n",
    "for _ in range(num_to_visualize):\n",
    "    row = removed_episodes_other_overlap.sample()\n",
    "    print(f\"Episode description: \\n\\t{row['episode_description'].values[0]}\")\n",
    "    print(f\"Other episode description: \\n\\t{row['other_episode_description'].values[0]}\")\n",
    "    print(f\"Overlapping score: \\n\\t{row['overlapping'].values[0]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Further cleaning of the data\n",
    "The podcast episodes should be restricted to the English language, but they cover a range of geographical regions and we found a number of non-English podcasts in the dataset. So we remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77415/77415 [00:26<00:00, 2925.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 374 episodes (0.48%) because of non english description\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nltk.data.find('corpora/words')\n",
    "except LookupError:\n",
    "    nltk.download('words')\n",
    "wordset = set(words.words())\n",
    "\n",
    "def is_english(text, threshold = 0.3):\n",
    "    \"\"\"\n",
    "    Check if the text is written in english\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text to check\n",
    "    threshold : float\n",
    "        The threshold of the ratio of english words in the text\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Boolean indicating if the text is written in english\n",
    "    \"\"\"\n",
    "    tokenized = word_tokenize(text)\n",
    "    alpha_tokenized = [word.lower() for word in tokenized if word.isalpha()]\n",
    "    dictionary_score = sum([word.lower() in wordset for word in alpha_tokenized\n",
    "                           ]) / len(alpha_tokenized)\n",
    "    return dictionary_score > threshold\n",
    "\n",
    "# remove episodes with non english description\n",
    "len_old_brass_set = len(brass_set)\n",
    "brass_set = brass_set[brass_set.progress_apply(lambda x: is_english(x['episode_description']), axis=1)]\n",
    "print(f\"Removed {len_old_brass_set - len(brass_set)} episodes ({(100-(len(brass_set)/len_old_brass_set*100)):.2f}%) because of non english description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store brass set\n",
    "brass_set.to_csv(os.path.join(os.path.dirname(metadata_path_train), \"brass_set.tsv\"), index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load brass set\n",
    "brass_set = pd.read_csv(os.path.join(dataset_path, \"brass_set.tsv\"), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Semantic segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6  Chunck classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"Hi, I'm Ben Folds. \",\n",
       "  \"Welcome to a new podcast series that I'm hosting called Arts vote 2020 where I talked to presidential candidates about arts and politics this year. \",\n",
       "  'I joined the board of the Americans for the Arts action fund to help artists enthusiasts like you and made a better understand where these candidates stand on the issues. ',\n",
       "  'We care about former US senator and Anchorage mayor Mark begich is joining me to moderate this series as we talked to candidates about Impact of the arts and arts education our communities schools and our lives.  ',\n",
       "  'I wanted to do this podcast series because I realized that if the Arts Community wants to move the needle on the future support of the Arts. ',\n",
       "  'Then we need to act now to engage candidates on these issues. ',\n",
       "  'Our first interviews will be with 20/20 presidential candidates. ',\n",
       "  'Mayor Pete Buddha judge and congressman John Delaney. ',\n",
       "  \"We're going to try our best to interview everyone running for president to learn where they stand on the Arts. \"],\n",
       " ['Thanks for listening. ',\n",
       "  'Be sure to subscribe today to the Arts vote 2020 podcast series with Ben Folds on anchor  Or any of your favorite podcast apps, please go to Arts action fund dot org slash podcast for more information.  ']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def look_ahead_chuck(sentences, lower_chunk_size):\n",
    "    \"\"\"\n",
    "    Look-ahead function to determine the next chunk\n",
    "    \"\"\"\n",
    "    if sum([len(s) for s in sentences]) < lower_chunk_size:\n",
    "        # if the remaining sentences size is smaller than the lower bound, we return the remaining sentences\n",
    "        return sentences\n",
    "    else:\n",
    "        # next chunk size should be at least the lower bound \n",
    "        for i in range(len(sentences)):\n",
    "            if sum([len(s) for s in sentences[:i+1]]) >= lower_chunk_size:\n",
    "                return sentences[:i+1]\n",
    "\n",
    "\n",
    "def semantic_segmentation(text, model, lower_chunk_size=300, upper_chunk_size=2000):\n",
    "    \"\"\"\n",
    "    Algorithm proposed by Moro et. al. (2022) to semantically segment long inputs into GPU memory-adaptable chunks.\n",
    "    https://www.aaai.org/AAAI22Papers/AAAI-3882.MoroG.pdf\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    text: str\n",
    "        The text to be segmented\n",
    "    model: SentenceTransformer\n",
    "        The model to be used for the sentence embeddings\n",
    "    lower_chunk_size: int\n",
    "        The lower bound of the chunk size\n",
    "    upper_chunk_size: int\n",
    "        The upper bound of the chunk size\n",
    "    Return\n",
    "    -------\n",
    "    List of chunks of text\n",
    "    \"\"\"\n",
    "\n",
    "    # segment the text into sentences\n",
    "    seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "    sentences = seg.segment(text)\n",
    "\n",
    "    chuncks = []\n",
    "    current_chunk = [sentences[0]]\n",
    "\n",
    "    # Iterate over the sentences in the text\n",
    "    for i, sentence in enumerate(sentences[1:]):\n",
    "        if sentence == sentences[-1]:\n",
    "            # If the sentence is the last one, we add it to the last chunk\n",
    "            current_chunk.append(sentence)\n",
    "            chuncks.append(current_chunk)\n",
    "        elif sum([len(s) for s in current_chunk]) + len(sentence) < lower_chunk_size:\n",
    "            # standardize each chunk to a minimum size to best leverage the capability of Transformers\n",
    "            current_chunk.append(sentence)\n",
    "        elif sum([len(s) for s in current_chunk]) + len(sentence) > upper_chunk_size:\n",
    "            # if the chunk is too big, we add it to the list of chunks and start a new one\n",
    "            chuncks.append(current_chunk)\n",
    "            current_chunk = [sentence]\n",
    "        else:\n",
    "            idx = i+1\n",
    "            next_chuck = look_ahead_chuck(sentences[idx+1:], lower_chunk_size)\n",
    "            \n",
    "            # get the embedding of the previous chunk and the next chunk\n",
    "            current_embedding = model.encode(current_chunk)\n",
    "            next_embedding = model.encode(next_chuck)\n",
    "            sentence_embedding = model.encode([sentence])\n",
    "\n",
    "            # get the cosine similarity between the embedding of the embeddings\n",
    "            score_current_chunk = util.cos_sim(sentence_embedding, current_embedding).numpy().mean()\n",
    "            score_next_chunk = util.cos_sim(sentence_embedding, next_embedding).numpy().mean()\n",
    "\n",
    "            # if the score_current_chunk is higher than the score_next_chunk, we add the sentence to the current chunk\n",
    "            if score_current_chunk > score_next_chunk:\n",
    "                current_chunk.append(sentence)\n",
    "            else:\n",
    "                if sum([len(s) for s in current_chunk]) >= lower_chunk_size:\n",
    "                    chuncks.append(current_chunk)\n",
    "                    current_chunk = [sentence]\n",
    "                else:\n",
    "                    current_chunk.append(sentence)\n",
    "    return chuncks\n",
    "\n",
    "# Initialize the sentence transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "semantic_segmentation(get_transcription(metadata_train.iloc[105325]), model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Chunk Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From each chunk an encoding of each sentence is extracted using a pretrained RoBerta Transformerss to obtain a dense encoding. The encoding of the chunk is the mean of the encoding of its sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "    def extract_features(self,text):\n",
    "        \"\"\"\n",
    "        Extract features from text using a mean of the tf-idf\n",
    "        Parameters:\n",
    "            - text: string representing a document\n",
    "        Returns:\n",
    "            - extracted features\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        for sentence in text:\n",
    "          embeddings.append(self.model.encode(sentence))\n",
    "\n",
    "        features = np.mean(embeddings, axis=0)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to keep only useful chunks for each transcript we compare the chunk with the corresponding summary of the transcript it belongs to and, if the score obtained with a certain metric is below a threshold (strictly coupled with the metric), the chunk is not taken into account as a part of the transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isChunkUseful(chunk, summary, metric, threshold):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - chunk: part of the transcript\n",
    "        - summary: summary of a transcript\n",
    "        - metric: function of ariety 2 (chunk, summary) used to evaluate the summary\n",
    "        - threshold: value used to decide whether chunk is a good summary or not\n",
    "    Returns:\n",
    "        - True if the chunk is a good summary, False otherwise\n",
    "    \"\"\"\n",
    "    score = metric(chunk, summary)\n",
    "    if score < threshold:\n",
    "        result = False\n",
    "    else:\n",
    "        result = True\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen metric is BERTscore f1-score because it is a semantic metric, i.e. it takes into account the meaning of words in the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bertscore_f1_score(reference, candidate):\n",
    "    \"\"\"\n",
    "    BERTScore score, see https://github.com/huggingface/datasets/tree/master/metrics/bertscore for API\n",
    "    Parameters:\n",
    "        reference: reference translation\n",
    "        candidate: generated translation\n",
    "    Returns:\n",
    "        BERTScore f1 score\n",
    "    \"\"\"\n",
    "    bertscore = load_metric(\"bertscore\")\n",
    "    result = bertscore.compute(\n",
    "        predictions=[candidate],\n",
    "        references=[reference],\n",
    "        lang=\"en\",\n",
    "        rescale_with_baseline=True\n",
    "    )\n",
    "    return result['f1'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The golden set is used for the training and the test of the CatBooster classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of gold set\n",
    "\n",
    "metadata_path_gold = os.path.join(dataset_path, '150gold.tsv')\n",
    "metadata_gold = pd.read_csv(metadata_path_gold, sep='\\t')\n",
    "metadata_gold = pd.merge(metadata_gold, metadata_train, left_on='episode id', right_on='episode_uri')\n",
    "\n",
    "quality = {\n",
    "    'B': 1,\n",
    "    'F': 2,\n",
    "    'G': 3,\n",
    "    'E': 4\n",
    "}\n",
    "\n",
    "# convert egfb columns to a quality score\n",
    "egfb_columns = ['EGFB', 'EGFB.1', 'EGFB.2', 'EGFB.3', 'EGFB.4', 'EGFB.5']\n",
    "egfb_to_quality = metadata_gold[egfb_columns].applymap(lambda x: quality[x])\n",
    "\n",
    "# remove rows with no quality > 1\n",
    "egfb_to_quality = egfb_to_quality[[any(row > 1) for row in egfb_to_quality.values]] \n",
    "\n",
    "# select the best transcript for each episode\n",
    "best_egfb = egfb_to_quality.apply(lambda x: x.idxmax(), axis=1)\n",
    "best_summary = [metadata_gold.iloc[i, np.argwhere(metadata_gold.columns == egfb)[0][0] - 1] for i, egfb in best_egfb.iteritems()]\n",
    "\n",
    "metadata_gold = metadata_gold.loc[best_egfb.index]\n",
    "metadata_gold['best_summary'] = best_summary\n",
    "\n",
    "# add transcripts\n",
    "metadata_gold['transcript'] = metadata_gold.apply(get_transcription, axis=1)\n",
    "\n",
    "data = metadata_gold[['episode id', 'transcript', 'best_summary']]\n",
    "num_episodes = data.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\boezi\\VisualStudioProjects\\PodcastSummarization\\AbstractivePodcastSummarizationBART.ipynb Cell 39'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000036?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(chunks)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000036?line=12'>13</a>\u001b[0m     description \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mbest_summary[i]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000036?line=13'>14</a>\u001b[0m     \u001b[39mif\u001b[39;00m isChunkUseful(\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(chunks[j]), description, metric, threshold):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000036?line=14'>15</a>\u001b[0m         targets\u001b[39m.\u001b[39mappend(\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000036?line=15'>16</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32mc:\\Users\\boezi\\VisualStudioProjects\\PodcastSummarization\\AbstractivePodcastSummarizationBART.ipynb Cell 34'\u001b[0m in \u001b[0;36misChunkUseful\u001b[1;34m(chunk, summary, metric, threshold)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000033?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39misChunkUseful\u001b[39m(chunk, summary, metric, threshold):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000033?line=1'>2</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000033?line=2'>3</a>\u001b[0m \u001b[39m    Parameters:\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000033?line=3'>4</a>\u001b[0m \u001b[39m        - chunk: part of the transcript\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000033?line=8'>9</a>\u001b[0m \u001b[39m        - True if the chunk is a good summary, False otherwise\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000033?line=9'>10</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000033?line=10'>11</a>\u001b[0m     score \u001b[39m=\u001b[39m metric(chunk, summary)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000033?line=11'>12</a>\u001b[0m     \u001b[39mif\u001b[39;00m score \u001b[39m<\u001b[39m threshold:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000033?line=12'>13</a>\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\boezi\\VisualStudioProjects\\PodcastSummarization\\AbstractivePodcastSummarizationBART.ipynb Cell 36'\u001b[0m in \u001b[0;36mbertscore_f1_score\u001b[1;34m(reference, candidate)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000035?line=1'>2</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000035?line=2'>3</a>\u001b[0m \u001b[39mBERTScore score, see https://github.com/huggingface/datasets/tree/master/metrics/bertscore for API\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000035?line=3'>4</a>\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000035?line=7'>8</a>\u001b[0m \u001b[39m    BERTScore f1 score\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000035?line=8'>9</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000035?line=9'>10</a>\u001b[0m bertscore \u001b[39m=\u001b[39m load_metric(\u001b[39m\"\u001b[39m\u001b[39mbertscore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000035?line=10'>11</a>\u001b[0m result \u001b[39m=\u001b[39m bertscore\u001b[39m.\u001b[39;49mcompute(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000035?line=11'>12</a>\u001b[0m     predictions\u001b[39m=\u001b[39;49m[candidate],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000035?line=12'>13</a>\u001b[0m     references\u001b[39m=\u001b[39;49m[reference],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000035?line=13'>14</a>\u001b[0m     lang\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000035?line=14'>15</a>\u001b[0m     rescale_with_baseline\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000035?line=15'>16</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000035?line=16'>17</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result[\u001b[39m'\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\boezi\\VisualStudioProjects\\PodcastSummarization\\env\\lib\\site-packages\\datasets\\metric.py:430\u001b[0m, in \u001b[0;36mMetric.compute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/datasets/metric.py?line=427'>428</a>\u001b[0m inputs \u001b[39m=\u001b[39m {input_name: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[input_name] \u001b[39mfor\u001b[39;00m input_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures}\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/datasets/metric.py?line=428'>429</a>\u001b[0m \u001b[39mwith\u001b[39;00m temp_seed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed):\n\u001b[1;32m--> <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/datasets/metric.py?line=429'>430</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcompute_kwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/datasets/metric.py?line=431'>432</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_writer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/datasets/metric.py?line=432'>433</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_writer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\datasets_modules\\metrics\\bertscore\\23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732\\bertscore.py:166\u001b[0m, in \u001b[0;36mBERTScore._compute\u001b[1;34m(self, predictions, references, lang, model_type, num_layers, verbose, idf, device, batch_size, nthreads, all_layers, rescale_with_baseline, baseline_path, use_fast_tokenizer)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=163'>164</a>\u001b[0m \u001b[39mwith\u001b[39;00m filter_logging_context():\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=164'>165</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcached_bertscorer\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached_bertscorer\u001b[39m.\u001b[39mhash \u001b[39m!=\u001b[39m hashcode:\n\u001b[1;32m--> <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=165'>166</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached_bertscorer \u001b[39m=\u001b[39m scorer(\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=166'>167</a>\u001b[0m             model_type\u001b[39m=\u001b[39;49mmodel_type,\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=167'>168</a>\u001b[0m             num_layers\u001b[39m=\u001b[39;49mnum_layers,\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=168'>169</a>\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=169'>170</a>\u001b[0m             nthreads\u001b[39m=\u001b[39;49mnthreads,\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=170'>171</a>\u001b[0m             all_layers\u001b[39m=\u001b[39;49mall_layers,\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=171'>172</a>\u001b[0m             idf\u001b[39m=\u001b[39;49midf,\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=172'>173</a>\u001b[0m             device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=173'>174</a>\u001b[0m             lang\u001b[39m=\u001b[39;49mlang,\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=174'>175</a>\u001b[0m             rescale_with_baseline\u001b[39m=\u001b[39;49mrescale_with_baseline,\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=175'>176</a>\u001b[0m             baseline_path\u001b[39m=\u001b[39;49mbaseline_path,\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=176'>177</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=178'>179</a>\u001b[0m (P, R, F) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached_bertscorer\u001b[39m.\u001b[39mscore(\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=179'>180</a>\u001b[0m     cands\u001b[39m=\u001b[39mpredictions,\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=180'>181</a>\u001b[0m     refs\u001b[39m=\u001b[39mreferences,\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=181'>182</a>\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=182'>183</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=183'>184</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=184'>185</a>\u001b[0m output_dict \u001b[39m=\u001b[39m {\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=185'>186</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mprecision\u001b[39m\u001b[39m\"\u001b[39m: P\u001b[39m.\u001b[39mtolist(),\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=186'>187</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m\"\u001b[39m: R\u001b[39m.\u001b[39mtolist(),\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=187'>188</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m\"\u001b[39m: F\u001b[39m.\u001b[39mtolist(),\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=188'>189</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mhashcode\u001b[39m\u001b[39m\"\u001b[39m: hashcode,\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=189'>190</a>\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\boezi\\VisualStudioProjects\\PodcastSummarization\\env\\lib\\site-packages\\bert_score\\scorer.py:103\u001b[0m, in \u001b[0;36mBERTScorer.__init__\u001b[1;34m(self, model_type, num_layers, batch_size, nthreads, all_layers, idf, idf_sents, device, lang, rescale_with_baseline, baseline_path, use_fast_tokenizer)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/bert_score/scorer.py?line=100'>101</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_fast_tokenizer \u001b[39m=\u001b[39m use_fast_tokenizer\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/bert_score/scorer.py?line=101'>102</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer \u001b[39m=\u001b[39m get_tokenizer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_type, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_fast_tokenizer)\n\u001b[1;32m--> <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/bert_score/scorer.py?line=102'>103</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model \u001b[39m=\u001b[39m get_model(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_layers)\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/bert_score/scorer.py?line=103'>104</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/bert_score/scorer.py?line=105'>106</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_idf_dict \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\boezi\\VisualStudioProjects\\PodcastSummarization\\env\\lib\\site-packages\\bert_score\\utils.py:232\u001b[0m, in \u001b[0;36mget_model\u001b[1;34m(model_type, num_layers, all_layers)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/bert_score/utils.py?line=229'>230</a>\u001b[0m     model \u001b[39m=\u001b[39m T5EncoderModel\u001b[39m.\u001b[39mfrom_pretrained(model_type)\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/bert_score/utils.py?line=230'>231</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/bert_score/utils.py?line=231'>232</a>\u001b[0m     model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(model_type)\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/bert_score/utils.py?line=232'>233</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/bert_score/utils.py?line=234'>235</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mdecoder\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mencoder\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\boezi\\VisualStudioProjects\\PodcastSummarization\\env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:446\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/transformers/models/auto/auto_factory.py?line=443'>444</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/transformers/models/auto/auto_factory.py?line=444'>445</a>\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/transformers/models/auto/auto_factory.py?line=445'>446</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/transformers/models/auto/auto_factory.py?line=446'>447</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/transformers/models/auto/auto_factory.py?line=447'>448</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/transformers/models/auto/auto_factory.py?line=448'>449</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/transformers/models/auto/auto_factory.py?line=449'>450</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\boezi\\VisualStudioProjects\\PodcastSummarization\\env\\lib\\site-packages\\transformers\\modeling_utils.py:1978\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/transformers/modeling_utils.py?line=1974'>1975</a>\u001b[0m \u001b[39mif\u001b[39;00m from_pt:\n\u001b[0;32m   <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/transformers/modeling_utils.py?line=1975'>1976</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sharded \u001b[39mand\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/transformers/modeling_utils.py?line=1976'>1977</a>\u001b[0m         \u001b[39m# Time to load the checkpoint\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/transformers/modeling_utils.py?line=1977'>1978</a>\u001b[0m         state_dict \u001b[39m=\u001b[39m load_state_dict(resolved_archive_file)\n\u001b[0;32m   <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/transformers/modeling_utils.py?line=1979'>1980</a>\u001b[0m     \u001b[39m# set dtype to instantiate the model under:\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/transformers/modeling_utils.py?line=1980'>1981</a>\u001b[0m     \u001b[39m# 1. If torch_dtype is not None, we use that dtype\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/transformers/modeling_utils.py?line=1981'>1982</a>\u001b[0m     \u001b[39m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/transformers/modeling_utils.py?line=1982'>1983</a>\u001b[0m     \u001b[39m#    weights entry - we assume all weights are of the same dtype\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/transformers/modeling_utils.py?line=1983'>1984</a>\u001b[0m     \u001b[39m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/transformers/modeling_utils.py?line=1984'>1985</a>\u001b[0m     dtype_orig \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\boezi\\VisualStudioProjects\\PodcastSummarization\\env\\lib\\site-packages\\transformers\\modeling_utils.py:392\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/transformers/modeling_utils.py?line=387'>388</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/transformers/modeling_utils.py?line=388'>389</a>\u001b[0m \u001b[39mReads a PyTorch checkpoint file, returning properly formatted errors if they arise.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/transformers/modeling_utils.py?line=389'>390</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/transformers/modeling_utils.py?line=390'>391</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/transformers/modeling_utils.py?line=391'>392</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(checkpoint_file, map_location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/transformers/modeling_utils.py?line=392'>393</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/transformers/modeling_utils.py?line=393'>394</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\boezi\\VisualStudioProjects\\PodcastSummarization\\env\\lib\\site-packages\\torch\\serialization.py:713\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/torch/serialization.py?line=710'>711</a>\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mload(opened_file)\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/torch/serialization.py?line=711'>712</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m--> <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/torch/serialization.py?line=712'>713</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[1;32mc:\\Users\\boezi\\VisualStudioProjects\\PodcastSummarization\\env\\lib\\site-packages\\torch\\serialization.py:938\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/torch/serialization.py?line=935'>936</a>\u001b[0m \u001b[39massert\u001b[39;00m key \u001b[39min\u001b[39;00m deserialized_objects\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/torch/serialization.py?line=936'>937</a>\u001b[0m typed_storage \u001b[39m=\u001b[39m deserialized_objects[key]\n\u001b[1;32m--> <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/torch/serialization.py?line=937'>938</a>\u001b[0m typed_storage\u001b[39m.\u001b[39;49m_storage\u001b[39m.\u001b[39;49m_set_from_file(\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/torch/serialization.py?line=938'>939</a>\u001b[0m     f, offset, f_should_read_directly,\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/torch/serialization.py?line=939'>940</a>\u001b[0m     torch\u001b[39m.\u001b[39;49m_utils\u001b[39m.\u001b[39;49m_element_size(typed_storage\u001b[39m.\u001b[39;49mdtype))\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/torch/serialization.py?line=940'>941</a>\u001b[0m \u001b[39mif\u001b[39;00m offset \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/torch/serialization.py?line=941'>942</a>\u001b[0m     offset \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mtell()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "metric = bertscore_f1_score\n",
    "\n",
    "# creation of the dataset for chunk classification\n",
    "# creation of the targets\n",
    "\n",
    "targets = []\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    print(f\"Episode: {i+1}\")\n",
    "    chunks = semantic_segmentation(data.transcript[i], model)\n",
    "    description = data.best_summary[i]\n",
    "    num_chunks = len(chunks)\n",
    "    print(f\"Num chunks: {num_chunks}\")\n",
    "    for j in range(num_chunks):\n",
    "        print(f\"Chunk {j+1}\")\n",
    "        if isChunkUseful(' '.join(chunks[j]), description, metric, threshold):\n",
    "            targets.append(1)\n",
    "        else:\n",
    "            targets.append(0)\n",
    "\n",
    "y = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction of the features\n",
    "\n",
    "extractor = FeatureExtractor()\n",
    "features = []\n",
    "for i in len(num_episodes):\n",
    "    chunks = semantic_segmentation(data.transcript[i], model)\n",
    "    for j in range(len(chunks)):\n",
    "        chunk = chunks[j]\n",
    "        features.append(extractor.extract_features(chunk))\n",
    "\n",
    "X = np.array(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.hstack((X,y))\n",
    "df_chunk = pd.DataFrame(dataset)\n",
    "df_chunk.to_csv(\"chunks.csv\",header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# training the model\n",
    "catboost = CatBoostClassifier(iterations=2,\n",
    "                           learning_rate=1,\n",
    "                           depth=2)\n",
    "# Fit model\n",
    "catboost.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = catboost.predict(X_test)\n",
    "accuracy = accuracy(y, y_pred)\n",
    "\n",
    "print(f\"Accuracy of chunk selection: {round(accuracy,2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 256\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "\n",
    "model_checkpoint = \"facebook/bart-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(dataset, text_column, summary_column, max_input_length, max_target_length, padding, prefix=\"summarize: \"):\n",
    "    inputs = dataset[text_column]\n",
    "    targets = dataset[summary_column]\n",
    "    inputs = [prefix + inp for inp in inputs]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = \"max_length\"\n",
    "train_dataset = train_dataset.map(\n",
    "                lambda x: preprocess_function(x, \"transcript\", \"best_summary\", max_input_length, max_target_length, padding, prefix=\"summarize: \"),\n",
    "                batched=True,\n",
    "                remove_columns=train_dataset.column_names,\n",
    "                desc=\"Running tokenizer on train dataset\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def sample_generator(dataset, model, tokenizer, shuffle, pad_to_multiple_of=None):\n",
    "    if shuffle:\n",
    "        sample_ordering = np.random.permutation(len(dataset))\n",
    "    else:\n",
    "        sample_ordering = np.arange(len(dataset))\n",
    "    for sample_idx in sample_ordering:\n",
    "        example = dataset[int(sample_idx)]\n",
    "        # Handle dicts with proper padding and conversion to tensor.\n",
    "        example = tokenizer.pad(example, return_tensors=\"np\", pad_to_multiple_of=pad_to_multiple_of)\n",
    "        example = {key: tf.convert_to_tensor(arr, dtype_hint=tf.int32) for key, arr in example.items()}\n",
    "        if model is not None and hasattr(model, \"prepare_decoder_input_ids_from_labels\"):\n",
    "            decoder_input_ids = model.prepare_decoder_input_ids_from_labels(\n",
    "                labels=tf.expand_dims(example[\"labels\"], 0)\n",
    "            )\n",
    "            example[\"decoder_input_ids\"] = tf.squeeze(decoder_input_ids, 0)\n",
    "        yield example, example[\"labels\"]  # TF needs some kind of labels, even if we don't use them\n",
    "    return\n",
    "\n",
    "# region Helper functions\n",
    "def dataset_to_tf(dataset, model, tokenizer, total_batch_size, num_epochs, shuffle):\n",
    "    if dataset is None:\n",
    "        return None\n",
    "    train_generator = partial(sample_generator, dataset, model, tokenizer, shuffle=shuffle)\n",
    "    train_signature = {\n",
    "        feature: tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "        for feature in dataset.features\n",
    "        if feature != \"special_tokens_mask\"\n",
    "    }\n",
    "    if (\n",
    "        model is not None\n",
    "        and \"decoder_input_ids\" not in train_signature\n",
    "        and hasattr(model, \"prepare_decoder_input_ids_from_labels\")\n",
    "    ):\n",
    "        train_signature[\"decoder_input_ids\"] = train_signature[\"labels\"]\n",
    "    # This may need to be changed depending on your particular model or tokenizer!\n",
    "    padding_values = {\n",
    "        key: tf.convert_to_tensor(tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0, dtype=tf.int32)\n",
    "        for key in train_signature.keys()\n",
    "    }\n",
    "    padding_values[\"labels\"] = tf.convert_to_tensor(-100, dtype=tf.int32)\n",
    "    train_signature[\"labels\"] = train_signature[\"input_ids\"]\n",
    "    train_signature = (train_signature, train_signature[\"labels\"])\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "    tf_dataset = (\n",
    "        tf.data.Dataset.from_generator(train_generator, output_signature=train_signature)\n",
    "        .with_options(options)\n",
    "        .padded_batch(\n",
    "            batch_size=total_batch_size,\n",
    "            drop_remainder=True,\n",
    "            padding_values=(padding_values, np.array(-100, dtype=np.int32)),\n",
    "        )\n",
    "        .repeat(int(num_epochs))\n",
    "    )\n",
    "    return tf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_batch_size = 2\n",
    "num_train_epochs = 3\n",
    "learning_rate = 5e-5\n",
    "tf_train_dataset = dataset_to_tf(\n",
    "            train_dataset,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            total_batch_size=total_train_batch_size,\n",
    "            num_epochs=num_train_epochs,\n",
    "            shuffle=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "# region Optimizer, loss and LR scheduling\n",
    "# Scheduler and math around the number of training steps.\n",
    "num_update_steps_per_epoch = len(train_dataset) // total_train_batch_size\n",
    "num_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=learning_rate, num_train_steps=num_train_steps, num_warmup_steps=0\n",
    ")\n",
    "\n",
    "def masked_sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    # We clip the negative labels to 0 to avoid NaNs appearing in the output and\n",
    "    # fouling up everything that comes afterwards. The loss values corresponding to clipped values\n",
    "    # will be masked later anyway, but even masked NaNs seem to cause overflows for some reason.\n",
    "    # 1e6 is chosen as a reasonable upper bound for the number of token indices - in the unlikely\n",
    "    # event that you have more than 1 million tokens in your vocabulary, consider increasing this value.\n",
    "    # More pragmatically, consider redesigning your tokenizer.\n",
    "    losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        tf.clip_by_value(y_true, 0, int(1e6)), y_pred, from_logits=True\n",
    "    )\n",
    "    # Compute the per-sample loss only over the unmasked tokens\n",
    "    losses = tf.ragged.boolean_mask(losses, y_true != -100)\n",
    "    losses = tf.reduce_mean(losses, axis=-1)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "# region Metric\n",
    "metric = load_metric(\"rouge\")\n",
    "# endregion\n",
    "\n",
    "# region Training\n",
    "model.compile(loss={\"logits\": masked_sparse_categorical_crossentropy}, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "                tf_train_dataset,\n",
    "                epochs=int(num_train_epochs),\n",
    "                steps_per_epoch=num_update_steps_per_epoch,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transcript_exaple = train_data.iloc[45].transcript\n",
    "transcript_exaple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best summarization\n",
    "train_data.iloc[45].best_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = np.reshape(tokenizer(transcript_exaple, max_length=max_input_length, padding=padding, truncation=True).input_ids, (1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output =model.generate(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e800dd11dddfb1e3886769e91ed8bbe987a221798b85010fca298ae8afbc389e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
