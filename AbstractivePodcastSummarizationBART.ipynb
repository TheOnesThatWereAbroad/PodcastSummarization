{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Abstractive Summarization of Long Podcast Transcripts with BART using Semantic Self-segmentation\n",
    "Podcasts are a rapidly growing medium for news, commentary, entertainment, and learning.  Some podcast shows release new episodes on a regular schedule (daily, weekly, etc); others irregularly.  Some podcast shows feature short episodes of 5 minutes or less touching on one or two topics; others may release 3+ hour long episodes touching on a wide range of topics.  Some are structured as news delivery, some as conversations, some as storytelling.\n",
    "\n",
    "Given a podcast episode, its audio, and transcription, return a short text snippet capturing the most important information in the content. Returned summaries should be grammatical, standalone statement of significantly shorter length than the input episode description.\n",
    "\n",
    "The user task is to provide a short text summary that the user might read when deciding whether to listen to a podcast. Thus the summary should accurately convey the content of the podcast, and be short enough to quickly read on a smartphone screen. It should also be human-readable.\n",
    "\n",
    "For further information about the challenge, take a look to Podcasts Track Guidelines:\n",
    "- [TREC 2020 Podcasts Track](https://trecpodcasts.github.io/participant-instructions-2020.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import pysbd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dataset reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:  Index(['show_uri', 'show_name', 'show_description', 'publisher', 'language',\n",
      "       'rss_link', 'episode_uri', 'episode_name', 'episode_description',\n",
      "       'duration', 'show_filename_prefix', 'episode_filename_prefix'],\n",
      "      dtype='object')\n",
      "Shape:  (105360, 12)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.join(os.path.abspath(\"\"), 'podcasts-no-audio-13GB')\n",
    "\n",
    "metadata_path_train = os.path.join(dataset_path, 'metadata.tsv')\n",
    "metadata_train = pd.read_csv(metadata_path_train, sep='\\t')\n",
    "print(\"Columns: \", metadata_train.columns)\n",
    "print(\"Shape: \", metadata_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(episode):\n",
    "    \"\"\"\n",
    "    Get the path of the episode json file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    episode : pandas.Series\n",
    "        A row from the metadata file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    path : str\n",
    "        The absolute path of the episode json file\n",
    "    \"\"\"\n",
    "    # extract the 2 reference number/letter to access the episode transcript\n",
    "    show_filename = episode['show_filename_prefix']\n",
    "    episode_filename = episode['episode_filename_prefix'] + \".json\"\n",
    "    dir_1, dir_2 = re.match(r'show_(\\d)(\\w).*', show_filename).groups()\n",
    "\n",
    "    # check if the transcript file in all the derived subfolders exist\n",
    "    transcipt_path = os.path.join(dataset_path, \"spotify-podcasts-2020\",\n",
    "                                \"podcasts-transcripts\", dir_1, dir_2,\n",
    "                                show_filename, episode_filename)\n",
    "    return transcipt_path\n",
    "\n",
    "def get_transcription(episode):\n",
    "    \"\"\"\n",
    "    Extract the transcript from the episode json file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    episode : pandas.Series\n",
    "        A row from the metadata file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    transcript : str\n",
    "        The transcript of the episode\n",
    "    \"\"\"\n",
    "    with open(get_path(episode), 'r') as f:\n",
    "        episode_json = json.load(f)\n",
    "        # seems that the last result in each trastcript is a repetition of the first one, so we ignore it\n",
    "        transcripts = [\n",
    "            result[\"alternatives\"][0]['transcript'] if 'transcript' in result[\"alternatives\"][0] else \"\"\n",
    "            for result in episode_json[\"results\"][:-1]\n",
    "        ]\n",
    "        return \" \".join(transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metadata_train['duration'].hist(bins=1000, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Dataset cleaning\n",
    "\n",
    "Some of the episodes contain a `NaN` value in the `episode_description` and `show_description` columns. Let's remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping NaN values: \n",
      " show_uri                   False\n",
      "show_name                  False\n",
      "show_description            True\n",
      "publisher                  False\n",
      "language                   False\n",
      "rss_link                   False\n",
      "episode_uri                False\n",
      "episode_name               False\n",
      "episode_description         True\n",
      "duration                   False\n",
      "show_filename_prefix       False\n",
      "episode_filename_prefix    False\n",
      "dtype: bool\n",
      "\n",
      "After dropping NaN values:\n",
      " show_uri                   False\n",
      "show_name                  False\n",
      "show_description           False\n",
      "publisher                  False\n",
      "language                   False\n",
      "rss_link                   False\n",
      "episode_uri                False\n",
      "episode_name               False\n",
      "episode_description        False\n",
      "duration                   False\n",
      "show_filename_prefix       False\n",
      "episode_filename_prefix    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(\"Before dropping NaN values: \\n\", metadata_train.isna().any())\n",
    "metadata_train.dropna(subset=['episode_description', 'show_description'], inplace=True)\n",
    "print(\"\\nAfter dropping NaN values:\\n\", metadata_train.isna().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to select a subset of the corpus that is suitable for training supervised models, we filtered the descriptions using three heuristics shown in the table below. These filters overlap to some extent, and remove about a third of the entire set. The remaining 66,245 descriptions we call the Brass Set.\n",
    "\n",
    "| Criterion                        | Threshold                                                    |\n",
    "| -------------------------------- | ------------------------------------------------------------ |\n",
    "| Length                           | descriptions that are very long (> 750 characters) or short (< 20 characters) amounting to 24033 or 23% of the descriptions. |\n",
    "| Similarity to show description   | descriptions with high lexical overlap (over 40%) with their show description, amounting to 9444 or 9% of the descriptions. |\n",
    "| Similarity to other descriptions | descriptions with high lexical overlap (over 50%) with other episode descriptions amounting 15375 or 15% of the descriptions. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105153/105153 [00:01<00:00, 76314.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 25478 episodes because of too long or too short descriptions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79675/79675 [00:48<00:00, 1643.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 4221 episodes because of too high overlap with the show description\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def check_lenght_brass(episode, upper_bound=750, lower_bound=20):\n",
    "    \"\"\"\n",
    "    Check if the episode descriptions is not too long (> 750 characters) or not too short (< 20 characters)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    episode : pandas.Series\n",
    "        A row from the metadata file\n",
    "    upper_bound : int\n",
    "        The upper bound of the episode description length\n",
    "    lower_bound : int\n",
    "        The lower bound of the episode description length\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Boolean indicating if the episode description is long enough\n",
    "    \"\"\"\n",
    "    return len(episode['episode_description']) <= upper_bound and len(episode['episode_description']) >= lower_bound\n",
    "    \n",
    "def description_similarity(a, b):\n",
    "    \"\"\"\n",
    "    Measure the overlapping between two descriptions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : str\n",
    "        The first description\n",
    "    b : str\n",
    "        The second description\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Value indicating the overlapping between the two descriptions\n",
    "    \"\"\"\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def check_show_description_overlap_brass(episode, thresh=0.4):\n",
    "    \"\"\"\n",
    "    Check if the episode descriptions overlapping with the show description is not too high (< 0.4)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    episode : pandas.Series\n",
    "        A row from the metadata file\n",
    "    thresh : float\n",
    "        The threshold of the overlap between the episode description and the show description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Boolean indicating if the episode description is different enough from the show description\n",
    "    \"\"\"\n",
    "    return description_similarity(episode['show_description'], episode['episode_description']) < thresh\n",
    "    \n",
    "def check_other_description_overlap_brass(episode, all_episodes, thresh=0.5):\n",
    "    \"\"\"\n",
    "    Check if the episode descriptions overlapping with the other description is not too high (< 0.5)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    episode : pandas.Series\n",
    "        A row from the metadata file\n",
    "    all_episodes : pandas.DataFrame\n",
    "        A dataframe containing all the episodes\n",
    "    thresh : float\n",
    "        The threshold of the overlap between the episode description and the other description\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Boolean indicating if the episode description is different enough from the other description\n",
    "    \"\"\"\n",
    "    other_descriptions = all_episodes[all_episodes['episode_filename_prefix'] != episode['episode_filename_prefix']]['episode_description'].values\n",
    "    \n",
    "    # sample the 5% of the other descriptions due to the fact that the dataset is huge\n",
    "    sample_size = int(len(other_descriptions)*0.05) \n",
    "    other_descriptions_sample = random.sample(list(other_descriptions), sample_size)\n",
    "\n",
    "    for other in other_descriptions_sample:\n",
    "        if description_similarity(episode['episode_description'], other) > thresh:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "brass_set_lenght = metadata_train[metadata_train.progress_apply(check_lenght_brass, axis=1)]\n",
    "print(f\"Removed {len(metadata_train) - len(brass_set_lenght)} episodes because of too long or too short descriptions\")\n",
    "\n",
    "brass_set_show_overlap = brass_set_lenght[brass_set_lenght.progress_apply(check_show_description_overlap_brass, axis=1)]\n",
    "print(f\"Removed {len(brass_set_lenght) - len(brass_set_show_overlap)} episodes because of too high overlap with the show description\")\n",
    "\n",
    "#brass_set = brass_set_show_overlap[brass_set_show_overlap.progress_apply(lambda x: check_other_description_overlap_brass(x, brass_set_show_overlap), axis=1)]\n",
    "#print(f\"Removed {len(brass_set_show_overlap) - len(brass_set)} episodes because of too high overlap with other descriptions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Trans women and YouTubers Gage Adkins and Olivia Noel discuss LGBTQ+ topics/stories from their own experiences. As women of color in their early twenties, the two strive to create a podcast that speaks volumes by discussing issues like: transphobia, coming out in college, dysphoria and what it’s like being trans in the 21st century. Support this podcast: https://anchor.fm/girlish/support',\n",
       "        'Welcome to Girl-ish! Trans women and YouTubers Gage Adkins and Jae Noel discuss LGBTQ+ topics/stories from their own experiences. As women of color in their early twenties, the two strive to create a podcast that speaks volumes by discussing issues like: transphobia, coming out in college, dysphoria and what it’s like being trans in the 21st century.\\xa0  ---   Support this podcast: https://anchor.fm/girlish/support',\n",
       "        0.9528535980148883],\n",
       "       ['Follow us to enjoy hour long ASMR episodes that are posted every day! If you enjoy this make sure to give us a 5 star rating!  Support this podcast: https://anchor.fm/AdamDino/support',\n",
       "        'If you like ASMR you will love this White Noise Machine on Amazon! Tap here to check it out! If you enjoyed this make sure to give us a 5 star rating!  ---   This episode is sponsored by  · Anchor: The easiest way to make a podcast.  https://anchor.fm/app  Support this podcast: https://anchor.fm/AdamDino/support',\n",
       "        0.41935483870967744],\n",
       "       [\"Each Friday, join John Oliver, as he has a discussion about everyday life situations. It's a thought provoking conversation to help motivate and coach you into making the best decision possible, in order for you to pursue positive personal growth. Support this podcast: https://anchor.fm/johnoliverpodcast/support\",\n",
       "        'In the first episode of our new series, John Oliver goes over Relationships and the Drama Triangle.  ---   Support this podcast: https://anchor.fm/johnoliverpodcast/support',\n",
       "        0.4824742268041237],\n",
       "       ['A podcast for the Modern heathen, all are welcome. Come along and learn, listen, and enjoy as we learn to be heathen together. Pick up the Hammer , Horn and lets sit in the Hoff together.  You can Email me at: modernheathenman@gmail.com  You can also find us on Facebook at : https://www.facebook.com/Modern-Heathen-Man-172895453325451/ Support this podcast: https://anchor.fm/modern-heathen/support',\n",
       "        'we discuss altars and the types I have and why I have them\\xa0  ---   Send in a voice message: https://anchor.fm/modern-heathen/message Support this podcast: https://anchor.fm/modern-heathen/support',\n",
       "        0.4478114478114478],\n",
       "       ['Follow us to enjoy hour long ASMR episodes that are posted every day! If you enjoy this make sure to give us a 5 star rating!  Support this podcast: https://anchor.fm/AdamDino/support',\n",
       "        'If you enjoyed this ASMR make sure to leave us a five star rating!  ---   This episode is sponsored by  · Anchor: The easiest way to make a podcast.  https://anchor.fm/app  Support this podcast: https://anchor.fm/AdamDino/support',\n",
       "        0.42718446601941745],\n",
       "       ['Come and join me while I talk about random topics, from video games, movies, tech, news, to board games, life experiences, and much more!',\n",
       "        'Today I am joined by my sister (Meg) and her two friends Delaney and Maria. We talk about a variety of topics ranging from social media, high school life, and childhood tv shows. ',\n",
       "        0.4430379746835443],\n",
       "       ['Why bother searching for the best blogs about health & fitness when it can be found and read for you? Think of Optimal Health Daily as an audioblog or blogcast. Support this podcast: https://anchor.fm/optimal-health-daily/support',\n",
       "        \"Nia Shanks shares her thoughts on changing women's health and fitness.  ---   Support this podcast: https://anchor.fm/optimal-health-daily/support\",\n",
       "        0.56],\n",
       "       ['The Love Chat is a Youtube Channel (www.Youtube.com/C/TheLoveChat) based on having healthier relationships.  Coaching is offered at www.thelovechat.net/coaching Support this podcast: https://anchor.fm/the-love-chat/support',\n",
       "        '\\xa0 The Love Chat is a Youtube Channel (www.Youtube.com/C/TheLoveChat) based on having healthier relationships.\\xa0 Coaching is offered at www.thelovechat.net/coaching\\xa0  ---   Support this podcast: https://anchor.fm/the-love-chat/support',\n",
       "        0.973568281938326],\n",
       "       ['Welcome to Stadia Cast, your one stop shop for all things Google Stadia   Hosted by Bill and Lloyd.  Bill - @RunJumpStomp Lloyd - @Dasme Support this podcast: https://anchor.fm/stadiacast/support',\n",
       "        'shownotes at runjumpstomp.com  ---   This episode is sponsored by  · Anchor: The easiest way to make a podcast.  https://anchor.fm/app  Support this podcast: https://anchor.fm/stadiacast/support',\n",
       "        0.4524421593830334],\n",
       "       ['Distance love is sometimes hard so you decide to call your girlfriend but after some minutes the conversation on the phone turned intense and hot. Welcome to Virgin Nights by Lady Exotic',\n",
       "        'Distance love is sometimes hard so you decide to call your girlfriend but after some minutes the conversation on the phone turned intense and hot Welcome to Virgin Nights by Lady Exotic\\xa0 ',\n",
       "        0.9919571045576407]], dtype=object)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look to the removed episode descriptions\n",
    "look = pd.concat([brass_set_lenght,brass_set_show_overlap]).drop_duplicates(keep=False)[['show_description', 'episode_description']]\n",
    "look['overlapping'] = look.apply(lambda row: description_similarity(row['show_description'], row['episode_description']), axis=1)\n",
    "look.values[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Further cleaning of the data\n",
    "The podcast episodes should be restricted to the English language, but they cover a range of geographical regions and we found a number of non-English podcasts in the dataset. So we remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens that correspond to URLs, email addresses, @mentions, #hashtags, and those excessively long tokens (>25 characters) are directly removed from the summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We futher cleaned the descriptions computing a *salience score* for each sentence of the description by summing over word IDF scores.\n",
    "Then we remove sentences if their salience scores are lower than a threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Golden set integration\n",
    "In addition we have a **golden set** of 150 episodes composed by 6 set of summaries for each episode (900 document-summary-grade triplets) that were graded on the Bad/Fair/Good/Excellent scale (0-3). We integrated the best summary of each episodes in the *brass set*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Semantic segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6  Chunck classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"Hi, I'm Ben Folds. \",\n",
       "  \"Welcome to a new podcast series that I'm hosting called Arts vote 2020 where I talked to presidential candidates about arts and politics this year. \",\n",
       "  'I joined the board of the Americans for the Arts action fund to help artists enthusiasts like you and made a better understand where these candidates stand on the issues. ',\n",
       "  'We care about former US senator and Anchorage mayor Mark begich is joining me to moderate this series as we talked to candidates about Impact of the arts and arts education our communities schools and our lives.  ',\n",
       "  'I wanted to do this podcast series because I realized that if the Arts Community wants to move the needle on the future support of the Arts. ',\n",
       "  'Then we need to act now to engage candidates on these issues. ',\n",
       "  'Our first interviews will be with 20/20 presidential candidates. ',\n",
       "  'Mayor Pete Buddha judge and congressman John Delaney. ',\n",
       "  \"We're going to try our best to interview everyone running for president to learn where they stand on the Arts. \"],\n",
       " ['Thanks for listening. ',\n",
       "  'Be sure to subscribe today to the Arts vote 2020 podcast series with Ben Folds on anchor  Or any of your favorite podcast apps, please go to Arts action fund dot org slash podcast for more information.  ']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def look_ahead_chuck(sentences, lower_chunk_size):\n",
    "    \"\"\"\n",
    "    Look-ahead function to determine the next chunk\n",
    "    \"\"\"\n",
    "    if sum([len(s) for s in sentences]) < lower_chunk_size:\n",
    "        # if the remaining sentences size is smaller than the lower bound, we return the remaining sentences\n",
    "        return sentences\n",
    "    else:\n",
    "        # next chunk size should be at least the lower bound \n",
    "        for i in range(len(sentences)):\n",
    "            if sum([len(s) for s in sentences[:i+1]]) >= lower_chunk_size:\n",
    "                return sentences[:i+1]\n",
    "\n",
    "\n",
    "def semantic_segmentation(text, model, lower_chunk_size=300, upper_chunk_size=2000):\n",
    "    \"\"\"\n",
    "    Algorithm proposed by Moro et. al. (2022) to semantically segment long inputs into GPU memory-adaptable chunks.\n",
    "    https://www.aaai.org/AAAI22Papers/AAAI-3882.MoroG.pdf\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    text: str\n",
    "        The text to be segmented\n",
    "    model: SentenceTransformer\n",
    "        The model to be used for the sentence embeddings\n",
    "    lower_chunk_size: int\n",
    "        The lower bound of the chunk size\n",
    "    upper_chunk_size: int\n",
    "        The upper bound of the chunk size\n",
    "    Return\n",
    "    -------\n",
    "    List of chunks of text\n",
    "    \"\"\"\n",
    "\n",
    "    # segment the text into sentences\n",
    "    seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "    sentences = seg.segment(text)\n",
    "\n",
    "    chuncks = []\n",
    "    current_chunk = [sentences[0]]\n",
    "\n",
    "    # Iterate over the sentences in the text\n",
    "    for i, sentence in enumerate(sentences[1:]):\n",
    "        if sentence == sentences[-1]:\n",
    "            # If the sentence is the last one, we add it to the last chunk\n",
    "            current_chunk.append(sentence)\n",
    "            chuncks.append(current_chunk)\n",
    "        elif sum([len(s) for s in current_chunk]) + len(sentence) < lower_chunk_size:\n",
    "            # standardize each chunk to a minimum size to best leverage the capability of Transformers\n",
    "            current_chunk.append(sentence)\n",
    "        elif sum([len(s) for s in current_chunk]) + len(sentence) > upper_chunk_size:\n",
    "            # if the chunk is too big, we add it to the list of chunks and start a new one\n",
    "            chuncks.append(current_chunk)\n",
    "            current_chunk = [sentence]\n",
    "        else:\n",
    "            idx = i+1\n",
    "            next_chuck = look_ahead_chuck(sentences[idx+1:], lower_chunk_size)\n",
    "            \n",
    "            # get the embedding of the previous chunk and the next chunk\n",
    "            current_embedding = model.encode(current_chunk)\n",
    "            next_embedding = model.encode(next_chuck)\n",
    "            sentence_embedding = model.encode([sentence])\n",
    "\n",
    "            # get the cosine similarity between the embedding of the embeddings\n",
    "            score_current_chunk = util.cos_sim(sentence_embedding, current_embedding).numpy().mean()\n",
    "            score_next_chunk = util.cos_sim(sentence_embedding, next_embedding).numpy().mean()\n",
    "\n",
    "            # if the score_current_chunk is higher than the score_next_chunk, we add the sentence to the current chunk\n",
    "            if score_current_chunk > score_next_chunk:\n",
    "                current_chunk.append(sentence)\n",
    "            else:\n",
    "                if sum([len(s) for s in current_chunk]) >= lower_chunk_size:\n",
    "                    chuncks.append(current_chunk)\n",
    "                    current_chunk = [sentence]\n",
    "                else:\n",
    "                    current_chunk.append(sentence)\n",
    "    return chuncks\n",
    "\n",
    "# Initialize the sentence transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "semantic_segmentation(get_transcription(metadata_train.iloc[105325]), model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 256\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "\n",
    "model_checkpoint = \"facebook/bart-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(dataset, text_column, summary_column, max_input_length, max_target_length, padding, prefix=\"summarize: \"):\n",
    "    inputs = dataset[text_column]\n",
    "    targets = dataset[summary_column]\n",
    "    inputs = [prefix + inp for inp in inputs]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = \"max_length\"\n",
    "train_dataset = train_dataset.map(\n",
    "                lambda x: preprocess_function(x, \"transcript\", \"best_summary\", max_input_length, max_target_length, padding, prefix=\"summarize: \"),\n",
    "                batched=True,\n",
    "                remove_columns=train_dataset.column_names,\n",
    "                desc=\"Running tokenizer on train dataset\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def sample_generator(dataset, model, tokenizer, shuffle, pad_to_multiple_of=None):\n",
    "    if shuffle:\n",
    "        sample_ordering = np.random.permutation(len(dataset))\n",
    "    else:\n",
    "        sample_ordering = np.arange(len(dataset))\n",
    "    for sample_idx in sample_ordering:\n",
    "        example = dataset[int(sample_idx)]\n",
    "        # Handle dicts with proper padding and conversion to tensor.\n",
    "        example = tokenizer.pad(example, return_tensors=\"np\", pad_to_multiple_of=pad_to_multiple_of)\n",
    "        example = {key: tf.convert_to_tensor(arr, dtype_hint=tf.int32) for key, arr in example.items()}\n",
    "        if model is not None and hasattr(model, \"prepare_decoder_input_ids_from_labels\"):\n",
    "            decoder_input_ids = model.prepare_decoder_input_ids_from_labels(\n",
    "                labels=tf.expand_dims(example[\"labels\"], 0)\n",
    "            )\n",
    "            example[\"decoder_input_ids\"] = tf.squeeze(decoder_input_ids, 0)\n",
    "        yield example, example[\"labels\"]  # TF needs some kind of labels, even if we don't use them\n",
    "    return\n",
    "\n",
    "# region Helper functions\n",
    "def dataset_to_tf(dataset, model, tokenizer, total_batch_size, num_epochs, shuffle):\n",
    "    if dataset is None:\n",
    "        return None\n",
    "    train_generator = partial(sample_generator, dataset, model, tokenizer, shuffle=shuffle)\n",
    "    train_signature = {\n",
    "        feature: tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "        for feature in dataset.features\n",
    "        if feature != \"special_tokens_mask\"\n",
    "    }\n",
    "    if (\n",
    "        model is not None\n",
    "        and \"decoder_input_ids\" not in train_signature\n",
    "        and hasattr(model, \"prepare_decoder_input_ids_from_labels\")\n",
    "    ):\n",
    "        train_signature[\"decoder_input_ids\"] = train_signature[\"labels\"]\n",
    "    # This may need to be changed depending on your particular model or tokenizer!\n",
    "    padding_values = {\n",
    "        key: tf.convert_to_tensor(tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0, dtype=tf.int32)\n",
    "        for key in train_signature.keys()\n",
    "    }\n",
    "    padding_values[\"labels\"] = tf.convert_to_tensor(-100, dtype=tf.int32)\n",
    "    train_signature[\"labels\"] = train_signature[\"input_ids\"]\n",
    "    train_signature = (train_signature, train_signature[\"labels\"])\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "    tf_dataset = (\n",
    "        tf.data.Dataset.from_generator(train_generator, output_signature=train_signature)\n",
    "        .with_options(options)\n",
    "        .padded_batch(\n",
    "            batch_size=total_batch_size,\n",
    "            drop_remainder=True,\n",
    "            padding_values=(padding_values, np.array(-100, dtype=np.int32)),\n",
    "        )\n",
    "        .repeat(int(num_epochs))\n",
    "    )\n",
    "    return tf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_batch_size = 2\n",
    "num_train_epochs = 3\n",
    "learning_rate = 5e-5\n",
    "tf_train_dataset = dataset_to_tf(\n",
    "            train_dataset,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            total_batch_size=total_train_batch_size,\n",
    "            num_epochs=num_train_epochs,\n",
    "            shuffle=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "# region Optimizer, loss and LR scheduling\n",
    "# Scheduler and math around the number of training steps.\n",
    "num_update_steps_per_epoch = len(train_dataset) // total_train_batch_size\n",
    "num_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=learning_rate, num_train_steps=num_train_steps, num_warmup_steps=0\n",
    ")\n",
    "\n",
    "def masked_sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    # We clip the negative labels to 0 to avoid NaNs appearing in the output and\n",
    "    # fouling up everything that comes afterwards. The loss values corresponding to clipped values\n",
    "    # will be masked later anyway, but even masked NaNs seem to cause overflows for some reason.\n",
    "    # 1e6 is chosen as a reasonable upper bound for the number of token indices - in the unlikely\n",
    "    # event that you have more than 1 million tokens in your vocabulary, consider increasing this value.\n",
    "    # More pragmatically, consider redesigning your tokenizer.\n",
    "    losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        tf.clip_by_value(y_true, 0, int(1e6)), y_pred, from_logits=True\n",
    "    )\n",
    "    # Compute the per-sample loss only over the unmasked tokens\n",
    "    losses = tf.ragged.boolean_mask(losses, y_true != -100)\n",
    "    losses = tf.reduce_mean(losses, axis=-1)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "# region Metric\n",
    "metric = load_metric(\"rouge\")\n",
    "# endregion\n",
    "\n",
    "# region Training\n",
    "model.compile(loss={\"logits\": masked_sparse_categorical_crossentropy}, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "                tf_train_dataset,\n",
    "                epochs=int(num_train_epochs),\n",
    "                steps_per_epoch=num_update_steps_per_epoch,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transcript_exaple = train_data.iloc[45].transcript\n",
    "transcript_exaple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best summarization\n",
    "train_data.iloc[45].best_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = np.reshape(tokenizer(transcript_exaple, max_length=max_input_length, padding=padding, truncation=True).input_ids, (1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output =model.generate(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e800dd11dddfb1e3886769e91ed8bbe987a221798b85010fca298ae8afbc389e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
