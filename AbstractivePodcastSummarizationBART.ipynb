{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Abstractive Summarization of Long Podcast Transcripts with BART using Semantic Self-segmentation\n",
    "Podcasts are a rapidly growing medium for news, commentary, entertainment, and learning.  Some podcast shows release new episodes on a regular schedule (daily, weekly, etc); others irregularly.  Some podcast shows feature short episodes of 5 minutes or less touching on one or two topics; others may release 3+ hour long episodes touching on a wide range of topics.  Some are structured as news delivery, some as conversations, some as storytelling.\n",
    "\n",
    "Given a podcast episode, its audio, and transcription, return a short text snippet capturing the most important information in the content. Returned summaries should be grammatical, standalone statement of significantly shorter length than the input episode description.\n",
    "\n",
    "The user task is to provide a short text summary that the user might read when deciding whether to listen to a podcast. Thus the summary should accurately convey the content of the podcast, and be short enough to quickly read on a smartphone screen. It should also be human-readable.\n",
    "\n",
    "For further information about the challenge, take a look to Podcasts Track Guidelines:\n",
    "- [TREC 2020 Podcasts Track](https://trecpodcasts.github.io/participant-instructions-2020.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\boezi\\VisualStudioProjects\\PodcastSummarization\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import pysbd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import words\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from datasets import Dataset\n",
    "from datasets import load_metric\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dataset reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:  Index(['show_uri', 'show_name', 'show_description', 'publisher', 'language',\n",
      "       'rss_link', 'episode_uri', 'episode_name', 'episode_description',\n",
      "       'duration', 'show_filename_prefix', 'episode_filename_prefix'],\n",
      "      dtype='object')\n",
      "Shape:  (105360, 12)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.join(os.path.abspath(\"D:/\"), 'podcasts-no-audio-13GB')\n",
    "\n",
    "metadata_path_train = os.path.join(dataset_path, 'metadata.tsv')\n",
    "metadata_train = pd.read_csv(metadata_path_train, sep='\\t')\n",
    "print(\"Columns: \", metadata_train.columns)\n",
    "print(\"Shape: \", metadata_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(episode):\n",
    "    \"\"\"\n",
    "    Get the path of the episode json file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    episode : pandas.Series\n",
    "        A row from the metadata file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    path : str\n",
    "        The absolute path of the episode json file\n",
    "    \"\"\"\n",
    "    # extract the 2 reference number/letter to access the episode transcript\n",
    "    show_filename = episode['show_filename_prefix']\n",
    "    episode_filename = episode['episode_filename_prefix'] + \".json\"\n",
    "    dir_1, dir_2 = re.match(r'show_(\\d)(\\w).*', show_filename).groups()\n",
    "\n",
    "    # check if the transcript file in all the derived subfolders exist\n",
    "    transcipt_path = os.path.join(dataset_path, \"spotify-podcasts-2020\",\n",
    "                                \"podcasts-transcripts\", dir_1, dir_2,\n",
    "                                show_filename, episode_filename)\n",
    "    return transcipt_path\n",
    "\n",
    "def get_transcription(episode):\n",
    "    \"\"\"\n",
    "    Extract the transcript from the episode json file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    episode : pandas.Series\n",
    "        A row from the metadata file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    transcript : str\n",
    "        The transcript of the episode\n",
    "    \"\"\"\n",
    "    with open(get_path(episode), 'r') as f:\n",
    "        episode_json = json.load(f)\n",
    "        # seems that the last result in each trastcript is a repetition of the first one, so we ignore it\n",
    "        transcripts = [\n",
    "            result[\"alternatives\"][0]['transcript'] if 'transcript' in result[\"alternatives\"][0] else \"\"\n",
    "            for result in episode_json[\"results\"][:-1]\n",
    "        ]\n",
    "        return \" \".join(transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics about episode duration:\n",
      "count    105360.000000\n",
      "mean         33.845715\n",
      "std          22.735674\n",
      "min           0.175317\n",
      "25%          13.552638\n",
      "50%          31.643375\n",
      "75%          50.446825\n",
      "max         304.953900\n",
      "Name: duration, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Statistics about episode duration:\\n\"\n",
    "      f\"{metadata_train['duration'].describe()}\")\n",
    "metadata_train['duration'].hist(bins=1000, figsize=(10,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics about number of episodes per show:\n",
      "count    18376.000000\n",
      "mean         5.733566\n",
      "std         19.310585\n",
      "min          1.000000\n",
      "25%          1.000000\n",
      "50%          2.000000\n",
      "75%          4.000000\n",
      "max       1072.000000\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_episodes = metadata_train.groupby(['show_filename_prefix']).apply(lambda x: list(zip(x['episode_filename_prefix'], x['episode_description']))).to_dict()\n",
    "show_n_episodes = {k: len(v) for k, v in show_episodes.items()}\n",
    "print(\"Statistics about number of episodes per show:\\n\"\n",
    "      f\"{pd.Series(show_n_episodes.values()).describe()}\")\n",
    "pd.Series(show_n_episodes.values()).hist(bins=1000, figsize=(10,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Dataset cleaning\n",
    "We filtered the descriptions to establish a subset that is more appropriate as a ground truth set compared to full set of descriptions.\n",
    "\n",
    "\n",
    "First of all, some of the episodes contain a `NaN` value in the `episode_description` and `show_description` columns. Let's remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping NaN values: \n",
      " show_uri                   False\n",
      "show_name                  False\n",
      "show_description            True\n",
      "publisher                  False\n",
      "language                   False\n",
      "rss_link                   False\n",
      "episode_uri                False\n",
      "episode_name               False\n",
      "episode_description         True\n",
      "duration                   False\n",
      "show_filename_prefix       False\n",
      "episode_filename_prefix    False\n",
      "dtype: bool\n",
      "\n",
      "After dropping NaN values:\n",
      " show_uri                   False\n",
      "show_name                  False\n",
      "show_description           False\n",
      "publisher                  False\n",
      "language                   False\n",
      "rss_link                   False\n",
      "episode_uri                False\n",
      "episode_name               False\n",
      "episode_description        False\n",
      "duration                   False\n",
      "show_filename_prefix       False\n",
      "episode_filename_prefix    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(\"Before dropping NaN values: \\n\", metadata_train.isna().any())\n",
    "metadata_train.dropna(subset=['episode_description', 'show_description'], inplace=True)\n",
    "print(\"\\nAfter dropping NaN values:\\n\", metadata_train.isna().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We strive to enhance the quality of creator descriptions using heuristics. In order to do that, the following cleaning steps are preformed:\n",
    "- remove sentences that contain URLs, email addresses, @mentions, #hashtags in the episode descriptions\n",
    "- removing the content after `\"---\"` that usually is a sponsorship or a boilerplate (e.g., ‚Äú--- This episode is sponsored by ...‚Äù ‚Äú--- Send in a voice message‚Äù)\n",
    "- identify sentences that contain improper content and remove them from the descriptions. In order to do that, we compute a *salience score* for each sentence of the description by summing over word IDF scores. Then we remove sentences if their salience scores are lower than a threshold. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Removing boilerplate from the episode descriptions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105153/105153 [00:00<00:00, 406005.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing links and sponsors from the episode descriptions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105153/105153 [00:00<00:00, 118689.66it/s]\n"
     ]
    }
   ],
   "source": [
    "links_or_sponsors_re = re.compile(\n",
    "    #r\"(http|https|@|[pP]atreon|[eE]mail|[dD]onate|[iI]nstagram|[fF]acebook|[tT]witter|[dD]iscord|[fF]ollow|www|\\.com|#|\\*|[sS]potify)\"\n",
    "    r\"(\\B@\\S+|\\B#\\S+|http\\S+)\"\n",
    ")\n",
    "boilerplate_re = re.compile(r\"---.*\")\n",
    "\n",
    "def remove_link_or_sponsors(description):\n",
    "    \"\"\"\n",
    "    Remove links and sponsors from the episode description\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    description : str\n",
    "        The episode description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A description without links and sponsors (str)\n",
    "    \"\"\"\n",
    "    return links_or_sponsors_re.sub(\"\", description)\n",
    "\n",
    "    \"\"\"# segment the text into sentences\n",
    "    seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "    sentences = seg.segment(description)\n",
    "    sentences = [sentence for sentence in sentences if not links_or_sponsors_re.search(sentence)] \n",
    "    return \" \".join(sentences)\"\"\"\n",
    "\n",
    "def remove_boilerplate(description):\n",
    "    \"\"\"\n",
    "    Remove boilerplate from the episode description\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    description : str\n",
    "        The episode description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A description without boilerplate (str)\n",
    "    \"\"\"\n",
    "    return boilerplate_re.sub(\"\", description)\n",
    "\n",
    "print(\"\\nRemoving boilerplate from the episode descriptions:\")\n",
    "metadata_train['episode_description'] = metadata_train['episode_description'].progress_map(remove_boilerplate)\n",
    "\n",
    "print(\"Removing links and sponsors from the episode descriptions:\")\n",
    "metadata_train['episode_description'] = metadata_train['episode_description'].progress_map(remove_link_or_sponsors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing word frequencies: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105153/105153 [05:59<00:00, 292.90it/s]\n"
     ]
    }
   ],
   "source": [
    "def compute_word_frequencies(descriptions):\n",
    "    \"\"\"\n",
    "    Compute the word frequencies in the whole dataset descriptions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    descriptions : list of str\n",
    "        The descriptions of the episodes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary of word frequencies\n",
    "    \"\"\"\n",
    "    seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "\n",
    "    flattened_descriptions = [\n",
    "        word for line in\n",
    "        [seg.segment(description) for description in tqdm(descriptions, desc=\"Computing word frequencies\")]\n",
    "        for sentence in line\n",
    "        for word in word_tokenize(sentence)\n",
    "    ]\n",
    "    counts = pd.Series(Counter(flattened_descriptions))  # Get counts and transform to Series\n",
    "    return counts\n",
    "\n",
    "word_frequencies = compute_word_frequencies(metadata_train['episode_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the first ever episode of Kream in your Koffee, Katie talks about tips for Christmas shopping. \n",
      "4.524041873747507\n",
      "We also get a little insight into who and what we‚Äôll be hearing about in next weeks episode! \n",
      "3.042732630719105\n",
      "See something, say something. \n",
      "3.9697601184166835\n",
      "It‚Äôs a mantra many live by. \n",
      "4.0260372381406695\n",
      "If you see something strange, call it in or make someone aware, even if it seems innocuous. \n",
      "3.844252208868794\n",
      "Jennifer San Marco had strange behaviors. \n",
      "5.602293927215452\n",
      "It was clear to many that the woman suffered from mental illness that was being untreated. \n",
      "3.8767086363565406\n",
      "But, many wrote it off. \n",
      "3.8251451846619835\n",
      "And, on January 30th 2006 her strange behaviors bubbled over and the Goleta Postal Facility shootings began. \n",
      "5.488942697981391\n",
      "Jennifer San Marco Kills (2006) Become a supporter of this podcast on Patreon:  Follow Morning Cup of Murder on Twitter:   Follow MCOM on Instagram:  Have a Murder or strange true crime story you want to share, email the show here: morningcupofmurder@gmail.com Morning Cup of Murder is researched, written and performed by Korina Biemesderfer. \n",
      "4.418760398211883\n",
      "Follow Korina on Instagram:   \n",
      "3.2833974641286514\n",
      "Today‚Äôs episode is a sit down Michael and Omar had with former USMNT and former USWNT goalkeeper coach, Phil Wheddon. \n",
      "5.182135011419389\n",
      "Phil discusses how he started the International Goalkeeper Coaches Conference (IGCC), what the US needs to do in order to create a successful goalkeeping curriculum across the United States and lastly, he gives his best advice to young goalkeepers and goalkeeper coaches.¬† \n",
      "4.965303300403703\n",
      "Make sure to Subscribe to Inside the 18 on both Apple Podcasts and Spotify and don‚Äôt forget to leave us a rating and a comment to let us know how we‚Äôre doing!¬† \n",
      "3.8938265335540403\n",
      "Contact Information:     \n",
      "5.61743596953076\n",
      "Join us as we take a look at all current Chiefs news, including the draft, free agency, in-house contract talks, and much more!  \n",
      "4.148895480898864\n",
      "The modern morality tail of how to stay good for Christmas. \n",
      "5.436337142707662\n",
      "Ashley Beall of Make It Modern Podcast ( joins me to discuss the redemption arcs of one of KidLit's biggest fuckbois, Edmund Pevensie. ¬†\n",
      "4.968420209002781\n",
      "iTunes Spotify YouTube Stitcher Google Play Music Anchor Do you want to work with us? \n",
      "3.726355461921847\n",
      "Email us!  \n",
      "3.720892932276973\n",
      ". \n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\peppe\\UNIBO\\Natural Language Processing\\lab\\PodcastSummarization\\AbstractivePodcastSummarizationBART.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/peppe/UNIBO/Natural%20Language%20Processing/lab/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000074?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sentences:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/peppe/UNIBO/Natural%20Language%20Processing/lab/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000074?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(sentence)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/peppe/UNIBO/Natural%20Language%20Processing/lab/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000074?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(sentence_salience_score(sentence, metadata_train\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], word_frequencies))\n",
      "\u001b[1;32mc:\\Users\\peppe\\UNIBO\\Natural Language Processing\\lab\\PodcastSummarization\\AbstractivePodcastSummarizationBART.ipynb Cell 16'\u001b[0m in \u001b[0;36msentence_salience_score\u001b[1;34m(sentence, num_descriptions, word_frequencies)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/peppe/UNIBO/Natural%20Language%20Processing/lab/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000059?line=27'>28</a>\u001b[0m         \u001b[39mif\u001b[39;00m idf_score \u001b[39m>\u001b[39m \u001b[39m1.5\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/peppe/UNIBO/Natural%20Language%20Processing/lab/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000059?line=28'>29</a>\u001b[0m             idf_scores\u001b[39m.\u001b[39mappend(idf_score)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/peppe/UNIBO/Natural%20Language%20Processing/lab/PodcastSummarization/AbstractivePodcastSummarizationBART.ipynb#ch0000059?line=29'>30</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msum\u001b[39;49m(idf_scores)\u001b[39m/\u001b[39;49m\u001b[39mlen\u001b[39;49m(idf_scores)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "for description in metadata_train['episode_description'][:10]:\n",
    "    seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "    sentences = seg.segment(description)\n",
    "    for sentence in sentences:\n",
    "        print(sentence)\n",
    "        print(sentence_salience_score(sentence, metadata_train.shape[0], word_frequencies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_salience_score(sentence, num_descriptions, word_frequencies):\n",
    "    \"\"\"\n",
    "    Compute the salience score of a sentence by summing over word IDF scores.\n",
    "    Only words occurring 5 times or more in the corpus and with IDF scores greater than 1.5 \n",
    "    are considered when computing sentence salience scores.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence : str\n",
    "        The sentence to compute the salience score for\n",
    "    num_descriptions : int\n",
    "        The number of descriptions in the dataset\n",
    "    word_frequencies : pandas.Series\n",
    "        The word frequencies in the whole dataset descriptions\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    The salience score of the sentence (float)\n",
    "    \"\"\"\n",
    "    idf_scores = []\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "\n",
    "    # compute IDF scores for each word in the sentence and sum them up \n",
    "    # (only words occurring 5 times or more in the corpus and with IDF scores greater than 1.5 are considered)\n",
    "    for word in tokenized_sentence:\n",
    "        if word_frequencies[word] >= 5:\n",
    "            idf_score = np.log(num_descriptions/word_frequencies[word])\n",
    "            if idf_score > 1.5:\n",
    "                idf_scores.append(idf_score)\n",
    "    return sum(idf_scores)/len(idf_scores)\n",
    "\n",
    "def remove_unuseful_sentences(description, num_descriptions, word_frequencies, threshold=10):\n",
    "    \"\"\"\n",
    "    Remove sentences that are not useful for the transcriptions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    description : str\n",
    "        The episode description\n",
    "    num_descriptions : int\n",
    "        The number of descriptions in the dataset\n",
    "    word_frequencies : pandas.Series\n",
    "        The word frequencies in the whole dataset descriptions\n",
    "    threshold : double\n",
    "        The threshold for the salience score of a sentence to be considered useful\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A description without unuseful sentences (str)\n",
    "    \"\"\"\n",
    "    # segment the text into sentences\n",
    "    seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "    sentences = seg.segment(description)\n",
    "    # remove sentences that are not useful for the transcriptions\n",
    "    sentences = [sentence for sentence in sentences if sentence_salience_score(sentence, num_descriptions, word_frequencies) < threshold]\n",
    "    return \" \".join(sentences)\n",
    "\n",
    "metadata_train['episode_description'] = metadata_train['episode_description'].progress_map(lambda x: remove_unuseful_sentences(x, metadata_train.shape[0], word_frequencies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to select a subset of the corpus that is suitable for training supervised models, we filtered the descriptions using three heuristics shown in the table below. These filters overlap to some extent, and remove about a third of the entire set. The remaining episodes we call the **Brass Set**.\n",
    "\n",
    "| Criterion                        | Threshold                                                    |\n",
    "| -------------------------------- | ------------------------------------------------------------ |\n",
    "| Length                           | descriptions that are very long (> 750 characters) or short (< 20 characters). |\n",
    "| Similarity to show description   | descriptions with high lexical overlap (over 40%) with their show description. |\n",
    "| Similarity to other descriptions | descriptions with high lexical overlap (over 50%) with other episode descriptions in the same show. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_lenght_brass(episode, upper_bound=750, lower_bound=20):\n",
    "    \"\"\"\n",
    "    Check if the episode descriptions is not too long (> 750 characters) or not too short (< 20 characters)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    episode : pandas.Series\n",
    "        A row from the metadata file\n",
    "    upper_bound : int\n",
    "        The upper bound of the episode description length\n",
    "    lower_bound : int\n",
    "        The lower bound of the episode description length\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Boolean indicating if the episode description is long enough\n",
    "    \"\"\"\n",
    "    return len(episode['episode_description']) <= upper_bound and len(episode['episode_description']) >= lower_bound\n",
    "    \n",
    "def description_similarity(a, b):\n",
    "    \"\"\"\n",
    "    Measure the overlapping between two descriptions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : str\n",
    "        The first description\n",
    "    b : str\n",
    "        The second description\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Value indicating the overlapping between the two descriptions\n",
    "    \"\"\"\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def check_show_description_overlap_brass(episode, thresh=0.4):\n",
    "    \"\"\"\n",
    "    Check if the episode descriptions overlapping with the show description is not too high (< 0.4)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    episode : pandas.Series\n",
    "        A row from the metadata file\n",
    "    thresh : float\n",
    "        The threshold of the overlap between the episode description and the show description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Boolean indicating if the episode description is different enough from the show description\n",
    "    \"\"\"\n",
    "    return description_similarity(episode['show_description'], episode['episode_description']) < thresh\n",
    "    \n",
    "def check_other_description_overlap_brass(episode, show_episodes, thresh=0.5):\n",
    "    \"\"\"\n",
    "    Check if the episode descriptions overlapping with the other description in the same show is not too high (< 0.5)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    episode : pandas.Series\n",
    "        A row from the metadata file\n",
    "    show_episodes : dict\n",
    "        A dictionary of the episodes of the same show\n",
    "    thresh : float\n",
    "        The threshold of the overlap between the episode description and the other description\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Boolean indicating if the episode description is different enough from the other description\n",
    "    \"\"\"\n",
    "    for other_prefix, other_description in show_episodes[episode['show_filename_prefix']]:\n",
    "        if other_prefix != episode['episode_filename_prefix'] and description_similarity(episode['episode_description'], other_description) > thresh and len(episode['episode_description']) < other_description:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "brass_set_lenght = metadata_train[metadata_train.progress_apply(check_lenght_brass, axis=1)]\n",
    "print(f\"Removed {len(metadata_train) - len(brass_set_lenght)} episodes ({(100-(len(brass_set_lenght)/len(metadata_train)*100)):.2f}%) because of too long or too short descriptions\")\n",
    "\n",
    "brass_set_show_overlap = brass_set_lenght[brass_set_lenght.progress_apply(check_show_description_overlap_brass, axis=1)]\n",
    "print(f\"Removed {len(brass_set_lenght) - len(brass_set_show_overlap)} episodes ({(100-(len(brass_set_show_overlap)/len(brass_set_lenght)*100)):.2f}%) because of too high overlap with the show description\")\n",
    "\n",
    "show_episodes = brass_set_show_overlap.groupby(['show_filename_prefix']).apply(lambda x: list(zip(x['episode_filename_prefix'], x['episode_description']))).to_dict()\n",
    "brass_set = brass_set_show_overlap[brass_set_show_overlap.progress_apply(lambda x: check_other_description_overlap_brass(x, show_episodes), axis=1)]\n",
    "print(f\"Removed {len(brass_set_show_overlap) - len(brass_set)} episodes ({(100-(len(brass_set)/len(brass_set_show_overlap)*100)):.2f}%) because of too high overlap with other descriptions in the same show\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store brass set\n",
    "brass_set.to_csv(os.path.join(os.path.dirname(metadata_path_train), \"brass_set.tsv\"), index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode description: \n",
      "\tWe will find out shortly  ---   This episode is sponsored by  ¬∑ Anchor: The easiest way to make a podcast.  https://anchor.fm/app  Support this podcast: https://anchor.fm/a100savage/support\n",
      "Show description: \n",
      "\t‚úÖVerified Podcast Favorite üëÜ Sponsored By Ducqets Clothing Brandüëå Support this podcast: https://anchor.fm/a100savage/support\n",
      "Overlapping score: \n",
      "\t0.5686900958466453\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tFun Fact: Julias Caesar once rode a boat. He was quoted saying \" Et tu, boate\" which roughly translates to \"Nice boat joke Brandon!\" ¬† Before the Common Era is an educational and conversational podcast between 3 friends and their sound guy. Join them as they go on an adventure through a hat and all of the mysteries it may contain. A special thank you to Michael Morrise for our music. Link dump: Mantis Peacock Shrimp img.izismile.com/img/img9/2016100‚Ä¶e_you_640_04.jpg \n",
      "Show description: \n",
      "\tBefore the Common Era is an educational and conversational podcast between 3 friends and their sound guy. Join them as they go on an adventure through a hat and all of the mysteries it may contain. \n",
      "Overlapping score: \n",
      "\t0.5919282511210763\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tThis week we speak about a whole heap of subjects, including favourite movies, dating up and a game of would you rather. #TheReceiptsPodcast is a fun, honest podcast fronted by three girls who are willing to talk about anything and everything.From relationships to situationships to everyday life experiences, you can expect unadulterated girl talk with no filter. Hosts: Audrey Twitter: @Ghanasfinestx Instagram: @Ghanas_Finest Tolani Twitter: @Tolly_T Instagram: @Tolly_T Milena Sanchez Twitter: @Milenasanchezx Instagram: @milenasanchezx Get in touch and share your receipts with us, keepthereceipts@gmail.com\n",
      "Show description: \n",
      "\t#TheReceiptsPodcast is a fun, honest podcast fronted by three girls who are willing to talk about anything and everything. From relationships to situationships to everyday life experiences, you can expect unadulterated girl talk with no filter. #TheReceiptsPodcast is a Spotify Original - new episodes available every Wednesday, only on Spotify.  Hosts:  Ghana's Finest Twitter: @Ghanasfinestx  Instagram: @Ghanas_Finest  Tolly Twitter: @Tolly_T Instagram: @Tolly_T   Milena Sanchez Twitter: @Milenasanchezx Instagram: @milenasanchezx  Get in touch and share your receipts with us, keepthereceipts@gmail.com.\n",
      "Overlapping score: \n",
      "\t0.7852459016393443\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tOn this pilot episode, Karinna and Izzy talk about the most disgusting Halloween candies, Karinna spots a Church of Scientology downtown, and Izzy blames Hello Kitty for ruining her life. Plus, Izzy talks about her experience with Sleep Paralysis.¬† --- Welcome to Izkapop, where your hosts Karinna and Izzy talk about anything from what's going on in their lives to entertainment and pop culture. Think of it as a casual conversation that touches on the most random topics - with two weirdos who are passionate about dipping their feet into various subjects. New Episode Every Wednesday at 9AM! \n",
      "Show description: \n",
      "\tWelcome to Izkapop - where your hosts Karinna and Izzy talk about anything from what's going on in their lives to entertainment, conspiracy theories, and pop culture! Think of it as a casual conversation that touches on the most random topics - with two weirdos who are passionate about dipping their feet into various subjects.  New Episode Every Week Either Tuesday or Wednesday!   \n",
      "Overlapping score: \n",
      "\t0.6332992849846782\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tThis week we discuss the Pitts and peaks of touring, friendships, paternity fraud and much much more¬†#TheReceiptsPodcast¬†is a fun, honest podcast fronted by three girls who are willing to talk about anything and everything. From relationships to situationships to everyday life experiences, you can expect unadulterated girl talk with no filter. Hosts: Audrey Twitter: @Ghanasfinestx Instagram: @Ghanas_Finest Tolani Twitter: @Tolly_T Instagram: @Tolly_T Milena Sanchez Twitter: @Milenasanchezx Instagram: @milenasanchezx Get in touch and share your receipts with us,¬†keepthereceipts@gmail.com\n",
      "Show description: \n",
      "\t#TheReceiptsPodcast is a fun, honest podcast fronted by three girls who are willing to talk about anything and everything. From relationships to situationships to everyday life experiences, you can expect unadulterated girl talk with no filter. #TheReceiptsPodcast is a Spotify Original - new episodes available every Wednesday, only on Spotify.  Hosts:  Ghana's Finest Twitter: @Ghanasfinestx  Instagram: @Ghanas_Finest  Tolly Twitter: @Tolly_T Instagram: @Tolly_T   Milena Sanchez Twitter: @Milenasanchezx Instagram: @milenasanchezx  Get in touch and share your receipts with us, keepthereceipts@gmail.com.\n",
      "Overlapping score: \n",
      "\t0.7776852622814321\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tA podcast about Queen Victoria‚Äôs life and what kind of leader she was. \n",
      "Show description: \n",
      "\tThis podcast is about Queen Victoria and her life. What kind of ruler she was, whether or not people liked her, and what she accomplished.\n",
      "Overlapping score: \n",
      "\t0.5550239234449761\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tTrain your inner coach.   ---   Send in a voice message: https://anchor.fm/coachkristi/message Support this podcast: https://anchor.fm/coachkristi/support\n",
      "Show description: \n",
      "\tA discussion about how horses can teach us to be more evolved as a species. Support this podcast: https://anchor.fm/coachkristi/support\n",
      "Overlapping score: \n",
      "\t0.5605536332179931\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tHere's the first episode of the all new podcast series from Kesh! 4 simple, yet effective ideas that help you stick through till the end of any art challenge like Inktober. Enjoy! A podcast on art & creativity where we explore interesting ideas, ¬†gain new insights and grow a little better every day, one step at a ¬†time. Brought to you by Kesh, with friends dropping in every now and then. Join us in our sketching expedition on this audio canvas to make something meaningful! Links: Website YouTube Instagram  \n",
      "Show description: \n",
      "\tA podcast on art & creativity where we explore interesting ideas, gain new insights and grow a little better every day, one step at a time.  Hosted by Kesh, with friends dropping in every now and then. Join us in our sketching expedition on this audio canvas to make something meaningful!\n",
      "Overlapping score: \n",
      "\t0.7025\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tJoin our FREE Study: http://sadhusangaonline.com \n",
      "Show description: \n",
      "\tReadings from FREE online study of Srimad Bhagavatam at SadhusangaOnline.com\n",
      "Overlapping score: \n",
      "\t0.576\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tThis podcast breaks down the hit movie Avengers: Endgame in four short minutes, with four short reviews. \n",
      "Show description: \n",
      "\tThis podcast breaks down the hit movie Avengers: Endgame in four short minutes, with four short reviews\n",
      "Overlapping score: \n",
      "\t0.9903846153846154\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look to the removed episode descriptions due to the overlap with the show description\n",
    "removed_episodes_show_overlap = pd.concat([brass_set_lenght, brass_set_show_overlap]).drop_duplicates(keep=False)[['show_description', 'episode_description']]\n",
    "removed_episodes_show_overlap['overlapping'] = removed_episodes_show_overlap.apply(lambda row: description_similarity(row['show_description'], row['episode_description']), axis=1)\n",
    "\n",
    "num_to_visualize = 10\n",
    "\n",
    "for _ in range(num_to_visualize):\n",
    "    row = removed_episodes_show_overlap.sample()\n",
    "    print(f\"Episode description: \\n\\t{row['episode_description'].values[0]}\")\n",
    "    print(f\"Show description: \\n\\t{row['show_description'].values[0]}\")\n",
    "    print(f\"Overlapping score: \\n\\t{row['overlapping'].values[0]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode description: \n",
      "\tEpisode 1 (6/11/19) ~ Kyle and Davis discuss the MLB, NHL, NBA, and more to kick off At The Buzzer!  ---   Send in a voice message: https://anchor.fm/AtTheBuzzerRadio/message Support this podcast: https://anchor.fm/AtTheBuzzerRadio/support\n",
      "EOther episode description: \n",
      "\tEpisode 6 (8/10/19) ~ Kyle and Davis invite 6 guest on the show for a wild ride of fun and sports talk!!  ---   Send in a voice message: https://anchor.fm/AtTheBuzzerRadio/message Support this podcast: https://anchor.fm/AtTheBuzzerRadio/support\n",
      "Overlapping score: \n",
      "\t0.7163561076604554\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tAdvice on how to attract someone. www.dateme.tips \n",
      "EOther episode description: \n",
      "\tAdvice on how to raise the flames of attraction. www.dateme.tips \n",
      "Overlapping score: \n",
      "\t0.7652173913043478\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tIn this episode you will learn how to play songs using a lyric sheet. www.cynthiaalistudios.com \n",
      "EOther episode description: \n",
      "\tIn this episode, you will learn the basics of rhythm and an introductory strum pattern. www.cynthiaalistudios.com \n",
      "Overlapping score: \n",
      "\t0.6476190476190476\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tPacking our bags for #stagecoach2019 Guess who calls into the studio to talk to the Cali Country team? ¬†The man, Dustin Lynch! ¬† Graham discussing his golf swing and getting feedback from friends. IG: ¬†@calicountrypodcast www.cali-country.com  IG: ¬†@calicountrypodcast www.cali-country.com ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ #countrymusic #musicfestival #festivalseason #countryboys #audiocontent #calicountrypodcast #grahambunn #derekriker #manicflow #flowers #host #cohosts #producers #creators #blessed #podcasts #podcast #podcasting #podcastlife #podcaster #podcastshow #radio #comedy #podcasters #podcastinglife #music #podcastersofinstagram #podcastlove #podcastaddict  ---   Support this podcast: https://anchor.fm/calicountry/support\n",
      "EOther episode description: \n",
      "\tThe Cali-Country Team sits down in studio to recap Stagecoach and share their favorite highlights and sound bites from the weekend. ¬†This is a wild one folks so hold on to your horses and lace your bootstraps on tight. ¬†¬†There's a lot to cover in this one :) IG: ¬†@calicountrypodcast www.cali-country.com ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ #countrymusic #musicfestival #festivalseason #countryboys #audiocontent #calicountrypodcast #grahambunn #derekriker #manicflow #flowers #host #cohosts #producers #creators #blessed #podcasts #podcast #podcasting #podcastlife #podcaster #podcastshow #radio #comedy #podcasters #podcastinglife #music #podcastersofinstagram #podcastlove #podcastaddict   ---   Support this podcast: https://anchor.fm/calicountry/support\n",
      "Overlapping score: \n",
      "\t0.6629834254143646\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tIn Episode 2 of Mary and Andy Aren‚Äôt Good At This: Mary and Andy discuss Etiquette, Things Being on Fire, and the Ins and Outs of Scorpion Farming. Please Rate/Subscribe/Follow us on the podcasting app of your choice. Follow us on Facebook at ‚ÄúMary and Andy Aren‚Äôt Good This‚Äù and share this episode with your friends. Please submit your stories and questions to maryandandycare@gmail.com  ---   This episode is sponsored by  ¬∑ Anchor: The easiest way to make a podcast.  https://anchor.fm/app \n",
      "EOther episode description: \n",
      "\tIn Episode 1 of Mary and Andy Aren‚Äôt Good At This: Mary and Andy discuss Cowardice, Bailin on Grandma, and Bird Suicide? *UPDATE* We are now available on Apple Podcasts. Please Rate/Subscribe/Follow us on the podcasting app of your choice. Follow us on Facebook at ‚ÄúMary and Andy Aren‚Äôt Good This‚Äù and share this episode with your friends. Please submit your stories and questions to maryandandycare@gmail.com.   ---   This episode is sponsored by  ¬∑ Anchor: The easiest way to make a podcast.  https://anchor.fm/app \n",
      "Overlapping score: \n",
      "\t0.8316831683168316\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tstill here! and the doc said i could keep fasting so guess what imma doo!?  ---   This episode is sponsored by  ¬∑ Anchor: The easiest way to make a podcast.  https://anchor.fm/app \n",
      "EOther episode description: \n",
      "\tSo i lost a bunch of weight fasting but i still got a ways to go. wanna come with me?  ---   This episode is sponsored by  ¬∑ Anchor: The easiest way to make a podcast.  https://anchor.fm/app \n",
      "Overlapping score: \n",
      "\t0.7223719676549866\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tWhat word will guide you throughout the year? Listen to #ColtsMath students share their words.  ---   This episode is sponsored by  ¬∑ Anchor: The easiest way to make a podcast.  https://anchor.fm/app  ---   Send in a voice message: https://anchor.fm/buistcoltsmath/message\n",
      "EOther episode description: \n",
      "\tInspired by the work of the Casteel Equity and Inclusion Team, this week's theme is \"I wish my teacher knew...\" starring the voices of Mr. Buist's periods 5&6.  ---   This episode is sponsored by  ¬∑ Anchor: The easiest way to make a podcast.  https://anchor.fm/app  ---   Send in a voice message: https://anchor.fm/buistcoltsmath/message\n",
      "Overlapping score: \n",
      "\t0.6042692939244664\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tNever f*ck with Greg. He always wins. 'Nuff Said.  ---   This episode is sponsored by  ¬∑ Anchor: The easiest way to make a podcast.  https://anchor.fm/app  Support this podcast: https://anchor.fm/rusty-gate/support\n",
      "EOther episode description: \n",
      "\tListen this week as Jaysin and Tyler go on about attractive celebrities, acts of utter disrespect, and much more on the Rusty Gate Show!  ---   This episode is sponsored by  ¬∑ Anchor: The easiest way to make a podcast.  https://anchor.fm/app  Support this podcast: https://anchor.fm/rusty-gate/support\n",
      "Overlapping score: \n",
      "\t0.6563106796116505\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tJoin us as we discuss the hi's and low's of our natural hair journey, confront the black haircare landscape, and debate whether product accessibility equals good product quality! #UKCurlyChronicles is a podcast that aims to have authentic conversations about hair, work-life balance, relationships, society and what it means to be a black female in the UK.¬† Natural Queens: @Justchin_1 @Azsiamldn¬† Sponsors: @Eyokohair @Fashionnrail¬† Share your stories and send in your questions to Instagram: @UK_curlychronicles Email: curlychroniclesuk@gmail.com¬† \n",
      "EOther episode description: \n",
      "\tJoin us in our new episode, where we take on the popular argument wigs or weaves!? Provide you with the weekly hair care tip, as well as a challenge! All whilst getting to know the girls behind the pod! #UKCurlyChronicles is a podcast that aims to have authentic conversations about hair, work-life balance, relationships, society and what it means to be a black female in the UK.¬† Natural Queens: @Justchin_1 @Azsiamldn¬† Sponsors: @Eyokohair @Fashionnrail¬† Share your stories and send in your questions to Instagram: @UK_curlychronicles¬† Email: curlychroniclesuk@gmail.com \n",
      "Overlapping score: \n",
      "\t0.6814946619217082\n",
      "\n",
      "\n",
      "Episode description: \n",
      "\tJess and Kristin dive deep into a whole bunch of urban legends.  ---   This episode is sponsored by  ¬∑ Anchor: The easiest way to make a podcast.  https://anchor.fm/app  Support this podcast: https://anchor.fm/everythingology/support\n",
      "EOther episode description: \n",
      "\tKristin and Jessica talk about possible glitches in reality and how real the 'Matrix' really is.¬†  ---   This episode is sponsored by  ¬∑ Anchor: The easiest way to make a podcast.  https://anchor.fm/app  Support this podcast: https://anchor.fm/everythingology/support\n",
      "Overlapping score: \n",
      "\t0.7\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look to the removed episode descriptions due to the overlap with the other episode descriptions in the same show\n",
    "removed_episodes_other_overlap = pd.concat([brass_set, brass_set_show_overlap]).drop_duplicates(keep=False)[['show_filename_prefix', 'episode_filename_prefix', 'episode_description']]\n",
    "two_episodes_show  = {str(show_filename_prefix): show_episodes[show_filename_prefix] for show_filename_prefix in removed_episodes_other_overlap['show_filename_prefix'] if len(show_episodes[show_filename_prefix]) == 2 }\n",
    "removed_episodes_other_overlap = removed_episodes_other_overlap[removed_episodes_other_overlap['show_filename_prefix'].isin(two_episodes_show.keys())]\n",
    "other_episode_show = {}\n",
    "for i, row in removed_episodes_other_overlap.iterrows():\n",
    "    if row['show_filename_prefix'] in two_episodes_show:\n",
    "        if row['episode_filename_prefix'] in two_episodes_show[row['show_filename_prefix']][0]:\n",
    "            other_episode_show[row['show_filename_prefix']] = two_episodes_show[row['show_filename_prefix']][1][1]\n",
    "        else:\n",
    "            other_episode_show[row['show_filename_prefix']] = two_episodes_show[row['show_filename_prefix']][0][1]\n",
    "removed_episodes_other_overlap['other_episode_description'] = removed_episodes_other_overlap.apply(lambda row: other_episode_show[row['show_filename_prefix']], axis=1)\n",
    "removed_episodes_other_overlap['overlapping'] = removed_episodes_other_overlap.apply(lambda row: description_similarity(row['episode_description'], row['other_episode_description']), axis=1)\n",
    "\n",
    "num_to_visualize = 10\n",
    "\n",
    "for _ in range(num_to_visualize):\n",
    "    row = removed_episodes_other_overlap.sample()\n",
    "    print(f\"Episode description: \\n\\t{row['episode_description'].values[0]}\")\n",
    "    print(f\"EOther episode description: \\n\\t{row['other_episode_description'].values[0]}\")\n",
    "    print(f\"Overlapping score: \\n\\t{row['overlapping'].values[0]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Further cleaning of the data\n",
    "The podcast episodes should be restricted to the English language, but they cover a range of geographical regions and we found a number of non-English podcasts in the dataset. So we remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('corpora/words')\n",
    "except LookupError:\n",
    "    nltk.download('words')\n",
    "wordset = set(words.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Golden set integration\n",
    "In addition we have a **golden set** of 150 episodes composed by 6 set of summaries for each episode (900 document-summary-grade triplets) that were graded on the Bad/Fair/Good/Excellent scale (0-3). We integrated the best summary of each episodes in the *brass set*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Semantic segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6  Chunck classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"Hi, I'm Ben Folds. \",\n",
       "  \"Welcome to a new podcast series that I'm hosting called Arts vote 2020 where I talked to presidential candidates about arts and politics this year. \",\n",
       "  'I joined the board of the Americans for the Arts action fund to help artists enthusiasts like you and made a better understand where these candidates stand on the issues. ',\n",
       "  'We care about former US senator and Anchorage mayor Mark begich is joining me to moderate this series as we talked to candidates about Impact of the arts and arts education our communities schools and our lives.  ',\n",
       "  'I wanted to do this podcast series because I realized that if the Arts Community wants to move the needle on the future support of the Arts. ',\n",
       "  'Then we need to act now to engage candidates on these issues. ',\n",
       "  'Our first interviews will be with 20/20 presidential candidates. ',\n",
       "  'Mayor Pete Buddha judge and congressman John Delaney. ',\n",
       "  \"We're going to try our best to interview everyone running for president to learn where they stand on the Arts. \"],\n",
       " ['Thanks for listening. ',\n",
       "  'Be sure to subscribe today to the Arts vote 2020 podcast series with Ben Folds on anchor  Or any of your favorite podcast apps, please go to Arts action fund dot org slash podcast for more information.  ']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def look_ahead_chuck(sentences, lower_chunk_size):\n",
    "    \"\"\"\n",
    "    Look-ahead function to determine the next chunk\n",
    "    \"\"\"\n",
    "    if sum([len(s) for s in sentences]) < lower_chunk_size:\n",
    "        # if the remaining sentences size is smaller than the lower bound, we return the remaining sentences\n",
    "        return sentences\n",
    "    else:\n",
    "        # next chunk size should be at least the lower bound \n",
    "        for i in range(len(sentences)):\n",
    "            if sum([len(s) for s in sentences[:i+1]]) >= lower_chunk_size:\n",
    "                return sentences[:i+1]\n",
    "\n",
    "\n",
    "def semantic_segmentation(text, model, lower_chunk_size=300, upper_chunk_size=2000):\n",
    "    \"\"\"\n",
    "    Algorithm proposed by Moro et. al. (2022) to semantically segment long inputs into GPU memory-adaptable chunks.\n",
    "    https://www.aaai.org/AAAI22Papers/AAAI-3882.MoroG.pdf\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    text: str\n",
    "        The text to be segmented\n",
    "    model: SentenceTransformer\n",
    "        The model to be used for the sentence embeddings\n",
    "    lower_chunk_size: int\n",
    "        The lower bound of the chunk size\n",
    "    upper_chunk_size: int\n",
    "        The upper bound of the chunk size\n",
    "    Return\n",
    "    -------\n",
    "    List of chunks of text\n",
    "    \"\"\"\n",
    "\n",
    "    # segment the text into sentences\n",
    "    seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "    sentences = seg.segment(text)\n",
    "\n",
    "    chuncks = []\n",
    "    current_chunk = [sentences[0]]\n",
    "\n",
    "    # Iterate over the sentences in the text\n",
    "    for i, sentence in enumerate(sentences[1:]):\n",
    "        if sentence == sentences[-1]:\n",
    "            # If the sentence is the last one, we add it to the last chunk\n",
    "            current_chunk.append(sentence)\n",
    "            chuncks.append(current_chunk)\n",
    "        elif sum([len(s) for s in current_chunk]) + len(sentence) < lower_chunk_size:\n",
    "            # standardize each chunk to a minimum size to best leverage the capability of Transformers\n",
    "            current_chunk.append(sentence)\n",
    "        elif sum([len(s) for s in current_chunk]) + len(sentence) > upper_chunk_size:\n",
    "            # if the chunk is too big, we add it to the list of chunks and start a new one\n",
    "            chuncks.append(current_chunk)\n",
    "            current_chunk = [sentence]\n",
    "        else:\n",
    "            idx = i+1\n",
    "            next_chuck = look_ahead_chuck(sentences[idx+1:], lower_chunk_size)\n",
    "            \n",
    "            # get the embedding of the previous chunk and the next chunk\n",
    "            current_embedding = model.encode(current_chunk)\n",
    "            next_embedding = model.encode(next_chuck)\n",
    "            sentence_embedding = model.encode([sentence])\n",
    "\n",
    "            # get the cosine similarity between the embedding of the embeddings\n",
    "            score_current_chunk = util.cos_sim(sentence_embedding, current_embedding).numpy().mean()\n",
    "            score_next_chunk = util.cos_sim(sentence_embedding, next_embedding).numpy().mean()\n",
    "\n",
    "            # if the score_current_chunk is higher than the score_next_chunk, we add the sentence to the current chunk\n",
    "            if score_current_chunk > score_next_chunk:\n",
    "                current_chunk.append(sentence)\n",
    "            else:\n",
    "                if sum([len(s) for s in current_chunk]) >= lower_chunk_size:\n",
    "                    chuncks.append(current_chunk)\n",
    "                    current_chunk = [sentence]\n",
    "                else:\n",
    "                    current_chunk.append(sentence)\n",
    "    return chuncks\n",
    "\n",
    "# Initialize the sentence transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "semantic_segmentation(get_transcription(metadata_train.iloc[105325]), model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Chunk Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From each chunk an encoding of each sentence is extracted using a pretrained RoBerta Transformerss to obtain a dense encoding. The encoding of the chunk is the mean of the encoding of its sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "    def extract_features(self,text):\n",
    "        \"\"\"\n",
    "        Extract features from text using a mean of the tf-idf\n",
    "        Parameters:\n",
    "            - text: string representing a document\n",
    "        Returns:\n",
    "            - extracted features\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        for sentence in text:\n",
    "          embeddings.append(self.model.encode(sentence))\n",
    "\n",
    "        features = np.mean(embeddings, axis=0)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to keep only useful chunks for each transcript we compare the chunk with the corresponding summary of the transcript it belongs to and, if the score obtained with a certain metric is below a threshold (strictly coupled with the metric), the chunk is not taken into account as a part of the transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isChunkUseful(chunk, summary, metric, threshold):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - chunk: part of the transcript\n",
    "        - summary: summary of a transcript\n",
    "        - metric: function of ariety 2 (chunk, summary) used to evaluate the summary\n",
    "        - threshold: value used to decide whether chunk is a good summary or not\n",
    "    Returns:\n",
    "        - True if the chunk is a good summary, False otherwise\n",
    "    \"\"\"\n",
    "    score = metric(chunk, summary)\n",
    "    if score < threshold:\n",
    "        result = False\n",
    "    else:\n",
    "        result = True\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen metric is BERTscore f1-score because it is a semantic metric, i.e. it takes into account the meaning of words in the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bertscore_f1_score(reference, candidate):\n",
    "    \"\"\"\n",
    "    BERTScore score, see https://github.com/huggingface/datasets/tree/master/metrics/bertscore for API\n",
    "    Parameters:\n",
    "        reference: reference translation\n",
    "        candidate: generated translation\n",
    "    Returns:\n",
    "        BERTScore f1 score\n",
    "    \"\"\"\n",
    "    bertscore = load_metric(\"bertscore\")\n",
    "    result = bertscore.compute(\n",
    "        predictions=[candidate],\n",
    "        references=[reference],\n",
    "        lang=\"en\",\n",
    "        rescale_with_baseline=True\n",
    "    )\n",
    "    return result['f1'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The golden set is used for the training and the test of the CatBooster classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['episode id', 'transcript', 'best_summary'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# creation of gold set\n",
    "\n",
    "metadata_path_gold = os.path.join(dataset_path, '150gold.tsv')\n",
    "metadata_gold = pd.read_csv(metadata_path_gold, sep='\\t')\n",
    "metadata_gold = pd.merge(metadata_gold, metadata_train, left_on='episode id', right_on='episode_uri')\n",
    "\n",
    "quality = {\n",
    "    'B': 1,\n",
    "    'F': 2,\n",
    "    'G': 3,\n",
    "    'E': 4\n",
    "}\n",
    "\n",
    "# convert egfb columns to a quality score\n",
    "egfb_columns = ['EGFB', 'EGFB.1', 'EGFB.2', 'EGFB.3', 'EGFB.4', 'EGFB.5']\n",
    "egfb_to_quality = metadata_gold[egfb_columns].applymap(lambda x: quality[x])\n",
    "\n",
    "# remove rows with no quality > 1\n",
    "egfb_to_quality = egfb_to_quality[[any(row > 1) for row in egfb_to_quality.values]] \n",
    "\n",
    "# select the best transcript for each episode\n",
    "best_egfb = egfb_to_quality.apply(lambda x: x.idxmax(), axis=1)\n",
    "best_summary = [metadata_gold.iloc[i, np.argwhere(metadata_gold.columns == egfb)[0][0] - 1] for i, egfb in best_egfb.iteritems()]\n",
    "\n",
    "metadata_gold = metadata_gold.loc[best_egfb.index]\n",
    "metadata_gold['best_summary'] = best_summary\n",
    "\n",
    "# add transcripts\n",
    "metadata_gold['transcript'] = metadata_gold.apply(get_transcription, axis=1)\n",
    "\n",
    "data = metadata_gold[['episode id', 'transcript', 'best_summary']]\n",
    "num_episodes = data.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "metric = bertscore_f1_score\n",
    "\n",
    "# creation of the dataset for chunk classification\n",
    "# creation of the targets\n",
    "\n",
    "chunks = []\n",
    "for i in range(num_episodes):\n",
    "    print(f\"Episode {i}\")\n",
    "    chunk = semantic_segmentation(data.iloc[i].transcript, model)\n",
    "    chunks.append(chunk)\n",
    "\n",
    "targets = []\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    print(f\"Episode: {i+1}\")\n",
    "    for j in range(len(chunks[i])):\n",
    "        description = data.iloc[i].best_summary\n",
    "        if isChunkUseful(' '.join(chunks[i][j]), description, metric, threshold):\n",
    "            targets.append(1)\n",
    "        else:\n",
    "            targets.append(0)\n",
    "\n",
    "y = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction of the features\n",
    "\n",
    "extractor = FeatureExtractor()\n",
    "features = []\n",
    "for i in len(metadata_train):\n",
    "    for j in range(len(chunks[i])):\n",
    "        chunk = chunks[i][j]\n",
    "        features.append(extractor.extract_features(chunk))\n",
    "\n",
    "X = np.array(features)\n",
    "\n",
    "\n",
    "\n",
    "# splitting dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "catboost = CatBoostClassifier(iterations=2,\n",
    "                           learning_rate=1,\n",
    "                           depth=2)\n",
    "# Fit model\n",
    "catboost.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = catboost.predict(X_test)\n",
    "accuracy = accuracy(y, y_pred)\n",
    "\n",
    "print(f\"Accuracy of chunk selection: {round(accuracy,2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 256\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "\n",
    "model_checkpoint = \"facebook/bart-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(dataset, text_column, summary_column, max_input_length, max_target_length, padding, prefix=\"summarize: \"):\n",
    "    inputs = dataset[text_column]\n",
    "    targets = dataset[summary_column]\n",
    "    inputs = [prefix + inp for inp in inputs]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = \"max_length\"\n",
    "train_dataset = train_dataset.map(\n",
    "                lambda x: preprocess_function(x, \"transcript\", \"best_summary\", max_input_length, max_target_length, padding, prefix=\"summarize: \"),\n",
    "                batched=True,\n",
    "                remove_columns=train_dataset.column_names,\n",
    "                desc=\"Running tokenizer on train dataset\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def sample_generator(dataset, model, tokenizer, shuffle, pad_to_multiple_of=None):\n",
    "    if shuffle:\n",
    "        sample_ordering = np.random.permutation(len(dataset))\n",
    "    else:\n",
    "        sample_ordering = np.arange(len(dataset))\n",
    "    for sample_idx in sample_ordering:\n",
    "        example = dataset[int(sample_idx)]\n",
    "        # Handle dicts with proper padding and conversion to tensor.\n",
    "        example = tokenizer.pad(example, return_tensors=\"np\", pad_to_multiple_of=pad_to_multiple_of)\n",
    "        example = {key: tf.convert_to_tensor(arr, dtype_hint=tf.int32) for key, arr in example.items()}\n",
    "        if model is not None and hasattr(model, \"prepare_decoder_input_ids_from_labels\"):\n",
    "            decoder_input_ids = model.prepare_decoder_input_ids_from_labels(\n",
    "                labels=tf.expand_dims(example[\"labels\"], 0)\n",
    "            )\n",
    "            example[\"decoder_input_ids\"] = tf.squeeze(decoder_input_ids, 0)\n",
    "        yield example, example[\"labels\"]  # TF needs some kind of labels, even if we don't use them\n",
    "    return\n",
    "\n",
    "# region Helper functions\n",
    "def dataset_to_tf(dataset, model, tokenizer, total_batch_size, num_epochs, shuffle):\n",
    "    if dataset is None:\n",
    "        return None\n",
    "    train_generator = partial(sample_generator, dataset, model, tokenizer, shuffle=shuffle)\n",
    "    train_signature = {\n",
    "        feature: tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "        for feature in dataset.features\n",
    "        if feature != \"special_tokens_mask\"\n",
    "    }\n",
    "    if (\n",
    "        model is not None\n",
    "        and \"decoder_input_ids\" not in train_signature\n",
    "        and hasattr(model, \"prepare_decoder_input_ids_from_labels\")\n",
    "    ):\n",
    "        train_signature[\"decoder_input_ids\"] = train_signature[\"labels\"]\n",
    "    # This may need to be changed depending on your particular model or tokenizer!\n",
    "    padding_values = {\n",
    "        key: tf.convert_to_tensor(tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0, dtype=tf.int32)\n",
    "        for key in train_signature.keys()\n",
    "    }\n",
    "    padding_values[\"labels\"] = tf.convert_to_tensor(-100, dtype=tf.int32)\n",
    "    train_signature[\"labels\"] = train_signature[\"input_ids\"]\n",
    "    train_signature = (train_signature, train_signature[\"labels\"])\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "    tf_dataset = (\n",
    "        tf.data.Dataset.from_generator(train_generator, output_signature=train_signature)\n",
    "        .with_options(options)\n",
    "        .padded_batch(\n",
    "            batch_size=total_batch_size,\n",
    "            drop_remainder=True,\n",
    "            padding_values=(padding_values, np.array(-100, dtype=np.int32)),\n",
    "        )\n",
    "        .repeat(int(num_epochs))\n",
    "    )\n",
    "    return tf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_batch_size = 2\n",
    "num_train_epochs = 3\n",
    "learning_rate = 5e-5\n",
    "tf_train_dataset = dataset_to_tf(\n",
    "            train_dataset,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            total_batch_size=total_train_batch_size,\n",
    "            num_epochs=num_train_epochs,\n",
    "            shuffle=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "# region Optimizer, loss and LR scheduling\n",
    "# Scheduler and math around the number of training steps.\n",
    "num_update_steps_per_epoch = len(train_dataset) // total_train_batch_size\n",
    "num_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=learning_rate, num_train_steps=num_train_steps, num_warmup_steps=0\n",
    ")\n",
    "\n",
    "def masked_sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    # We clip the negative labels to 0 to avoid NaNs appearing in the output and\n",
    "    # fouling up everything that comes afterwards. The loss values corresponding to clipped values\n",
    "    # will be masked later anyway, but even masked NaNs seem to cause overflows for some reason.\n",
    "    # 1e6 is chosen as a reasonable upper bound for the number of token indices - in the unlikely\n",
    "    # event that you have more than 1 million tokens in your vocabulary, consider increasing this value.\n",
    "    # More pragmatically, consider redesigning your tokenizer.\n",
    "    losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        tf.clip_by_value(y_true, 0, int(1e6)), y_pred, from_logits=True\n",
    "    )\n",
    "    # Compute the per-sample loss only over the unmasked tokens\n",
    "    losses = tf.ragged.boolean_mask(losses, y_true != -100)\n",
    "    losses = tf.reduce_mean(losses, axis=-1)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "# region Metric\n",
    "metric = load_metric(\"rouge\")\n",
    "# endregion\n",
    "\n",
    "# region Training\n",
    "model.compile(loss={\"logits\": masked_sparse_categorical_crossentropy}, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "                tf_train_dataset,\n",
    "                epochs=int(num_train_epochs),\n",
    "                steps_per_epoch=num_update_steps_per_epoch,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transcript_exaple = train_data.iloc[45].transcript\n",
    "transcript_exaple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best summarization\n",
    "train_data.iloc[45].best_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = np.reshape(tokenizer(transcript_exaple, max_length=max_input_length, padding=padding, truncation=True).input_ids, (1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output =model.generate(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd86b5c47339ba7c128568c1fdf273a8327309a329b63c3cc5de2095ec077d11"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
