{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 fine-tuning in a Text Summarization task\n",
    "\n",
    "Nowadays, the AI community has two ways to approach automatic text summarization, Extractive Summarization and Abstractive Summarization:\n",
    "- _Extractive Summarization_: the extractive approach selects the most important phrases and lines from the documents. It then combines all the important lines to create the summary. So, in this case, every line and word of the summary actually belongs to the original document which is summarized.\n",
    "- _Abstractive Summarization_: The abstractive approach uses new phrases and terms that are different from the original document, keeping the meaning the same, just like how humans do in summarization. So, it is much harder than the extractive approach.\n",
    "\n",
    "The **abstractive text summarization** is one of the most challenging tasks in natural language processing, involving understanding of long passages, information compression, and language generation. The dominant paradigm for training machine learning models to do this is sequence-to-sequence (seq2seq) learning, where a neural network learns to map input sequences to output sequences. While these seq2seq models were initially developed using recurrent neural networks, Transformer encoder-decoder models have recently become favored as they are more effective at modeling the dependencies present in the long sequences encountered in summarization.\n",
    "\n",
    "Transformer models combined with self-supervised pre-training (e.g., BERT, GPT-2, RoBERTa, XLNet, ALBERT, T5, ELECTRA) have shown to be a powerful framework for producing general language learning.\n",
    "Text-To-Text Transfer Transformer (T5) model, pre-trained on Colossal Clean Crawled Corpus (C4), a cleaned version of Common Crawl that is two orders of magnitude larger than Wikipedia, achieves state-of-the-art results on many NLP benchmarks while being flexible enough to be fine-tuned to a variety of important downstream tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import os\n",
    "import regex as re\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(os.path.abspath(\"\"), 'podcasts-no-audio-13GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:  Index(['show_uri', 'show_name', 'show_description', 'publisher', 'language',\n",
      "       'rss_link', 'episode_uri', 'episode_name', 'episode_description',\n",
      "       'duration', 'show_filename_prefix', 'episode_filename_prefix'],\n",
      "      dtype='object')\n",
      "Shape:  (105360, 12)\n"
     ]
    }
   ],
   "source": [
    "metadata_path_train = os.path.join(dataset_path, 'metadata.tsv')\n",
    "metadata_train = pd.read_csv(metadata_path_train, sep='\\t')\n",
    "print(\"Columns: \", metadata_train.columns)\n",
    "print(\"Shape: \", metadata_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(episode):\n",
    "    # extract the 2 reference number/letter to access the episode transcript\n",
    "    show_filename = episode['show_filename_prefix']\n",
    "    episode_filename = episode['episode_filename_prefix'] + \".json\"\n",
    "    dir_1, dir_2 = re.match(r'show_(\\d)(\\w).*', show_filename).groups()\n",
    "\n",
    "    # check if the transcript file in all the derived subfolders exist\n",
    "    transcipt_path = os.path.join(dataset_path, \"spotify-podcasts-2020\",\n",
    "                                \"podcasts-transcripts\", dir_1, dir_2,\n",
    "                                show_filename, episode_filename)\n",
    "\n",
    "    return transcipt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the transcript files exist\n",
    "for i in range(len(metadata_train)):\n",
    "    assert os.path.exists(get_path(metadata_train.iloc[i]))\n",
    "\n",
    "print(\"All files exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcription(episode):\n",
    "    with open(get_path(episode), 'r') as f:\n",
    "        episode_json = json.load(f)\n",
    "        # seems that the last result in each trastcript is a repetition of the first one, so we ignore it\n",
    "        transcripts = [\n",
    "            result[\"alternatives\"][0]['transcript'] if 'transcript' in result[\"alternatives\"][0] else \"\"\n",
    "            for result in episode_json[\"results\"][:-1]\n",
    "        ]\n",
    "        return \" \".join(transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build gold dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:  Index(['show name', 'episode name', 'episode id', 'creator description',\n",
      "       'EGFB', 'lexrank summary', 'EGFB.1', 'textrank summary', 'EGFB.2',\n",
      "       'lsa summary', 'EGFB.3', 'quasi-supervised summary', 'EGFB.4',\n",
      "       'supervised summary', 'EGFB.5', 'show_uri', 'show_name',\n",
      "       'show_description', 'publisher', 'language', 'rss_link', 'episode_uri',\n",
      "       'episode_name', 'episode_description', 'duration',\n",
      "       'show_filename_prefix', 'episode_filename_prefix'],\n",
      "      dtype='object')\n",
      "Shape:  (150, 27)\n"
     ]
    }
   ],
   "source": [
    "metadata_path_gold = os.path.join(dataset_path, '150gold.tsv')\n",
    "metadata_gold = pd.read_csv(metadata_path_gold, sep='\\t')\n",
    "metadata_gold = pd.merge(metadata_gold, metadata_train, left_on='episode id', right_on='episode_uri')\n",
    "\n",
    "print(\"Columns: \", metadata_gold.columns)\n",
    "print(\"Shape: \", metadata_gold.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality = {\n",
    "    'B': 1,\n",
    "    'F': 2,\n",
    "    'G': 3,\n",
    "    'E': 4\n",
    "}\n",
    "\n",
    "# convert egfb columns to a quality score\n",
    "egfb_columns = ['EGFB', 'EGFB.1', 'EGFB.2', 'EGFB.3', 'EGFB.4', 'EGFB.5']\n",
    "egfb_to_quality = metadata_gold[egfb_columns].applymap(lambda x: quality[x])\n",
    "\n",
    "# remove rows with no quality > 1\n",
    "egfb_to_quality = egfb_to_quality[[any(row > 1) for row in egfb_to_quality.values]] \n",
    "\n",
    "# select the best transcript for each episode\n",
    "best_egfb = egfb_to_quality.apply(lambda x: x.idxmax(), axis=1)\n",
    "best_summary = [metadata_gold.iloc[i, np.argwhere(metadata_gold.columns == egfb)[0][0] - 1] for i, egfb in best_egfb.iteritems()]\n",
    "\n",
    "metadata_gold = metadata_gold.loc[best_egfb.index]\n",
    "metadata_gold['best_summary'] = best_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add transcripts\n",
    "metadata_gold['transcript'] = metadata_gold.apply(get_transcription, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode id</th>\n",
       "      <th>transcript</th>\n",
       "      <th>best_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spotify:episode:4KRC1TZ28FavN3J5zLHEtQ</td>\n",
       "      <td>What's up fellas? So I got a patron supported...</td>\n",
       "      <td>All right guys now as y'all guys might know so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spotify:episode:4tdDQcsBOUVWnA9XrpgTzS</td>\n",
       "      <td>If you are bored you are boring.  One of my ki...</td>\n",
       "      <td>It was the first and last time I ever said tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spotify:episode:626YAxomH0HZ6nCW9NLlGY</td>\n",
       "      <td>Visit Larisa English club.com English everyday...</td>\n",
       "      <td>Prepositions of movement review two is the sec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spotify:episode:6AUFl7KQWN6pzGFEIEKFQu</td>\n",
       "      <td>So so and salutations Summers and welcome to t...</td>\n",
       "      <td>My passion for The Sims 4 Grew From consuming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spotify:episode:6IDbemwG5t6XMlctbqcna7</td>\n",
       "      <td>Hi everyone. This is Justin from a liquidy pla...</td>\n",
       "      <td>This week on Nothing But A Bob Thang, Nathan a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>spotify:episode:2zr8iztbD8xSbuWO60tfHg</td>\n",
       "      <td>Well, everybody's clear here with a word from ...</td>\n",
       "      <td>It was a significant weekend in the NWSL, with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>spotify:episode:2SfUG4VtJFkyiuNgHALlsC</td>\n",
       "      <td>All right. Now - just one second now I'm confu...</td>\n",
       "      <td>During the first ever Frank and Eric Movie Par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>spotify:episode:2c2WPjRpoCSxtnAw0WsoqG</td>\n",
       "      <td>What is up? Everyone? Alright, so I've been pr...</td>\n",
       "      <td>LFT Radio - Lifelong Fitness and Training In t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>spotify:episode:6oZYPfBhCdpTSamM9Uj0v9</td>\n",
       "      <td>Oh, so you have to do a lot eight game. It was...</td>\n",
       "      <td>On today's show, we sit down with LSU freshman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>spotify:episode:1QOhuDRITc31SeemiqoibR</td>\n",
       "      <td>Hello and welcome to a special edition of th...</td>\n",
       "      <td>The Olympic qualifiers are here and we have pl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 episode id  \\\n",
       "0    spotify:episode:4KRC1TZ28FavN3J5zLHEtQ   \n",
       "1    spotify:episode:4tdDQcsBOUVWnA9XrpgTzS   \n",
       "2    spotify:episode:626YAxomH0HZ6nCW9NLlGY   \n",
       "3    spotify:episode:6AUFl7KQWN6pzGFEIEKFQu   \n",
       "5    spotify:episode:6IDbemwG5t6XMlctbqcna7   \n",
       "..                                      ...   \n",
       "145  spotify:episode:2zr8iztbD8xSbuWO60tfHg   \n",
       "146  spotify:episode:2SfUG4VtJFkyiuNgHALlsC   \n",
       "147  spotify:episode:2c2WPjRpoCSxtnAw0WsoqG   \n",
       "148  spotify:episode:6oZYPfBhCdpTSamM9Uj0v9   \n",
       "149  spotify:episode:1QOhuDRITc31SeemiqoibR   \n",
       "\n",
       "                                            transcript  \\\n",
       "0     What's up fellas? So I got a patron supported...   \n",
       "1    If you are bored you are boring.  One of my ki...   \n",
       "2    Visit Larisa English club.com English everyday...   \n",
       "3    So so and salutations Summers and welcome to t...   \n",
       "5    Hi everyone. This is Justin from a liquidy pla...   \n",
       "..                                                 ...   \n",
       "145  Well, everybody's clear here with a word from ...   \n",
       "146  All right. Now - just one second now I'm confu...   \n",
       "147  What is up? Everyone? Alright, so I've been pr...   \n",
       "148  Oh, so you have to do a lot eight game. It was...   \n",
       "149    Hello and welcome to a special edition of th...   \n",
       "\n",
       "                                          best_summary  \n",
       "0    All right guys now as y'all guys might know so...  \n",
       "1    It was the first and last time I ever said tha...  \n",
       "2    Prepositions of movement review two is the sec...  \n",
       "3    My passion for The Sims 4 Grew From consuming ...  \n",
       "5    This week on Nothing But A Bob Thang, Nathan a...  \n",
       "..                                                 ...  \n",
       "145  It was a significant weekend in the NWSL, with...  \n",
       "146  During the first ever Frank and Eric Movie Par...  \n",
       "147  LFT Radio - Lifelong Fitness and Training In t...  \n",
       "148  On today's show, we sit down with LSU freshman...  \n",
       "149  The Olympic qualifiers are here and we have pl...  \n",
       "\n",
       "[142 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = metadata_gold[['episode id', 'transcript', 'best_summary']]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Translating text to numbers is known as encoding. Encoding is done in a two-step process: the tokenization, followed by the conversion to input IDs using the same vocabulary used when the model was pretrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max input length:  638\n",
      "Max target length:  188\n"
     ]
    }
   ],
   "source": [
    "max_input_length = int(np.quantile(train_data['transcript'].apply(len), 0.01))\n",
    "max_target_length = int(np.quantile(train_data['best_summary'].apply(len), 0.1))\n",
    "print(\"Max input length: \", max_input_length)\n",
    "print(\"Max target length: \", max_target_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoTokenizer class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_pandas(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(dataset, text_column, summary_column, max_input_length, max_target_length, padding, prefix=\"summarize: \"):\n",
    "    inputs = dataset[text_column]\n",
    "    targets = dataset[summary_column]\n",
    "    inputs = [prefix + inp for inp in inputs]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on train dataset: 100%|██████████| 1/1 [00:00<00:00,  1.29ba/s]\n"
     ]
    }
   ],
   "source": [
    "padding = \"max_length\"\n",
    "train_dataset = train_dataset.map(\n",
    "                lambda x: preprocess_function(x, \"transcript\", \"best_summary\", max_input_length, max_target_length, padding, prefix=\"summarize: \"),\n",
    "                batched=True,\n",
    "                remove_columns=train_dataset.column_names,\n",
    "                desc=\"Running tokenizer on train dataset\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def sample_generator(dataset, model, tokenizer, shuffle, pad_to_multiple_of=None):\n",
    "    if shuffle:\n",
    "        sample_ordering = np.random.permutation(len(dataset))\n",
    "    else:\n",
    "        sample_ordering = np.arange(len(dataset))\n",
    "    for sample_idx in sample_ordering:\n",
    "        example = dataset[int(sample_idx)]\n",
    "        # Handle dicts with proper padding and conversion to tensor.\n",
    "        example = tokenizer.pad(example, return_tensors=\"np\", pad_to_multiple_of=pad_to_multiple_of)\n",
    "        example = {key: tf.convert_to_tensor(arr, dtype_hint=tf.int32) for key, arr in example.items()}\n",
    "        if model is not None and hasattr(model, \"prepare_decoder_input_ids_from_labels\"):\n",
    "            decoder_input_ids = model.prepare_decoder_input_ids_from_labels(\n",
    "                labels=tf.expand_dims(example[\"labels\"], 0)\n",
    "            )\n",
    "            example[\"decoder_input_ids\"] = tf.squeeze(decoder_input_ids, 0)\n",
    "        yield example, example[\"labels\"]  # TF needs some kind of labels, even if we don't use them\n",
    "    return\n",
    "\n",
    "# region Helper functions\n",
    "def dataset_to_tf(dataset, model, tokenizer, total_batch_size, num_epochs, shuffle):\n",
    "    if dataset is None:\n",
    "        return None\n",
    "    train_generator = partial(sample_generator, dataset, model, tokenizer, shuffle=shuffle)\n",
    "    train_signature = {\n",
    "        feature: tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "        for feature in dataset.features\n",
    "        if feature != \"special_tokens_mask\"\n",
    "    }\n",
    "    if (\n",
    "        model is not None\n",
    "        and \"decoder_input_ids\" not in train_signature\n",
    "        and hasattr(model, \"prepare_decoder_input_ids_from_labels\")\n",
    "    ):\n",
    "        train_signature[\"decoder_input_ids\"] = train_signature[\"labels\"]\n",
    "    # This may need to be changed depending on your particular model or tokenizer!\n",
    "    padding_values = {\n",
    "        key: tf.convert_to_tensor(tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0, dtype=tf.int32)\n",
    "        for key in train_signature.keys()\n",
    "    }\n",
    "    padding_values[\"labels\"] = tf.convert_to_tensor(-100, dtype=tf.int32)\n",
    "    train_signature[\"labels\"] = train_signature[\"input_ids\"]\n",
    "    train_signature = (train_signature, train_signature[\"labels\"])\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "    tf_dataset = (\n",
    "        tf.data.Dataset.from_generator(train_generator, output_signature=train_signature)\n",
    "        .with_options(options)\n",
    "        .padded_batch(\n",
    "            batch_size=total_batch_size,\n",
    "            drop_remainder=True,\n",
    "            padding_values=(padding_values, np.array(-100, dtype=np.int32)),\n",
    "        )\n",
    "        .repeat(int(num_epochs))\n",
    "    )\n",
    "    return tf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.modeling_tf_utils.TFSharedEmbeddings at 0x211064559a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_batch_size = 2\n",
    "num_train_epochs = 3\n",
    "learning_rate = 5e-5\n",
    "tf_train_dataset = dataset_to_tf(\n",
    "            train_dataset,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            total_batch_size=total_train_batch_size,\n",
    "            num_epochs=num_train_epochs,\n",
    "            shuffle=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "# region Optimizer, loss and LR scheduling\n",
    "# Scheduler and math around the number of training steps.\n",
    "num_update_steps_per_epoch = len(train_dataset) // total_train_batch_size\n",
    "num_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=learning_rate, num_train_steps=num_train_steps, num_warmup_steps=0\n",
    ")\n",
    "\n",
    "def masked_sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    # We clip the negative labels to 0 to avoid NaNs appearing in the output and\n",
    "    # fouling up everything that comes afterwards. The loss values corresponding to clipped values\n",
    "    # will be masked later anyway, but even masked NaNs seem to cause overflows for some reason.\n",
    "    # 1e6 is chosen as a reasonable upper bound for the number of token indices - in the unlikely\n",
    "    # event that you have more than 1 million tokens in your vocabulary, consider increasing this value.\n",
    "    # More pragmatically, consider redesigning your tokenizer.\n",
    "    losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        tf.clip_by_value(y_true, 0, int(1e6)), y_pred, from_logits=True\n",
    "    )\n",
    "    # Compute the per-sample loss only over the unmasked tokens\n",
    "    losses = tf.ragged.boolean_mask(losses, y_true != -100)\n",
    "    losses = tf.reduce_mean(losses, axis=-1)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "# region Metric\n",
    "metric = load_metric(\"rouge\")\n",
    "# endregion\n",
    "\n",
    "# region Training\n",
    "model.compile(loss={\"logits\": masked_sparse_categorical_crossentropy}, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "71/71 [==============================] - 13s 174ms/step - loss: 3.3629 - logits_loss: 3.3629\n",
      "Epoch 2/3\n",
      "71/71 [==============================] - 21s 302ms/step - loss: 3.3379 - logits_loss: 3.3379\n",
      "Epoch 3/3\n",
      "71/71 [==============================] - 13s 177ms/step - loss: 3.3458 - logits_loss: 3.3458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21108e13490>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "                tf_train_dataset,\n",
    "                epochs=int(num_train_epochs),\n",
    "                steps_per_epoch=num_update_steps_per_epoch,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi, my name is Kate's cooker. And this is your everyday positivity here on Vale. So goal setting I need to talk to you about goal setting there are a few tricks around goal-setting. Sometimes it's about making sure that there's one clear goal. And at this time of the year you can end up with having loads of goals. There's another trick around goal setting and that is this if you had a goal in 2019 that you look at back at your kind of December January time, and he's written down and go. Oh, I didn't do that goal. I'm here to tell you. That it's okay. So I had a goal. I wanted to play the piano by the time I was 40 and for one reason or another that goal has been put by the wayside mainly because I wanted to be able to send you stuff but Facebook kept on telling me I couldn't because it had like a real song in it. So I've kept that goal and I'm going to stick to that goal and I'm going to make sure that I work to that goal in 2020.  It's a shame. I didn't make it to the my 40th birthday playing piano in the street Your Song by Elton John. That's what I wanted to do. Equally if I beat myself up about not achieving that goal when I know that it's still something that I could do. Then I think that I've done myself a disservice. So it's something I want to do. I'm going to extend the goal and as should you if you have not quite achieved what you wanted to achieve it's not about making sure that you beat yourself up about it. It's about making sure that you look at it and go right? Why didn't that happen? What was it about that goal that  It didn't happen and how could I make it happen again? So cut yourself some slack. Give yourself a break. Look at those goals from 2019 that maybe you didn't achieve and think about how 2020 could be your year. All right, make sure you check in the Facebook group today everyday positivity with Kate's Cocker. There's a video in there about how you can move your Wheel of Life discoveries from the last week or so and turn them into goals. And then tomorrow in the Facebook group. I will be doing a Facebook live around how you can set some good goals for 2020.  NT and indeed potentially the next decade so check out Facebook every day positivity with Kate Cocker come and join. I'll be there doing a Facebook Live 5 p.m. UK time 12:00 noon eastern time and 9 a.m. Pacific time. I will see you there. In the meantime. Have a great day. If you enjoy the everyday positivity podcast. I hope you do then, please could you leave a review at Apple podcasts or wherever it is that you get your podcasts the reason we ask for reviews.  Is because we want to get this to as many people as possible and your positive reviews mean that we can help spread positivity around the world and get it to as many people as we can.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_exaple = train_data.iloc[46].transcript\n",
    "transcript_exaple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kate Cocker is the host of the everyday positivity podcast. Today she talks about goal setting and how to set goals for 2020. She will also be doing a Facebook live about setting goals for the next decade. If you enjoy the podcast, please leave a review at Apple podcasts or wherever it is that you get your podcasts.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best summarization\n",
    "train_data.iloc[46].best_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (695 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "example = np.reshape(tokenizer(transcript_exaple).input_ids, (1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "output =model.generate(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that goal. It's about making sure that you look at it and go right?\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e800dd11dddfb1e3886769e91ed8bbe987a221798b85010fca298ae8afbc389e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
