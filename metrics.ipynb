{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\boezi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\boezi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from datasets import load_metric\n",
    "from bert_score import plot_example\n",
    "# from bleurt import score as score_bleurt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_tokenizer(corpus):\n",
    "    \"\"\"\n",
    "    Returns a function which tokenizes a sentence according to a vocabularu built on corpus\n",
    "    Parameters:\n",
    "     corpus: list of sentences\n",
    "    Returns:\n",
    "     function to compute sentence tokenization\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    def tokenize_sentence(sentence):\n",
    "        \"\"\"\n",
    "        Returns a list of string token given a sentence\n",
    "        Parameters:\n",
    "         sentence: sentence to tokenize\n",
    "        Returns:\n",
    "         list of string tokens\n",
    "        \"\"\"\n",
    "        sequences = tokenizer.texts_to_sequences([sentence])\n",
    "        sequence = sequences[0]\n",
    "        tokenized_sentence = [tokenizer.index_word[i] for i in sequence]\n",
    "        return tokenized_sentence\n",
    "    \n",
    "    return tokenize_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference sentence: the cat is on the table\n",
      "Sentence: the dog is on the shelf\n",
      "Score: 0.625\n",
      "Sentence: the cat is on the mat on the table\n",
      "Score: 0.8929\n"
     ]
    }
   ],
   "source": [
    "def meteor_score(reference, candidate):\n",
    "    \"\"\"\n",
    "    METEOR score\n",
    "    Parameters:\n",
    "        reference: reference translation\n",
    "        candidate: generated translation\n",
    "    Returns:\n",
    "        METEOR score\n",
    "    \"\"\"\n",
    "    tokenize_fun = generate_sentence_tokenizer([reference, candidate])\n",
    "    tokenized_candidate = tokenize_fun(candidate)\n",
    "    tokenized_reference = tokenize_fun(reference)\n",
    "    return round(single_meteor_score(tokenized_reference, tokenized_candidate), 4)\n",
    "\n",
    "reference = \"the cat is on the table\"\n",
    "\n",
    "candidate_one = \"the dog is on the shelf\"\n",
    "candidate_two = \"the cat is on the mat on the table\"\n",
    "\n",
    "sentence_tok = generate_sentence_tokenizer([reference, candidate_one, candidate_two])\n",
    "sentence_tok(\"the cat is on the shelf\")\n",
    "\n",
    "score_one = meteor_score(reference, candidate_one)\n",
    "score_two = meteor_score(reference, candidate_two)\n",
    "\n",
    "print(f\"Reference sentence: {reference}\")\n",
    "print(f\"Sentence: {candidate_one}\")\n",
    "print(f\"Score: {score_one}\")\n",
    "print(f\"Sentence: {candidate_two}\")\n",
    "print(f\"Score: {score_two}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "IDF weights are not computed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\boezi\\VisualStudioProjects\\PodcastSummarization\\metrics.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 62>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/metrics.ipynb#ch0000003?line=59'>60</a>\u001b[0m p \u001b[39m=\u001b[39m bertscore_precision_score(reference, candidate)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/metrics.ipynb#ch0000003?line=60'>61</a>\u001b[0m r \u001b[39m=\u001b[39m bertscore_recall_score(reference, candidate)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/metrics.ipynb#ch0000003?line=61'>62</a>\u001b[0m f \u001b[39m=\u001b[39m bertscore_f1_score(reference, candidate)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/metrics.ipynb#ch0000003?line=62'>63</a>\u001b[0m \u001b[39mprint\u001b[39m(p, r, f)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/metrics.ipynb#ch0000003?line=64'>65</a>\u001b[0m \u001b[39m# see https://github.com/Tiiiger/bert_score/blob/master/example/Demo.ipynb for plotting\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\boezi\\VisualStudioProjects\\PodcastSummarization\\metrics.ipynb Cell 4'\u001b[0m in \u001b[0;36mbertscore_f1_score\u001b[1;34m(reference, candidate)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/metrics.ipynb#ch0000003?line=38'>39</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/metrics.ipynb#ch0000003?line=39'>40</a>\u001b[0m \u001b[39mBERTScore score, see https://github.com/huggingface/datasets/tree/master/metrics/bertscore for API\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/metrics.ipynb#ch0000003?line=40'>41</a>\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/metrics.ipynb#ch0000003?line=44'>45</a>\u001b[0m \u001b[39m    BERTScore f1 score\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/metrics.ipynb#ch0000003?line=45'>46</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/metrics.ipynb#ch0000003?line=46'>47</a>\u001b[0m bertscore \u001b[39m=\u001b[39m load_metric(\u001b[39m\"\u001b[39m\u001b[39mbertscore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/metrics.ipynb#ch0000003?line=47'>48</a>\u001b[0m result \u001b[39m=\u001b[39m bertscore\u001b[39m.\u001b[39;49mcompute(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/metrics.ipynb#ch0000003?line=48'>49</a>\u001b[0m     predictions\u001b[39m=\u001b[39;49m[candidate],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/metrics.ipynb#ch0000003?line=49'>50</a>\u001b[0m     references\u001b[39m=\u001b[39;49m[reference],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/metrics.ipynb#ch0000003?line=50'>51</a>\u001b[0m     idf\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/metrics.ipynb#ch0000003?line=51'>52</a>\u001b[0m     lang\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/metrics.ipynb#ch0000003?line=52'>53</a>\u001b[0m     rescale_with_baseline\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/metrics.ipynb#ch0000003?line=53'>54</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/metrics.ipynb#ch0000003?line=54'>55</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result[\u001b[39m'\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\boezi\\VisualStudioProjects\\PodcastSummarization\\env\\lib\\site-packages\\datasets\\metric.py:430\u001b[0m, in \u001b[0;36mMetric.compute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/datasets/metric.py?line=427'>428</a>\u001b[0m inputs \u001b[39m=\u001b[39m {input_name: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[input_name] \u001b[39mfor\u001b[39;00m input_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures}\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/datasets/metric.py?line=428'>429</a>\u001b[0m \u001b[39mwith\u001b[39;00m temp_seed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed):\n\u001b[1;32m--> <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/datasets/metric.py?line=429'>430</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcompute_kwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/datasets/metric.py?line=431'>432</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_writer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/datasets/metric.py?line=432'>433</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_writer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\datasets_modules\\metrics\\bertscore\\23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732\\bertscore.py:179\u001b[0m, in \u001b[0;36mBERTScore._compute\u001b[1;34m(self, predictions, references, lang, model_type, num_layers, verbose, idf, device, batch_size, nthreads, all_layers, rescale_with_baseline, baseline_path, use_fast_tokenizer)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=164'>165</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcached_bertscorer\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached_bertscorer\u001b[39m.\u001b[39mhash \u001b[39m!=\u001b[39m hashcode:\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=165'>166</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached_bertscorer \u001b[39m=\u001b[39m scorer(\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=166'>167</a>\u001b[0m             model_type\u001b[39m=\u001b[39mmodel_type,\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=167'>168</a>\u001b[0m             num_layers\u001b[39m=\u001b[39mnum_layers,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=175'>176</a>\u001b[0m             baseline_path\u001b[39m=\u001b[39mbaseline_path,\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=176'>177</a>\u001b[0m         )\n\u001b[1;32m--> <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=178'>179</a>\u001b[0m (P, R, F) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcached_bertscorer\u001b[39m.\u001b[39;49mscore(\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=179'>180</a>\u001b[0m     cands\u001b[39m=\u001b[39;49mpredictions,\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=180'>181</a>\u001b[0m     refs\u001b[39m=\u001b[39;49mreferences,\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=181'>182</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=182'>183</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=183'>184</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=184'>185</a>\u001b[0m output_dict \u001b[39m=\u001b[39m {\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=185'>186</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mprecision\u001b[39m\u001b[39m\"\u001b[39m: P\u001b[39m.\u001b[39mtolist(),\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=186'>187</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m\"\u001b[39m: R\u001b[39m.\u001b[39mtolist(),\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=187'>188</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m\"\u001b[39m: F\u001b[39m.\u001b[39mtolist(),\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=188'>189</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mhashcode\u001b[39m\u001b[39m\"\u001b[39m: hashcode,\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=189'>190</a>\u001b[0m }\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/.cache/huggingface/modules/datasets_modules/metrics/bertscore/23c058b03785b916e9331e97245dd43a377e84fb477ebdb444aff40629e99732/bertscore.py?line=190'>191</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output_dict\n",
      "File \u001b[1;32mc:\\Users\\boezi\\VisualStudioProjects\\PodcastSummarization\\env\\lib\\site-packages\\bert_score\\scorer.py:206\u001b[0m, in \u001b[0;36mBERTScorer.score\u001b[1;34m(self, cands, refs, verbose, batch_size, return_hash)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/bert_score/scorer.py?line=202'>203</a>\u001b[0m     start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/bert_score/scorer.py?line=204'>205</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39midf:\n\u001b[1;32m--> <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/bert_score/scorer.py?line=205'>206</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_idf_dict, \u001b[39m\"\u001b[39m\u001b[39mIDF weights are not computed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/bert_score/scorer.py?line=206'>207</a>\u001b[0m     idf_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_idf_dict\n\u001b[0;32m    <a href='file:///c%3A/Users/boezi/VisualStudioProjects/PodcastSummarization/env/lib/site-packages/bert_score/scorer.py?line=207'>208</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mAssertionError\u001b[0m: IDF weights are not computed"
     ]
    }
   ],
   "source": [
    "def bertscore_precision_score(reference, candidate):\n",
    "    \"\"\"\n",
    "    BERTScore score,\n",
    "    see https://github.com/huggingface/datasets/tree/master/metrics/bertscore for API\n",
    "    Parameters:\n",
    "        reference: reference translation\n",
    "        candidate: generated translation\n",
    "    Returns:\n",
    "        BERTScore precision score\n",
    "    \"\"\"\n",
    "    bertscore = load_metric(\"bertscore\")\n",
    "    result = bertscore.compute(\n",
    "        predictions=[candidate],\n",
    "        references=[reference],\n",
    "        lang=\"en\",\n",
    "        rescale_with_baseline=True\n",
    "    )\n",
    "    return result['precision'][0]\n",
    "    \n",
    "def bertscore_recall_score(reference, candidate):\n",
    "    \"\"\"\n",
    "    BERTScore score, see https://github.com/huggingface/datasets/tree/master/metrics/bertscore for API\n",
    "    Parameters:\n",
    "        reference: reference translation\n",
    "        candidate: generated translation\n",
    "    Returns:\n",
    "        BERTScore recall score\n",
    "    \"\"\"\n",
    "    bertscore = load_metric(\"bertscore\")\n",
    "    result = bertscore.compute(\n",
    "        predictions=[candidate],\n",
    "        references=[reference],\n",
    "        lang=\"en\",\n",
    "        rescale_with_baseline=True\n",
    "    )\n",
    "    return result['recall'][0]\n",
    "\n",
    "def bertscore_f1_score(reference, candidate):\n",
    "    \"\"\"\n",
    "    BERTScore score, see https://github.com/huggingface/datasets/tree/master/metrics/bertscore for API\n",
    "    Parameters:\n",
    "        reference: reference translation\n",
    "        candidate: generated translation\n",
    "    Returns:\n",
    "        BERTScore f1 score\n",
    "    \"\"\"\n",
    "    bertscore = load_metric(\"bertscore\")\n",
    "    result = bertscore.compute(\n",
    "        predictions=[candidate],\n",
    "        references=[reference],\n",
    "        lang=\"en\",\n",
    "        rescale_with_baseline=True\n",
    "    )\n",
    "    return result['f1'][0]\n",
    "\n",
    "reference = \"All right guys now as y'all guys might know sometimes maybe she did have a little bit of interest but you just did some just went on a kilt that but a lot of times they don't even be that guys when you go up and get some of these women normal guys. Alright guys, so anytime a woman rejects you what she basically saying is you are not what I'm looking for because there's no such thing as a woman being off the market i period and for any of you guys go jump on your fucking soapbox talking shit if women approached me.\"\n",
    "candidate = \"That's what she's seeking.  She wants a man that's about two dollars that look out for her and a billionaire comes up and approached her and she knows he's a billionaire.   Yo chick gonna be gone not that day.  Not that day because she got to see what he's about.  She gots to see his is he he just want to fuck one time. \"\n",
    "\n",
    "p = bertscore_precision_score(reference, candidate)\n",
    "r = bertscore_recall_score(reference, candidate)\n",
    "f = bertscore_f1_score(reference, candidate)\n",
    "print(p, r, f)\n",
    "\n",
    "# see https://github.com/Tiiiger/bert_score/blob/master/example/Demo.ipynb for plotting\n",
    "plot_example(candidate, reference, lang='en', rescale_with_baseline=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: datasets.load_metric('bleurt', 'bleurt-large-512').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint C:\\Users\\boezi\\.cache\\huggingface\\metrics\\bleurt\\default\\downloads\\extracted\\906106823cd0ccac4c3923466f8fd0444690b6241d8ecbb7c2d45093eb3071de\\bleurt-base-128.\n",
      "INFO:tensorflow:Config file found, reading.\n",
      "INFO:tensorflow:Will load checkpoint bert_custom\n",
      "INFO:tensorflow:Loads full paths and checks that files exists.\n",
      "INFO:tensorflow:... name:bert_custom\n",
      "INFO:tensorflow:... vocab_file:vocab.txt\n",
      "INFO:tensorflow:... bert_config_file:bert_config.json\n",
      "INFO:tensorflow:... do_lower_case:True\n",
      "INFO:tensorflow:... max_seq_length:128\n",
      "INFO:tensorflow:Creating BLEURT scorer.\n",
      "INFO:tensorflow:Creating WordPiece tokenizer.\n",
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n",
      "INFO:tensorflow:Creating Eager Mode predictor.\n",
      "INFO:tensorflow:Loading model.\n",
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute __inference_pruned_30144 as input #0(zero-based) was expected to be a int64 tensor but is a int32 tensor [Op:__inference_pruned_30144]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-6f83e60f3981>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mcandidate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"the dog is on the shelf\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mbleurt_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreference\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-6f83e60f3981>\u001b[0m in \u001b[0;36mbleurt_score\u001b[1;34m(reference, candidate)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'BLEURT-20'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mbleurt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bleurt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbleurt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mreference\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\datasets\\metric.py\u001b[0m in \u001b[0;36mcompute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0minput_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0minput_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtemp_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m                 \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcompute_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_writer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.cache\\huggingface\\modules\\datasets_modules\\metrics\\bleurt\\f872b8bceb84260fee5854e8335168eadb5ce8aa80fd24a8ff2dbd15136546ac\\bleurt.py\u001b[0m in \u001b[0;36m_compute\u001b[1;34m(self, predictions, references)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_compute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscorer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreferences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreferences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"scores\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\bleurt\\score.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, references, candidates, batch_size, *args)\u001b[0m\n\u001b[0;32m    213\u001b[0m           \u001b[1;34m\"segment_ids\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m       }\n\u001b[1;32m--> 215\u001b[1;33m       \u001b[0mpredict_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m       \u001b[0mbatch_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m       \u001b[0mall_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\bleurt\\score.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, input_dict)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0minput_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_mask\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         segment_ids=tf.constant(\n\u001b[1;32m---> 71\u001b[1;33m             input_dict[\"segment_ids\"]))[\"predictions\"].numpy()\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1599\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mdo\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m     \"\"\"\n\u001b[1;32m-> 1601\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1603\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\wrap_function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m       return super(WrappedFunction, self)._call_impl(\n\u001b[1;32m--> 244\u001b[1;33m           args, kwargs, cancellation_manager)\n\u001b[0m\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mprune\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_signature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1617\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mstructured_err\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1618\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1619\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_with_flat_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1620\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1621\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_with_flat_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_with_flat_signature\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1666\u001b[0m                         \u001b[1;34mf\"#{i}(zero-based) to be a Tensor; \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m                         f\"got {type(arg).__name__} ({arg}).\")\n\u001b[1;32m-> 1668\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1670\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_with_structured_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1854\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    505\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 55\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: cannot compute __inference_pruned_30144 as input #0(zero-based) was expected to be a int64 tensor but is a int32 tensor [Op:__inference_pruned_30144]"
     ]
    }
   ],
   "source": [
    "def bleurt_score(reference, candidate):\n",
    "    \"\"\"\n",
    "    BLEURT score, see https://github.com/google-research/bleurt for the installation and use of the package\n",
    "    Parameters:\n",
    "        reference: reference translation\n",
    "        candidate: generated translation\n",
    "    Returns:\n",
    "        BLEURT score\n",
    "    \"\"\"\n",
    "    checkpoint = 'BLEURT-20'\n",
    "    bleurt = load_metric(\"bleurt\")\n",
    "    result = bleurt.compute(predictions=[candidate], references=[reference])\n",
    "    return result[0]\n",
    "\n",
    "reference = \"the cat is on the table\"\n",
    "candidate = \"the dog is on the shelf\"\n",
    "\n",
    "bleurt_score(reference, candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-46cc42c6687d>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-46cc42c6687d>\"\u001b[1;36m, line \u001b[1;32m25\u001b[0m\n\u001b[1;33m    for prediction in pred\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def evaluation_report(texts, model, references, metrics={'meteor'}):\n",
    "    \"\"\"\n",
    "    Show evaluation report comparing the outputs of model using input and references\n",
    "    Parameters:\n",
    "        texts: input of the network\n",
    "        model: neural network\n",
    "        references: ground truth of texts\n",
    "        metrics: set of metrics for the evaluation, metrics avaiable are 'meteor', 'bertscore_precision',\n",
    "                 'bertscore_recall', 'bertscore_f1', 'bleurt', default='meteor'\n",
    "    \"\"\"\n",
    "    # dictionary of metrics and functions to compute them\n",
    "    metrics_dict = {\n",
    "        'meteor': meteor_score,\n",
    "        'bertscore_precision': bertscore_precision_score,\n",
    "        'bertscore_recall': bertscore_recall_score,\n",
    "        'bertscore_f1': bertscore_f1_score,\n",
    "        'bleurt': bleurt_score\n",
    "    }\n",
    "    for metric in metrics:\n",
    "        assert metric in metrics_dict.keys()\n",
    "    # TODO skeleton of the function, I have to check the real format of the data\n",
    "    # TODO decode_sequence function signature\n",
    "    predictions = model.predict(texts)\n",
    "    # conversions of sequences of numbers into sentences\n",
    "    candidates = []\n",
    "    for prediction in predictions:\n",
    "        candidate = decode_sequence(prediction, model, X_vectorizer, y_vectorizer) # simmy function\n",
    "        candidates.append(candidate)\n",
    "\n",
    "    # managment of candidates to get candidates as a list of strings\n",
    "    df = pd.DataFrame({'Metric': [metric for metric in metrics], 'Score':[0.0 for _ in metrics]})\n",
    "    for metric in metrics:\n",
    "        single_scores = []\n",
    "        for candidate, reference in zip(candidates, references):\n",
    "            single_scores.append(metrics_dict[metric](candidate, reference))\n",
    "        mean = np.mean(single_scores)\n",
    "        df.loc[df['Metric'] == metric, 'Score'] = round(mean, 4)\n",
    "    # show report\n",
    "    return df\n",
    "\n",
    "texts = [\"Text 1\", \"Text 2\", \"Text 3\"]\n",
    "references = ['summary one', 'summary two', 'summary_three']\n",
    "\n",
    "def dummy_fun(texts):\n",
    "    return ['sum one', 'summary two', 'sum 3']\n",
    "\n",
    "metrics = {'meteor', 'bertscore_precision', 'bertscore_recall', 'bertscore_f1'}\n",
    "df = evaluation_report(texts, dummy_fun, references, metrics=metrics)\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd86b5c47339ba7c128568c1fdf273a8327309a329b63c3cc5de2095ec077d11"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
